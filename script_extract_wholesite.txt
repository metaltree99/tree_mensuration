#!/bin/bash

version="20210709"

#Script to extract information from whole sites
#By Sébastien Clément, started 21/12/2011
#Get from GitHub:
#https://github.com/metaltree99/tree_mensuration/blob/main/script_extract_wholesite_utf20170427.txt

create_tabs_despite_additional_data=0 #0=no, 1=yes. For test only. Will create tabulated files (turn additional_data to 0) for germplasms and germplasms_on_sites even if measurements and/or coded observations are already present in TreeSource (additional_data=1).
skip="no" # yes, no: skip most of the script. Look for SKIP_TAG below.

test=2 #Test mode: 1. yes 2. no
#	If in test mode, stops after 10 analyzed columns for validation, and displays more progress details on screen.
no_network=1 # 1 = true (no network is available), 2 = false (network P and U drives are available)


#**********TODO**********:
	#Make the "disable_trigger" and "enable_trigger" functions standalone scripts
	#ALSO: See "Modifications", 20130830
#**************************:


#Usage:
	#bash script_extract_wholesite.txt sitefile 
	
	#1. sitefile [REQUIRED]: the file containing site info
		#See sheet "Markers_for_wholesites_imports" of file ADDED.xls for required info in that file.


#For testing with ts_d
#	DELETE from coded_observations_on_germplasms where ri_date >= '2015-01-23';
#	DELETE from tree_mensurations where ri_date >= '2015-01-23';
#	DELETE from observations_on_sites where ri_date >= '2015-01-23';
#	DELETE from observation_codes where ri_date >= '2015-01-23';

#Modifications:
	#20120118: 	minor correction when SEQUENCE column is <> 0
	# 		added formatting into final utf8 files (step 13)
	#20120119: 	improved final files handling (naming)
	#		added uniformization of col number right after file cleaning to prevent some "Excel-last-column-sometimes-empty-sometimes-not"-related errors
	#20120120:	Removed 0-value observations when zero_ignored is specified in REQ_data_types.txt
	#20120124:	Added a module to check for inconsistency in measurements through time (e.g. decreasing height values)
	#20120127:	minor correction to SQL scripts signaling measurement errors.
	#		All required files including scripts are now in the folder specified by the req_files_folder variable
	#20120209:	Now puts all generated files and folders into a folder named after the site code
	#		Now checks for all specified date formats
	#20120210:	Now allows each tree to have a different species (abbreviation listed in REQ_species_codes.txt) when a SPECIES column exists and get organisms.id from all_organisms.txt
	#20120213:	Now replaces commas with periods for measurement columns (lines 1415 and 1417).
	#20120309: 	Major overhaul of tree status (E) handling, used to consider or not other coded observations (F, D, RT, QT, etc.) 
	#			- Observation codes (E, F, D, etc.) from different organizations (CFS, MRNF) can be used
	#			- Organization must be specified in the parameters as: data_origin=1 (CFS) or 2 (MRNF)
	#			- the NORMAL (living) TREE STATUS can have several values instead of only one.
	#			- reject_obs_when_abnormal_tree_status is now specified in the parameters 
	#			- If reject_obs_when_abnormal_tree_status is set to 
	#				1: the script will keep only other coded observations (F, D, RT, RQ, etc.) when tree is NORMAL (e.g. alive)
	#				2: the script will keep other coded observations EVEN if the tree as an ABNORMAL STATUS (e.g. dead, cut, or filler tree)
	#20120312:	
	#			Minor correction: When keeping other observations (e.g. F, D, QT, RT, etc.) for all values of E, the script was wrongfully excluding other observations when E was 1.
	#			Now removes coded observations if that have an empty values, before further processing. Beforehand, those values were wrongfully treated as different from no_msmt_value (e.g. 0) and this would mess up things when identifying which years had observation values <> 0 (or any other no_msmt_value).
	#			The program now checks from what machine (domain name) it is executed and finds the proper way to the P: itself. It can then be run from BioLinux or any other machine PROVIDED IT IS ON THE LOCAL NETWORK.
	#20120313: The final folder regrouping all files at the end is now timestamped
	#20120404: added the reading direction, with possible values in REQ_other_codes.txt
	#20120410: 		reading_direction added to REQ_reference_measurement_units.txt, and the possibility to add comments with #
	#			Non-numerical fields for tree_mensurations no longer have conversion of units
	#20120416		Now removes lines with field values specified in the file: REQ_values_to_remove.txt. This is done prior to field validations, and the changes are permanent.
	#20120417		REQ_required_fields.txt and REQ_optional_fields.txt are now joined into REQ_germplasm_name_fields.txt, and deal only with germplasms fields
	#			REQ_data_types.txt now contains columns for destination table (#13) and destination column (#14).
	#			With this, possible fields from germplasms_on_sites are now extracted directly from REQ_data_types.txt
	#20120418: Now checks for measurements with empty units.
	#20120713: Another BIG improvement of tree status (E) handling.
	#			Now, for each tree status, we can specify which observation(s) are allowed to be considered, by listing them in the REQ_tree_status_and_allowed_observations.txt file
	#			For example, QcE=2 status gives access to all other observations (QcD, QcMfm, QcRgbp, etc.)...
	#			...whereas QcE=1 status permits only QcD observations to be kept, and QcE=0 allows NO other observations.
	#20120716: Tiny correction on line #1042 to avoid error messages by validate_columns.txt when it was receiving units containing spaces, even if the field was to be ignored: units=$(echo $units|perl -pe 's/ //g') 
	#20120802:  Added a column with descriptions of each code in REQ_tree_status_and_allowed_observations.txt. This column is removed before using the file, with a "cut -f1-3" added at line line 1554.
	#		Also, trees with E5 CFS code (filler trees) are now allowed to have any observations in REQ_tree_status_and_allowed_observations.txt
	#20130221: Now initially ignores (Section 3A, 3C) during validation any value in the date column if it doesn't containt at least 2 consecutive digits. This avoids errors signalling when "-" were entered, even for fields not requiring a date.
	#20130225:	Creates a SQL script to insert comments on the trees on which observations (e.g. illnesses) were made while tree status was abnormal (dead, missing, etc.).
	#20130226: 
	#		While preparing tree mensurations, also replace empty values with NULL. Prior to this, only 0's were replaced by NULLs.
	#		Create SQL INSERT commands to add observations on sites when aberrant (i.e. decreasing) measurements were found.
	#		Now also copies a list of example SQL commands into the SQL_OTHER_COMMANDS_EXAMPLE.txt file, in the final_files folder.
	#20130320:
	#		REQ_obs_codes_other_than_tree_status.txt is no longer a required file. It is now extracted directly from REQ_data_types.txt
	#20130321:
	#		Major module added: substitution of values directly in the cleaned file, or in the final tabulated file, depending on the option chosen.
	#		Substitutions are listed in REQ_site-specific_values_substitution_utf.txt and can be site- and/or year-specific,.
	#			1. substitution before validation (e.g. codes for damages will be substituted from 11 to 7) - the substituted value must comply with validation rules. 
	#			2. substitution after validation (e.g. FAMPROV from 99999 TO XXXXX) - the substituted value is not submitted to validation rules, caution must be exercised.
	#			3. substitution in the final tabulated file (e.g. code F_7 ---> code F_7d) - the substituted value is checked against all existing codes from TreeSource in the observation_codes table, and if it doesn't exist, SQL commands are automatically created to insert it.
	#		Now gets observation final codes in real time from TreeSource. They are used to validate the existence or not of substitution codes.
	#		Now regroups all SQL commands (insert, update) into 2 clean scripts.
	#20130322:	Now checks and warns if replicated compartments are found.
	#20130325:
	#		Now counts the number of substitutions and lists them.
	#20130328:
	#		Now can automatically import the final tabulated data + SQL scripts into the development and production databases
	#20130402:	Now automatically generates a journal file
	#20130408: Line 1059: grep "0$" ---CORRECTED TO---> grep "[[:space:]]0$", because it was grabbing not only 0, but any other number ending by 0 (e.g. 10, 20) as well.
	#20130409:
	#		Now can choose in the parameters file to select the sub-block number instead of the block number to generate the tree name
	#			A comment is added to observations on sites when this is the case.
	#		For sites without sequence numbers, the all trees get a name with "00000" for sequences.
	#		Corrected replicated lines in value_substitutions_for_current_site_3_on_final_tabulated_file.txt when there were several columns of the same observation type (e.g. "D") for any given year.
	#		File encoding check and UTF-8 conversion is now done by a called function
	#20130410:
	#		Fixes to incorrect encoding of some files (partly iso-8859-1, partly utf-8)
	#		Infile and parameters files are now copied and converted to utf-8. The original files are left intact.
	#		NOW, the whole script is in UTF-8 to avoid encoding errors due to variables or strings with accents from the script (previously iso-8859-1) that were collated with other utf-8 files.
	#20130416:	
	#		Now can accomodate different generation methods for trees (e.g. germination, cuttings) with the header marker GEN_METH
	#		Columns with varying values are extracted and later reassembled for the germplasms tabulated file 
	#20130417:	Minor correction to fnct_replace_headers
	#20130424:	validation_data.txt is not put into the final_files folder for convenience.
	#20130425:  All lines removed  according to the criteria specified in REQ_values_to_remove.txt are now shown on screen.
	#20130827: Number of characters for the fam/prov number in the tree names is now specified in parameters_script_extract_wholesites
	#20130828: Corrected the check of existing SQL_insert_saved_errors* files, as an error was occuring with the following syntax:
		#	if [ -s ${temp_f}/SQL_insert_saved_errors* ]
		#	Now replaced with:
		#	if [ $(ls -1 ${temp_f}|grep 'SQL_insert_saved_errors'|wc -l) -gt 0 ]
		#	Also did the same for the 'SQL_update_meas*' files.
	#20130829: minor correction: copying the parameters file now uses a wildcard (*).
		#	Observation codes change: "M" is now "D", "MARKED" is now "MAR"
	#20130830
		#	Now extracts automatically the list of observation codes from TreeSource.
		#	Corrections in SQL scripts and tabulated files to accomodate the new germplasm_name ---> germplasm_id structure.
		#	NOTE: THE alternative_germplasm_name STILL HAS TO BE CORRECTED TO ACCOMODATE THIS NEW STRUCTURE
	#20130903
		#	Corrections of header names in final files generation. 
		#	Addition of temp_date_named in 2insert_01_germplasms_utf.txt.
		#	Now checks that every family/provenance number is not longer than the max specified in famprovchars
		#	#TREE: separation in two files:
		#		TREE.txt 				--->for germplasms_on_sites.tree_number. 	No change in the original file. If an integer, will be imported as is. If NULL, it will be automatically replaced by a \N in the tabulated file for import.
		#		TREE_left-padded.txt: 	---> for germplasm_names.name. 			If TREE is a number, will be left-padded. If anything else, will be replaced by "XXX".
	#20140310
		#	Correction of INSERT INTO observations_codes... 
		#	SQL_insert_and_update3.txt now has specific data_source_id and site_code
	#20140311
		#	The script now disables the trigger trg_germplasms_on_sites_03 before inserting data into germplasms_on_sites, to avoid it running forever. It re-enables it after.
	#20140312
		#	Now checks the format (digits vs other characters) of the family/provenance numbers
		#	Trigger disabling/enabling function has been refined.
	#20140313
		#	Now directly inserts germplasm names instead of germplasm ids in comments_on_germplasms, due to the newly (today) created trigger (trg_comments_on_germplasms_01) and trigger function (proc_insert_into_comments_on_germplasms_using_germplasm_names).
	#20140325
		#	all sort commands on tabulated files (>1 fields) now have the option: -t $'\t'
		#	This was necessary to prevent an error occuring when running this in the tabulated files generation of coded_observations_on_germplasms, line:
		#		join -t $'\t' <(grep -v '^#' ${req_files_folder}/REQ_obs_codes_conv_table2_sel_codes.txt |grep "^${obs_type}[[:space:]]" |cut -f2,3|sort -t $'\t' -k1,1) ${temp_f}/${temp_fileC} ...
		#		...as temp_fileC often contained empty values in the first, joining column, raising error: "extract_whole_sites_temp/E_tempC_sorted_by_obs.txt:863: is not sorted"
	#20140328
		#	No longer re-enables the trg_germplasms_on_sites_03 at the end of inserts, to avoid it happening in the middle of an insert in germplasms_on_sites by another instance of the same script, running in parallel.
	#20140708
		#	replaced site_code with temp_site_code to accomodate the new structure (PK: sites.code ===> sites.id). Also see: SQL20140626_change_site_code_for_site_id.txt
	#20140912
		#	Now reads site parameters directly from the Excel file, and validates them (name, value, format) using the file REQ_parameters_default_values_and_types_utf.txt
		#	Also checks if site code and data_source_id exist in the DB
		#	Also extracts plantation_date from DB
	#20140926
		#	Now automatically adds replicated compartment numbers whenever there are replicated block-family/provenance-tree combinations
		#	Does this for all replicated compartements except those from filler trees, which the script asks the user to identify
		#	Woooweee!
	#20150106
		#	Completed the automated replicated compartment detection block.
		#	Changed the team_code to team_id references, and allowed the use of employees full names in the file team_employees_per_date.txt (formerly team_codes_per_date.txt), and in the parameter plantation_team_employees.
	#20150113
		#	No more test_mode chosen as a parameter when executing the script
		#	Reject_obs_when_abnormal_tree_status value comes solely from the parameters. I used to be overwritten by line ~ 299.
		#	Trigger inactivation/reactivation is no longer done
		#	Now gets required files from ${ref_files_path_shared} (U:/Centre_Fibre/NationalWoodFibreAttributesDatabase/Partage/scripts/ref_files)
	#20150114
		#	Extracted the line removal based on certain values part from section "9. Create a cleaned file" to create "12. Remove lines based on certain field values". 
		#		Reason is to wait until after the replicated compartment check section (#11) has performed, because both use the all_possible_fields_pos.txt file, modified by 11.
		#	Some files extracted on-the-fly from TreeSource by the script and stored in ${req_files_folder} are now stored under ${temp_f}
		#		${req_files_folder}/REQ_obs_codes_conv_table2_all_codes.txt				--->	${temp_f}/Phenotree_all_obs_codes_and_descriptions.txt
		#		${req_files_folder}/REQ_data_source_ids_in_DB.txt						--->	${temp_f}/Phenotree_data_source_ids.txt
		#		${req_files_folder}/REQ_sites_code_and_plantation_dates_and_year_in_DB.txt		---> 	${temp_f}/Phenotree_sites_code_and_plantation_dates_and_years.txt
	#20150115
		#	Now checks existing measurement and coded observation years in TreeSource.
		#		If they exist, it means that data has already been imported for that site in the past. The following variable is set: additional_data=1
		#			It also means that records for germplasms and germplasms_on_sites have also been imported in the past as well, so tabulated files for those won't be generated.
		#		Also compares existing years in TreeSource vs Excel file for those measurements and coded observations
		#			If same years found for coded observations, offers choice to try to import nevertheless: if exactly same DATE and codes, will fail at INSERT. If not will work.
		#			If same years found for measurements, aborts, because it would fail at insert time.

	#20150116
		#	Now make a copy of critical (those to which lines are regularly added) required files on the P
	#20150122
		#	Now gets the local_network_paths.txt file from $HOME, whatever the operating system (Windows 7, BioLinux, on which the script is 
		#	The file local_network_paths.txt must now be put in /home/username for all computers where the script can be run, including computer for students or other hired employees.
		#	Note that this file will not be the same for the admin (seclemen) and other users, as well as the script_general_variables.txt file.
		#	Backup of user-modified required files is now done on the U instead of the P, avoiding problems when other users run the script.
	#20150123
		#	Convert site code to uppercase before checking it vs database.
		#	Now skips the check for replicated compartments section if data already exists in germplasms_on_sites (additional_data=1).
		#	Corrected validation_data.txt: generated from SQL_extract_data_for_validation.txt, which previously was extracting random trees based on data_source_id, but thus generated empty results when only tree_mensurations and/or coded_observations_on_germplasms were inserted.
		#		...now based on site_code search of germplasm_names.
	#20150210
		#	Doesn't use the external script script_insert_wholesite.txt anymore. It is now replaced by the function: fnct_auto_insert_main_tab_files
		#	fnct_auto_insert_in_db ==renamed==> fnct_auto_insert_all_in_db
	#20150227
		#	Now extracts each time all_organisms.txt, to ensure an up-to-date version if a SPECIES column is present.

	#20150525
		#	L'ajout d'une fonction pour calculer l'année de croissance réellement mesurée à partir de la date de mesurage est inutile, 
		#		car cette fonction est maintenant intégrée directement dans TreeSource. Voir: SQL20150521_add_growth_year_calc_to_tree_mensurations.txt
		#	Cependant, certains champs ont dû être renommés (ligne 3749): date ---> msmt_date, year ---> msmt_year

	#20160713
		#	The no_network variable allows to indicate if the P/U network drives are available, and if not, use local files instead.
		#	This was intended to overcome problems on BioLinux which now accesses the network drives erratically.

	#20160719
		#	xtraksites132.156.208.45.txt is now included in the required files under REQs_script_extract_wholesite.
		#	Section 9: cleaned file creation: modified code to get rid of lines even when they contain regular spaces or tabls (grep -v '^[[:space:]]*$').
		#	Section 23C. coded_observations_on_germplasms: now provides more explainations as to why the script is stopped when no E column could be found along other observations (e.g. D, F) of the same date.
		#	team_employees_per_date.txt is now produced under extract_whole_sites_temp.
		#	logging  is now done to ${ready2import_tabs_folder}/extract_sites_log.txt
	
	#20160728
		#	New section 5.A. that verifies the REQ_site-specific_values_substitution_utf.txt file values right after obtaining site parameters from the file (Section 5).
		#	(previously, there was some validations in section 18. (Prepare files for value substitutions before or after validation) C, D and G. These are now gone.
		#	The new section checks each column from REQ_site-specific_values_substitution_utf.txt to ensure that:
		#		Col 1: each site code exists. Note: current site code existence was confirmed in section 5.
		#		Col 2: data type (header) exists in REQ_data_types.txt.
		#		Col 3: year is valid.
		#		Col 4: original value (to be substituted):
		#			Option 1 (values to be substituted before validation): not verified, too many possible, often erraneous values.
		#			Option 2 (values to be substituted after validation): not verified, too many possible values.
		#			Option 3 (values to be substituted in tabulated file): value must be an observation code and must exist in TreeSource.
		#		Col 5: modified value
		#			Option 1 (values to be substituted before validation): value must be an observation code and must exist in TreeSource.
		#			Option 2 (values to be substituted after validation): not verified, too many possible values.
		#			Option 3 (values to be substituted in tabulated file): value is not verified, because may not exist yet in TreeSource (to be created during script execution, in Section 18).
		#		Col 6: must be: 1, 2 or 3.
	#20170426
		#	Section: "Extract the distinct measurement years for that site" : in the SELECT SQL command: measurement_year ---> growth_year_at_msmt
		#	La copie des fichiers "REQ_" (section 2) utilise l'option -f pour forcer l'écrasement des fichiers existants, le cas échéant.
	#20170427
		#	REQ_site-specific_values_substitution.txt ---> REQ_site-specific_values_substitution_utf.txt to avoid "Binary file" grep errors caused by accents.
	#20210706
		#Corrected:
		#	Database server IP address 132.156.208.45 ---> 132.156.208.30
		#	phenotree_development ---> ts_d
		#	phenotree_production ---> ts_pi

	#20210707
		#A new parameter, tree_name_format, specifies the tree name format output, and the fields required to calculate this name.
		#	There are 3 formats right now:
		#		S_B_F_T_S: the most standard format, Site code, block #, family/provenance #, compartment tree #, sequence #
		#			Example: E410D2_05_00425_002_06194_00
		#		S_B_BR_BC_F: Site_code, block #, block row #, block col #, family/provenance #
		#			Example: E188B_1_11_4_134_5
		#		S_B_F_T_C: Site_code, block #, family/provenance #, compartment tree #, compartment #
		#			Example: SLC30498_14_20921_003_0750
		#A check is done around lines 2093+ to see if the fields present in the Excel file match the requested format. If not, the script warns and exits.
		#When calculating the tree name, most fields used are now left-padded using the max number of digits encountered in all numeric values of these fields, rather than on a fixed number. See lines 3360+
		#Use_subblock_in_tree_name is no longer used as a parameter, as the tree name format is specified in the tree_name_format parameter above.


	#20210708
		#For tree name generation, a first section determines automatically the max number of digits for each field (e.g. max_digits_block_field) based on the values in the file
		#Then for each name format, that automatically generated value, or a manually-set value, is chosen to left-pad the values.

	#20210709
		#REQ_replacement_headers.txt is no longer needed, as replacement_headers.txt is dynamically created from REQ_data_types.txt.
		

#Dependencies:
	#script_general_variables.txt 
	#	location for administrator: 		P:/000_scripts/
	#	location for other users: 		U:/Centre_Fibre/NationalWoodFibreAttributesDatabase/Partage/scripts/
	#Required files in the following folder: 
	#	U:\Centre_Fibre\NationalWoodFibreAttributesDatabase\Partage\scripts\ref_files\REQs_script_extract_wholesite

#Required database privileges for the xtraksites user
	#OBJECT						Privileges				Added	Required for
	#coded_observations_on_germplasms*		INSERT,SELECT
	#comments_on_germplasms*			INSERT,SELECT
	#germplasm_names					INSERT,SELECT
	#germplasms	*					INSERT,SELECT
	#germplasms_on_sites*				INSERT,SELECT
	#mv_current_germplasms_site_and_container	SELECT,INSERT,TRUNCATE
	#observation_codes				INSERT,SELECT
	#observations_on_sites				INSERT,SELECT
	#organisms						SELECT				27/2/2015	Required to extract the table into ${exported_tables_path_shared}/all_organisms.txt
	#phenotree_variables				SELECT,UPDATE			27/2/2015	Required for the trigger trg_germplasms_on_sites_03 and trg_tree_mensurations_02, which launch proc_set_updated_site_or_container_to_1 (), which in turn updates phenotree_variables.
	#progenitors						UPDATE
	#seq_germplasms_id				USAGE
	#tree_mensurations*				INSERT,SELECT
	#v_current_germplasms_site_and_container	SELECT
	#v_grouped_employees_in_teams			SELECT
	#observation_codes_id_seq			USAGE 				23/1/2015	Inserting new records into observation_codes.
	#mv_accessible_tm_data_per_site_and_user	TRUNCATE, INSERT		23/1/2015	Truncating mv_accessible_tm_data_per_site_and_user before repopulating. This is done when inserting/deleting/updating data in tree_mensurations: trg_tree_mensurations_03 ---> proc_update_mv_accessible_tm_data_per_site_and_user()
	#v_accessible_tm_data_per_site_and_user	SELECT				23/1/2015	Truncating mv_accessible_tm_data_per_site_and_user before repopulating. This is done when inserting/deleting/updating data in tree_mensurations: trg_tree_mensurations_03 ---> proc_update_mv_accessible_tm_data_per_site_and_user()
	#	*Previously, UPDATE was required to clear the temp_germplasm_name by the trigger procedure in these tables. No longer needed (23/1/2015), as this action is no longer done immediately, but rather by the function fnct_clear_temp_fields(), executed every night by postgres, through bash script script_run_modifying_sql_scripts_daily.txt and SQL script SQL_PhenoTree_01_execute_functions.sql.


#Variables that will change depending on if network is available:
#Added: 13/7/2016
#1 = no network available. 2= network P and U drives available.
if [ ${no_network} -eq 1 ] #No network available/used.
then

	echo "NOTE: No network used!"
	echo "Make sure that the following folder from U:/Centre_Fibre/000_TreeSource/Partage/scripts/ref_files/ is copied to the current folder:"
	printf "\tREQs_script_extract_wholesite\n"
	
	#General variables replaced by script-specific variables for local use only.
		#exported_tables_path_shared: not used if local.
		cwfc_server_IP_address="132.156.208.30"
		#ref_files_path_shared: not used if local.
		#scripts_path_shared: not used if local.
		req_files_folder="./REQs_script_extract_wholesite"
		all_species_filepath="${req_files_folder}/all_organisms.txt" #NOTE: will be extracted by the current script, so no need to put it in required files.
	echo "NOTE: if any of the local files in ${req_files_folder} are modified, please copy them back to U:/Centre_Fibre/000_TreeSource/Partage/scripts/ref_files/REQs_script_extract_wholesite."

else #Network is available

	#LOCAL file specifying absolute paths to the P and U, and executing script_general_variables.txt, located on either the P (admin) or U (other users).
	#Note: the file below is different between the admin (seclemen) and the regular users, as is the script_general_variables.txt (more complete, on the P, for admin, shortened, on the U, for regular users).
	pathfile="${HOME}/local_network_paths.txt"

	#General variables:
		#Read local network paths & general variables:
		dos2unix ${pathfile} 2> /dev/null
		. ${pathfile}

		#General variables used below:
			#${exported_tables_path_shared}	Typically= U:/Centre_Fibre/000_TreeSource/Partage/scripts/extracted_lists
			#${cwfc_server_IP_address}		Typically= 132.156.208.30
			#${ref_files_path_shared}		Typically= U:/Centre_Fibre/000_TreeSource/Partage/scripts/ref_files
			#${scripts_path_shared}			Typically= U:/Centre_Fibre/000_TreeSource/Partage/scripts


		#This script specific variables
			#This will be used to extract and insert codes in observation_codes
			req_files_folder=${ref_files_path_shared}/REQs_script_extract_wholesite

			#Path to the up-to-date extracted list of organisms. Used to convert abbreviated names of the SPECIES column into organisms.id values
			#NOTE: change this path and/or provide the all_organisms.txt file when running the script on a machine that cannot readily access the U disk using the same path (e.g. Biolinux server)
			all_species_filepath=${exported_tables_path_shared}/all_organisms.txt
		
fi



#This script specific variables
	#This will be used to extract and insert codes in observatioon_codes
		PGPASSWORD=$(cat ${req_files_folder}/xtraksites132.156.208.30.txt)
		PGHOST=${cwfc_server_IP_address}
		PGUSER="xtraksites"
		export PGUSER PGPASSWORD PGHOST
	timestamp=$(date +%Y%m%d%H%M)
	date_psql_format=$(date +%Y-%m-%d)
	current_year=$(date +%Y)
	temp_f="extract_whole_sites_temp"
	germplasm_type='Arbre'	#Default germplasms.type value
	
	
	#File with default site parameters
	parameters_file=${req_files_folder}/REQ_parameters_default_values_and_types_utf.txt
	
	#Journal file to write to when running function fnct_get_and_validate_site_parameters
	param_jfile=${temp_f}/param_check_journal.txt

	#Create temporary folder if doesn't exist
	    if [ ! -e $temp_f ]; then mkdir $temp_f;fi

	#Create folder for final tabs to import 
	ready2import_tabs_folder="final_files"
	if [ ! -e ${ready2import_tabs_folder} ];then mkdir ${ready2import_tabs_folder};fi

	psql_commands_file="psql_commands_2insert_DEV.txt"

	#Fixed variables to pass to validate_column.txt
		display_full_info=2 # 2=The full info won't be displayed on screen
		display_summary=1 #1=The summary will be displayed on screen as it happens
		on_yellow_alert=1 #1=On yellow type of warning (ex: min/max values not within range), CONTINUE.
		on_red_alert=1 #1=On red type of warning (ex: text values in a numerical field), CONTINUE.
		validation_temp_f="wholesite_columns_validation_temp_folder"

	


#Array with required files
required_files=(
	${req_files_folder}/REQ_filler_tree_famprovs.txt
	${req_files_folder}/REQ_aberrant_meas_messages_utf.txt
	${req_files_folder}/REQ_data_types.txt
	${req_files_folder}/REQ_final_tabs_creation_parameters.txt
	${req_files_folder}/REQ_germplasm_name_fields.txt
	${req_files_folder}/REQ_line_markers.txt
	${req_files_folder}/REQ_obs_codes_conv_table2_header.txt
	${req_files_folder}/REQ_other_codes.txt
	${req_files_folder}/REQ_parameters_default_values_and_types_utf.txt
	${req_files_folder}/REQ_reference_measurement_units.txt
	#${req_files_folder}/REQ_replacement_headers.txt #No longer needed 20210709
	${req_files_folder}/REQ_site-specific_values_substitution_utf.txt
	${req_files_folder}/REQ_species_codes.txt
	${req_files_folder}/REQ_tree_status_and_allowed_observations.txt
	${req_files_folder}/REQ_units_conversion_table.txt
	${req_files_folder}/REQ_values_to_remove.txt
	#${req_files_folder}/script_insert_wholesite.txt No longer needed (10/2/2015)
	${req_files_folder}/SQL_count_records.txt
	${req_files_folder}/SQL_extract_data_for_validation.txt
	${req_files_folder}/SQL_extract_observation_codes.txt
	${req_files_folder}/SQL_OTHER_COMMANDS_EXAMPLE.txt
	${req_files_folder}/uniformize_col_no.txt
	${req_files_folder}/validate_column.txt
	${req_files_folder}/validate_columns_doc.txt
	${req_files_folder}/xtraksites132.156.208.30.txt
	)

#exit


#Column skipping character
skip_char="-"

#Change internal field separator
old_ifs=$IFS
IFS='
'

#Colors
red_color="\033[1;31m"
green_color="\033[1;32m"
blue_color="\033[1;34m"
normal_color="\033[0m"

#Reject observations when tree status abnormal (e.g. dead/cut/unknown filler tree)
#	For CFS: coded observations would be rejected when E <> 1
#	For MRNF: coded observations would be rejected when QcE is not in 1,2,3,4,5,8,9

#Now specified in the parameters
#reject_obs_when_abnormal_tree_status=2 # 1=yes, 2=no. NEW FORM 8/3/2012
#	Note (13/1/2015): 2 is the default value.


#------------------------------------------------------------------------------------------------------------------------------------------------------------
#Functions
#------------------------------------------------------------------------------------------------------------------------------------------------------------


function disable_trigger {
#--------------------------------------------------------------------------------------------------------------------------------------------------------------
#Deactivate the trigger trg_germplasms_on_sites_03
#Implemented on 11/3/2014
#AS OF 13/1/2015, NO LONGER USED.
	
	#Check for missing parameters
	if [ -z ${1} ] || [ -z ${2} ] || [ -z ${3} ]
	then #Missing parameters
		printf "${red_color}Invalid database, table or trigger name!\n\tNO TRIGGER DEACTIVATION!${normal_color}\n"

	else #All parameters present, proceed
		#NOTE: Eventually, check that all names are valid before proceeding.
	
		#parameter #1 = database name
		trg_database_name=${1}
		#parameter #2 = table name
		trg_table=${2}
		#parameter #3 = trigger name
		trigger_name=${3}
		
		#Need to connect with postgres for that
		PGUSER="postgres"
		PGPASSWORD=$(cat ${postgres_pw_cwfc_server_path})
		export PGUSER PGPASSWORD
		
		#This section is no longer used:
		
			#printf "Reading ${trigger_name} status in ${trg_database_name}...\n"
			#trigger_status=$(psql -d ${trg_database_name} -t -A -c "select tgenabled from pg_trigger WHERE tgname = '${trigger_name}'")

			#echo $trigger_status
			#if [ ! -z "$trigger_status" ]
			#then
			#	if [ $trigger_status == "O" ]; 
			#	then 
			#		printf "\tTrigger exists and is enabled.\n\n"
					
			#		printf "Disabling trigger..."
					
			#		psql -d ${trg_database_name}  -c "ALTER TABLE ${trg_table} DISABLE TRIGGER ${trigger_name}" > /dev/null
			#		printf "done.\n\n"
			#		trigger_status="D"
			#	else 
			#		printf "\tTrigger exists and is ALREADY DISABLED.\n"
			#	fi
					
			#else
			#	printf "\tTrigger DOES NOT exist!\n"
			#fi

		#Return to previous user
		PGUSER="xtraksites"
		PGPASSWORD=$(cat ${xtraksites_pw_cwfc_server_path})
		export PGUSER PGPASSWORD
	fi
}


function enable_trigger {
#--------------------------------------------------------------------------------------------------------------------------------------------------------------
#Reactivate the trigger trg_germplasms_on_sites_03
#Implemented on 11/3/2014
#***ATTENTION***: the UPDATE part is HARDCODED for germplasms_on_sites... need to find a way to fire that trigger without a DML...
#Also, trigger enabling and activating are two separate parts. Should have an on/off option for activating the trigger.
#AS OF 13/1/2015, NO LONGER USED.


	#Check for missing parameters
	if [ -z ${1} ] || [ -z ${2} ] || [ -z ${3} ]
	then #Missing parameters
		printf "${red_color}Invalid database, table or trigger name!\n\tNO TRIGGER ACTIVATION!${normal_color}\n"

	else #All parameters present, proceed
		#NOTE: Eventually, check that all names are valid before proceeding.
	
		#parameter #1 = database name
		trg_database_name=${1}
		#parameter #2 = table name
		trg_table=${2}
		#parameter #3 = trigger name
		trigger_name=${3}
	
		PGUSER="postgres"
		PGPASSWORD=$(cat ${postgres_pw_cwfc_server_path})
		export PGUSER PGPASSWORD

		#STATUS WILL HAVE TO BE CHECKED EACH TIME FROM THE DB (AS FOR THE disable_trigger function),  NOT ASSUMED FROM A VARIABLE
			#This section is no longer needed
			#if [ $trigger_status == "D" ]
			#then
			#	printf "Enabling ${trigger_name} in ${trg_database_name}..."
			#	psql -d ${trg_database_name}  -c "ALTER TABLE ${trg_table} ENABLE TRIGGER ${trigger_name}" > /dev/null
			#	printf "done.\n\n"
			#	trigger_status="O"
			#	printf "Executing ${trigger_name} in ${trg_database_name}..."
			#	psql -d ${trg_database_name}  -c "UPDATE germplasms_on_sites SET replicated_compartment_number=1 WHERE replicated_compartment_number=1" > /dev/null
			#	printf "done.\n\n"
			#	
			#fi

		#Return to previous user
		PGUSER="xtraksites"
		PGPASSWORD=$(cat ${xtraksites_pw_cwfc_server_path})
		export PGUSER PGPASSWORD
	fi
}

#For tests
	#disable_trigger ts_d germplasms_on_sites trg_germplasms_on_sites_03
	#echo $trigger_status

	#echo "ENABLE"
	#enable_trigger ts_d germplasms_on_sites trg_germplasms_on_sites_03
	#echo $trigger_status

	#exit


function fnct_clean_values {
#--------------------------------------------------------------------------------------------------------------------------------------------------------------
#Function to clean values of a single column file: 1) remove leading/trailing spaces, 2) replace or not 0 with NULL, 3)replace empty values with NULL
#Parameter 1: infile/outfile (with filepath relative to current directory).
#Parameter 2: ignore zeros. 1 = yes (zeros will be converted to NULLs), 2 = no

	in_outfile=${1}
	ignore_zeros=${2}
	if [ ${ignore_zeros} -eq 1 ]
	then
		search_string="^0$|^$"
	else
		search_string="^$"
	fi
	#Remove leading/trailing spaces
	#If ignore_zeros=1 (yes), replace zeros with NULLs
	#Replace empty values with NULLs
	#Overwrite the original file with the cleaned one
	dos2unix ${in_outfile} 2> /dev/null
	cat ${in_outfile}|perl -pe 's/^ +//g'|perl -pe 's/ +$//g'|perl -pe "s/${search_string}/NULL/g" > ${temp_f}/temp_cleaned_values.txt
	mv ${temp_f}/temp_cleaned_values.txt ${in_outfile}
}


function fnct_replace_headers {
#--------------------------------------------------------------------------------------------------------------------------------------------------------------
#Function to replace script headers (INFO_TYPE) with those of the TreeSource database
#Parameter 1: infile (with path relative to current directory). The infile contains original header + data
#Parameter 2: outfile (with path relative to current directory). The outfile will contain new header + data
#Parameter 3: destination table

	#Get header of infile ${1}, and transform header names from columns to lines
	#Number lines, put header name in 1st column, number in 2nd column, sort by header name.
	#Join INFO_TYPE header names from infile with those from the database in replacement_headers.txt, for the specified table ${3}
	#Remove INFO_TYPE headers (column 1)
	#Sort by column order and keep only final header names from the database
	#Replace them as columns
	#Put back the data from infile ${1} under them
	#Send that to outfile ${2}
	
	head -1 ${1}|perl -pe 's/\t/\n/g'|cat -n|awk -F "\t" '{print $2"\t"$1}'|sort -t $'\t' -k1,1|join -t $'\t' - <(awk -v awk_table_name=${3} -F "\t" '{if ($3 == awk_table_name) print $1"\t"$2}' ${temp_f}/replacement_headers.txt|sort -t $'\t' -k1,1)|cut -f1 --complement|sort -k1,1n|cut -f2|perl -pe 's/\n/\t/g'|perl -pe 's/\t$/\n/g'|cat - <(tail -n+2 ${1}) > ${2}
	#Test with reali filenames: head -1 ${temp_f}/temp_gos_main3.txt|perl -pe 's/\t/\n/g'|cat -n|awk -F "\t" '{print $2"\t"$1}'|sort -t $'\t' -k1,1|join -t $'\t' - <(awk -v awk_table_name="germplasms_on_sites" -F "\t" '{if ($3 == awk_table_name) print $1"\t"$2}' ${temp_f}/replacement_headers.txt|sort -t $'\t' -k1,1)|cut -f1 --complement|sort -k1,1n|cut -f2|perl -pe 's/\n/\t/g'|perl -pe 's/\t$/\n/g'|cat - <(tail -n+2 ${temp_f}/temp_gos_main3.txt)
	

	#Line before 20210709head -1 ${1}|perl -pe 's/\t/\n/g'|cat -n|awk -F "\t" '{print $2"\t"$1}'|sort -t $'\t' -k1,1|join -t $'\t' - <(grep ${3} ${req_files_folder}/REQ_replacement_headers.txt |cut -f2,3|sort -t $'\t' -k1,1)|sort -t $'\t' -k2,2n|cut -f3|perl -pe 's/\n/\t/g'|perl -pe 's/\t$/\n/g'|cat - <(tail -n+2 ${1}) > ${2}
	
}


function fnct_aborting {
#--------------------------------------------------------------------------------------------------------------------------------------------------------------
	printf "${red_color}\n======================================================"
	printf "\nPLEASE CHECK AND CORRECT THE FILE ${infile}\n"
	printf "\n***ABORTING***\n"
	printf "======================================================\n\n${normal_color}"
	exit
}


function fnct_clean_FMQTRT_files {
#--------------------------------------------------------------------------------------------------------------------------------------------------------------
#Function to clean only F.txt, D.txt, QT.txt and RT.txt
#The main task performed is to:
	#1. Warn when there are observations despite an abnormal tree status code (e.g. E > 1 for CFS observations)
	#2. Keep only observations for E=1, unless specified otherwise with the reject_obs_when_abnormal_tree_status variable *****************
	#3. Keep an observation = 0 only when no other information > 0 is present for that date. If so, keep only one replicate of the 0.
	#	i.e. multiple columns of an observation (e.g. F) for a single date, if all columns have 0, keep only one occurrence of 0.

	#F.txt
	#------
	#E292P1_01_05193_001_00417_00    1       0       1975-09-01    TRUE    123   99
	#E292P1_01_05193_002_00418_00    1       0       1975-09-01    TRUE    123   99
	#E292P1_01_05193_003_00419_00    1       2       1975-09-01    TRUE    123   99
	#E292P1_01_05193_004_00420_00    1       0       1975-09-01    TRUE    123   99
	#E292P1_01_05193_005_00421_00    1       0       1975-09-01    TRUE    123   99
	#E292P1_01_05193_006_00422_00    1       2       1975-09-01    TRUE    123   99
	#E292P1_01_05193_007_00423_00    1       0       1975-09-01    TRUE    123   99
	#E292P1_01_05193_008_00424_00    1       0       1975-09-01    TRUE    123   99
	#E292P1_01_05193_009_00425_00    1       0       1975-09-01    TRUE    123   99
	#E292P1_01_05193_010_00426_00    1       0       1975-09-01    TRUE    123   99
		#Colonne #2: E
		#Colonne #3: autre observation (ex: F, D, RT, QT)
	
	#${req_files_folder}/REQ_tree_status_and_allowed_observations.txt
	#These lines indicate for what tree status you can have other observations
		#E	0	(NONE)
		#E	1	(ALL)
		#QcE	1	QcD
		#QcE	2	(ALL)
		#QcE	3	(ALL)
		#QcE	4	(ALL)
		#QcE	5	QcD
		#QcE	6	(NONE)
		#QcE	7	(NONE)
		#QcE	8	(ALL)
		#QcE	9	(ALL)
	#Using this file, another file, specific to the current tree_status_obs_code (e.g. E or QcE) and observation (e.g. QcD), was created: ${temp_f}/${obs_file_allowed_tree_status}
	#Example:
		#QcD_allowed_tree_status.txt
		#1
		#2
		#3
		#4
		#5
		#8
		#9
	
	#Remove EMPTY values
	#Note: this avoids later errors caused by the script recognizing an empty value as a valid value (e.g. if no_msmt_value=0)
	awk -F "\t" '{if ($3 != "") print }' ${temp_f}/${obs_file} > ${temp_f}/temp_no_nulls.txt
	mv ${temp_f}/temp_no_nulls.txt ${temp_f}/${obs_file}
	
	

	#Extract into a file observations that present these errors: tree status is abnormal (e.g. E > 1 for CFS), but still some readings <> 0 for F, D, QT or RT.
	if [ $reject_obs_when_abnormal_tree_status -eq 1 ] 
	then #List errors if tree status is abnormal (e.g. E <> 1  for CFS)
		#1) Join the observation file and the list of tree status allowed to have other observations, each one by the tree status column
		#	NOTE: the join is exclusive to file 1 (with -v 1): it lists the abnormal tree status, where other observations are usually NOT allowed. 
		#		For example, it will list E values corresponding to dead/missing trees
		#2) For the lines kept, show only those that have observations with a non null value (e.g. F=4)
		#The result is that all the lines with an abnormal tree status that still have other observations will be listed. THESE ARE NOT SUPPOSED TO HAPPEN.
		join -t $'\t' <(paste <(cut -f2 ${temp_f}/${obs_file}) ${temp_f}/${obs_file}|sort -t $'\t' -k1,1) ${temp_f}/${obs_file_allowed_tree_status} -v1|cut -f1 --complement|awk -v awk_no_msmt_value=${no_msmt_value} -F "\t" '{if ($3 != awk_no_msmt_value) print }' > ${temp_f}/${obs_file_saved_errors}

		#F_saved_errors.txt
		#----------------------
		#E292P1_01_05202_004_00212_00    7       2       1998-09-01    TRUE    123   99
		#E292P1_03_05218_001_01366_00    2       2       1998-09-01    TRUE    123   99
		#E292P1_04_05201_001_02029_00    7       2       1998-09-01    TRUE    123   99
		#E292P1_01_05193_001_00417_00    7       2       1998-09-01    TRUE    123   99
		#E292P1_01_05193_012_00428_00    7       2       1998-09-01    TRUE    123   99
		#E292P1_01_05194_004_00238_00    7       2       1998-09-01    TRUE    123   99
		#E292P1_01_05194_005_00239_00    7       2       1998-09-01    TRUE    123   99
		#E292P1_01_05194_007_00241_00    7       2       1998-09-01    TRUE    123   99
		#E292P1_01_05194_010_00244_00    7       2       1998-09-01    TRUE    123   99
		#E292P1_01_05194_013_00247_00    7       2       1998-09-01    TRUE    123   99
		#Col. #1: tree name
		#Col. #2:  tree status
		#Col. #3: observation code
		#Col. #4: date
		#Col. #5: date is approximate
		#Col. #6: team_id
		#
		
		#NOTE: 	EMPTY VALUES for E will end up in that list as well, because they are not part of the allowed observations list (cat ${temp_f}/${obs_file_allowed_tree_status})
		#		EMPTY VALUES for D,F, QT, RT, etc. will only end up in that list if their associated E value is not allowed.
	
		#If there were errors, prepare a SQL script to insert a comment for each error:
			
			if [ -s ${temp_f}/${obs_file_saved_errors} ]
			then 
				#Temporary file 1:  get the observation code
				join -t $'\t' <(grep -v '^#' ${req_files_folder}/REQ_obs_codes_conv_table2_sel_codes.txt |grep "^${obs_type}[[:space:]]"|cut -f2,3|sort -t $'\t' -k1,1) <(awk -F "\t" '{print $3"\t"$0}' ${temp_f}/${obs_file_saved_errors}|sort -t $'\t' -k1,1)|cut -f1 --complement|awk -F "\t" '{print $3"\t"$0}'|sort -t $'\t' -k1,1 > ${temp_f}/saved_error_SQL_script_temp1.txt
				#Line before 30/8/2013: join -t $'\t' <(grep -v '^#' ${req_files_folder}/REQ_obs_codes_conv_table.txt |grep "^${obs_type}[[:space:]]"|cut -f2,3|sort -k1,1) <(awk -F "\t" '{print $3"\t"$0}' ${temp_f}/${obs_file_saved_errors}|sort -k1,1)|cut -f1 --complement|awk -F "\t" '{print $3"\t"$0}'|sort -k1,1 > ${temp_f}/saved_error_SQL_script_temp1.txt
				#3	F_1	E970A_01_02326_003_00063_00	3	1	2000-09-01	TRUE	NULL	14
				
				#Temporary file 2:  add the tree status code
				join -t $'\t' <(grep -v '^#' ${req_files_folder}/REQ_obs_codes_conv_table2_sel_codes.txt |grep "^${tree_status_obs_code}[[:space:]]"|cut -f2,3|sort -t $'\t' -k1,1) ${temp_f}/saved_error_SQL_script_temp1.txt|cut -f1 --complement > ${temp_f}/saved_error_SQL_script_temp2.txt
				#Line before 30/8/2013: join -t $'\t' <(grep -v '^#' ${req_files_folder}/REQ_obs_codes_conv_table.txt |grep "^${tree_status_obs_code}[[:space:]]"|cut -f2,3|sort -k1,1) ${temp_f}/saved_error_SQL_script_temp1.txt|cut -f1 --complement > ${temp_f}/saved_error_SQL_script_temp2.txt
				#E03	F_1	E970A_01_02326_003_00063_00	3	1	2000-09-01	TRUE	NULL	143

				#Create the observation type-specific SQL script
				#Line before 30/8/2013: awk -F "\t" '{print "\tINSERT INTO comments_on_germplasms(germplasm_name,date,date_is_approximate,comment,context,team_code,data_source_id) SELECT '\''"$3"'\'',now(),FALSE,'\''Attention: une observation "$2" ('\''||(SELECT description FROM observation_codes WHERE code='\''"$2"'\'')||'\'') a Ã©tÃ© faite le "$6", en dÃ©pit d'\'''\''un statut "$1" ('\''||(SELECT description FROM observation_codes WHERE code='\''"$1"'\'')||'\''). Cette observation n'\'''\''a pas Ã©tÃ© intÃ©grÃ©e Ã  la base de donnÃ©es.'\'','\''Erreur des donnÃ©es'\'','\''Sébastien Clément'\'',"$9";"}' ${temp_f}/saved_error_SQL_script_temp2.txt|cat <(echo "--Errors for ${obs_type} observation codes:") - > ${temp_f}/SQL_insert_saved_errors_${obs_type}.txt
				#***TEAMCODE_ok_20150107***
				awk -F "\t" '{print "\tINSERT INTO comments_on_germplasms(germplasm_id,date,date_is_approximate,comment,context,team_id,data_source_id) SELECT fnct_get_germplasm_id_from_refname('\''"$3"'\''),now(),FALSE,'\''Attention: une observation "$2" ('\''||(SELECT description FROM observation_codes WHERE code='\''"$2"'\'')||'\'') a Ã©tÃ© faite le "$6", en dÃ©pit d'\'''\''un statut "$1" ('\''||(SELECT description FROM observation_codes WHERE code='\''"$1"'\'')||'\''). Cette observation n'\'''\''a pas Ã©tÃ© intÃ©grÃ©e Ã  la base de donnÃ©es.'\'','\''Erreur des donnÃ©es'\'',fnct_get_team_from_employees('\''SÃ©bastien ClÃ©ment'\''),"$9";"}' ${temp_f}/saved_error_SQL_script_temp2.txt|cat <(echo "--Errors for ${obs_type} observation codes:") - > ${temp_f}/SQL_insert_saved_errors_${obs_type}.txt
			
			fi

	else #Do not list errors, because any value of E is accepted
		printf ""  > ${temp_f}/${obs_file_saved_errors}
	fi
		
	#WARN THAT THESE ERRORS EXIST AND THAT THEY WERE SAVED IN A FILE.  ALSO INDICATE THE EXISTENCE OF THE SQL SCRIPT TO KEEP THEM INTO THE DATABASE.
	if [ -s ${temp_f}/${obs_file_saved_errors} ]
	then
		printf "\n\tWARNING: there are lines in the original file where coded observations\n\twere recorded for the \"${obs_type}\" column despite an abnormal tree status (E <> $(perl -pe 's/\n/,/g' ${temp_f}/${obs_file_allowed_tree_status}|perl -pe 's/,$/\n/g'))\n"
		if [ $reject_obs_when_abnormal_tree_status -eq 1 ]
		then 
			printf "\tThese lines will be removed from the observations, but kept in ${temp_f}/${obs_file_saved_errors}.\n"
			printf "\tNOTE: SQL commands to comment those errors into the database will appear in ${ready2import_tabs_folder}/SQL_insert_and_update2.txt\n"
			
		else 
			printf "The lines will be kept in the final observations.\n"
		fi
	fi

	#Create a temporary file with a concatenation of germplasm_name and date, for each record having a normal tree status (e.g. E=1 for CFS) and non-null other observations (e.g. F, D, QT or RT >0 for CFS)
	#This will serve to mark dates where there was at least 1 non-zero observation for F, D, QT or RT
	if [ $reject_obs_when_abnormal_tree_status -eq 1 ] 
	then #Keep only observations when tree status is normal (e.g. E = 1 for CFS, QcE = 1,2,3,4,5,8,9 for MRNF)
		join -t $'\t' <(paste <(cut -f2 ${temp_f}/${obs_file}) ${temp_f}/${obs_file}|sort -t $'\t' -k1,1) ${temp_f}/${obs_file_allowed_tree_status} |cut -f1 --complement|awk -v awk_no_msmt_value=${no_msmt_value} -F "\t" '{if ($3 != awk_no_msmt_value) print $1"_"$4"\tY"}'|sort -u > ${temp_f}/${obs_file_with_meas}
	
	else #Keep observations for any value of E
		awk -v awk_no_msmt_value=${no_msmt_value} -F "\t" '{if ($3 != awk_no_msmt_value) print $1"_"$4"\tY"}' ${temp_f}/${obs_file}| sort -u > ${temp_f}/${obs_file_with_meas}
	fi
		#F_with_meas.txt
		#-------------------
		#E292P1_01_05193_001_00417_00_1986-09-01       Y
		#E292P1_01_05193_003_00419_00_1975-09-01       Y
		#E292P1_01_05193_003_00419_00_1981-09-01       Y
		#E292P1_01_05193_006_00422_00_1975-09-01       Y
		#E292P1_01_05193_006_00422_00_1981-09-01       Y
		#E292P1_01_05193_006_00422_00_1986-09-01       Y
		#E292P1_01_05193_006_00422_00_1998-09-01       Y
		#E292P1_01_05193_009_00425_00_1981-09-01       Y
		#E292P1_01_05193_012_00428_00_1975-09-01       Y
		#E292P1_01_05193_012_00428_00_1981-09-01       Y

	#Keep only observations where tree status code is either 1) normal 2) any code, and join with the markers above. Add "N" at the end where no observation is <> 0 for any given date.
	if [ $reject_obs_when_abnormal_tree_status -eq 1 ] 	
	then #keep only observations when tree status is normal (e.g. E = 1 for CFS)
		#1) list all observations where E is a normal status (join)
		#2) create a key with individual name and date
		#3) left join with the same key created above (e.g. F_with_meas.txt), with the file containing "Y" when there is at least 1 non-zero other coded observation. Fill empty values in the "Y"-column with "N"
		join -t $'\t' <(paste <(cut -f2 ${temp_f}/${obs_file}) ${temp_f}/${obs_file}|sort -t $'\t' -k1,1) ${temp_f}/${obs_file_allowed_tree_status}|cut -f1 --complement|awk -F "\t" '{print $1"_"$4"\t"$0}'|sort -t $'\t' -k1,1|join -t $'\t' - <(sort -t $'\t' -k1,1 ${temp_f}/${obs_file_with_meas} ) -a 1|cut -f1 --complement|perl -pe 's/([^Y])\n/\1\tN\n/g'  > ${temp_f}/${temp_fileA}
	
	else #keep observations for any VALID value of tree status (e.g. E= 1-9 for CFS)
		#Line before: 12/3/2012: join -t $'\t' <(paste <(cut -f2 ${temp_f}/${obs_file}) ${temp_f}/${obs_file}|sort -k1,1) <(grep "^${tree_status_obs_code}[[:space:]]" ${req_files_folder}/REQ_obs_codes_conv_table.txt|cut -f2|sort -u)|cut -f1 --complement|awk -F "\t" '{print $1"_"$4"\t"$0}'|sort -k1,1|join -t $'\t' - <(sort -k1,1 ${temp_f}/${obs_file_with_meas} ) -a 1|cut -f1 --complement|perl -pe 's/([^Y])\n/\1\tN\n/g'|awk -F "\t" '{if ($2 !="1") print }' > ${temp_f}/${temp_fileA}
		#Line before: 30/8/2013: join -t $'\t' <(paste <(cut -f2 ${temp_f}/${obs_file}) ${temp_f}/${obs_file}|sort -k1,1) <(grep "^${tree_status_obs_code}[[:space:]]" ${req_files_folder}/REQ_obs_codes_conv_table.txt|cut -f2|sort -u)|cut -f1 --complement|awk -F "\t" '{print $1"_"$4"\t"$0}'|sort -k1,1|join -t $'\t' - <(sort -k1,1 ${temp_f}/${obs_file_with_meas} ) -a 1|cut -f1 --complement|perl -pe 's/([^Y])\n/\1\tN\n/g' > ${temp_f}/${temp_fileA}
		join -t $'\t' <(paste <(cut -f2 ${temp_f}/${obs_file}) ${temp_f}/${obs_file}|sort -t $'\t' -k1,1) <(grep "^${tree_status_obs_code}[[:space:]]" ${req_files_folder}/REQ_obs_codes_conv_table2_sel_codes.txt|cut -f2|sort -u)|cut -f1 --complement|awk -F "\t" '{print $1"_"$4"\t"$0}'|sort -t $'\t' -k1,1|join -t $'\t' - <(sort -t $'\t' -k1,1 ${temp_f}/${obs_file_with_meas} ) -a 1|cut -f1 --complement|perl -pe 's/([^Y])\n/\1\tN\n/g' > ${temp_f}/${temp_fileA}
	
	fi
	
		#F_tempA_non0_marked.txt
		#------------------------------
		#E292P1_01_05193_001_00417_00    1       0       1975-09-01    TRUE    123   99      N
		#E292P1_01_05193_001_00417_00    1       0       1975-09-01    TRUE    123   99      N
		#E292P1_01_05193_001_00417_00    1       0       1975-09-01    TRUE    123   99      N
		#E292P1_01_05193_001_00417_00    1       0       1981-09-01    TRUE    123   99      N
		#E292P1_01_05193_001_00417_00    1       0       1981-09-01    TRUE    123   99      N
		#E292P1_01_05193_001_00417_00    1       0       1986-09-01    TRUE    123   99      Y
		#E292P1_01_05193_001_00417_00    1       2       1986-09-01    TRUE    123   99      Y
		#E292P1_01_05193_002_00418_00    1       0       1975-09-01    TRUE    123   99      N
		#E292P1_01_05193_002_00418_00    1       0       1975-09-01    TRUE    123   99      N
		#E292P1_01_05193_002_00418_00    1       0       1975-09-01    TRUE    123   99      N
			#All lines with abnormal tree status (e.g. E <> 1 for CFS) are removed (E is column #2)
			#The "Y" is added when there is at least one non-zero reading in the observation (column #3). This is done via a join with F_with_meas.txt



	#Keep only unique lines where:
		#1) observation was recorded (any value above 0)
		#2) observation = 0 and there are no observations <> 0 for the same date
			#Effect: all lines where observation = 0 and there is at least one observation <> 0 for the same date will be removed
	sort -t $'\t' -k1,1 -k4,4 -k3,3 -u ${temp_f}/${temp_fileA}|awk -F "\t" -v awk_no_msmt_value=${no_msmt_value} '{if ($3 != awk_no_msmt_value || ( $3 == awk_no_msmt_value && $8 == "N")) print }' > ${temp_f}/${temp_fileB}

	#F_tempB_0s_removed.txt
		#-----------------------------
		#E292P1_01_05193_001_00417_00    1       0       1975-09-01    TRUE    123   99      N
		#E292P1_01_05193_001_00417_00    1       0       1981-09-01    TRUE    123   99      N
		#E292P1_01_05193_001_00417_00    1       2       1986-09-01    TRUE    123   99      Y
		#E292P1_01_05193_002_00418_00    1       0       1975-09-01    TRUE    123   99      N
		#E292P1_01_05193_002_00418_00    1       0       1981-09-01    TRUE    123   99      N
		#E292P1_01_05193_002_00418_00    1       0       1986-09-01    TRUE    123   99      N
		#E292P1_01_05193_003_00419_00    1       2       1975-09-01    TRUE    123   99      Y
		#E292P1_01_05193_003_00419_00    1       2       1981-09-01    TRUE    123   99      Y
		#E292P1_01_05193_003_00419_00    1       0       1986-09-01    TRUE    123   99      N
		#E292P1_01_05193_003_00419_00    1       0       1998-09-01    TRUE    123   99      N
			#Only one value per date (i.e. only one 0 is kept instead of 3)
			#All trees with at least 1 observation <> 0 for a given date have their observations=0 removed for that date
}


function fnct_auto_insert_all_in_db {
#--------------------------------------------------------------------------------------------------------------------------------------------------------------
#Function to automatically insert values into the specified database
#Parameter 1: database number
#	1. ts_d
#	2. ts_pi

case $1 in
	"1")
		database_name="ts_d"
		;;
	"2")
		database_name="ts_pi"
		;;
	"3")
		database_name="TOBEDEFINED"
		;;
	*)
		echo "UNKNOWN DATABASE"
		echo "ABORT"
		exit
		;;
esac


	#ASK USER TO CHECK AND EVENTUALLY CORRECT THE FILES
	echo
	printf "${blue_color}PLEASE check the SQL scripts under '${ready2import_tabs_folder}' and make any necessary corrections (diacritics, incomplete data, etc.).\n\tWhen done press 'y'. To abort script press 'n'.\n${normal_color}"
	sql_check_choice="-";while [ ! $(echo $sql_check_choice|grep '[Yn]') ];do printf "\r\t"; read -p "[Y/n]?" -n1 -s sql_check_choice;done;printf "\n"
	if [ ${sql_check_choice} == "n" ];then printf "${red_color}\n***ABORTING***\n${normal_color}";exit;fi
	
	#Convert SQL files encoding to utf-8 when necessary
	#Check file encoding for the SQL scripts
	printf "Converting SQL scripts to UTF-8"
	
	for sql_file in $(ls -1 ${ready2import_tabs_folder}|grep '^SQL')
	do 
		printf "."
		
		
		fnct_convert_to_utf8 ${ready2import_tabs_folder}/${sql_file}
		
		#Lines below before 9/4/2013:
			#orig_encoding=$(file -bi ${ready2import_tabs_folder}/${sql_file}|perl -pe 's/.+charset=(.+)/\1/g')
			#if not utf-8 (i.e. ASCII ou iso-8859-1), convert to utf-8. Note: Ascii will remain ascii even after converion by iconv, but won't cause problem with insertion into the database.
			#if [ -z $(echo $orig_encoding|grep -i 'utf-8') ] #if not utf-8
			#then
				#iconv -f ${orig_encoding} -t utf-8 ${ready2import_tabs_folder}/${sql_file} > ${ready2import_tabs_folder}/temp.txt
				#mv ${ready2import_tabs_folder}/temp.txt ${ready2import_tabs_folder}/${sql_file}
			#fi
	
	
	
	done
	printf "done.\n\n"
	
	#Calculate stats before
	printf "Calculating stats before insertion..."
	psql -d ${database_name} -A -F "___" -t -f "${temp_f}/SQL_count_records.txt" |perl -pe 's/___/\t/g' > ${temp_f}/records_count_before_insertion.txt
	dos2unix ${temp_f}/records_count_before_insertion.txt 2> /dev/null
	printf "done.\n\n"
	
	#A. Run SQL script 1
	printf "A. Executing the first SQL script: ${ready2import_tabs_folder}/SQL_insert_and_update1.txt ..."
	psql -d ${database_name} -f "${ready2import_tabs_folder}/SQL_insert_and_update1.txt" > ${temp_f}/SQL_insert_and_update1_run_log.txt 2>&1
		#If there are errors in the execution log
		if [ $(grep 'ERROR' ${temp_f}/SQL_insert_and_update1_run_log.txt|wc -l) -gt 0 ]
		then
			printf "${red_color}ERROR(S)!\n${normal_color}"
			printf "${red_color}\t\tThere were errors during insertion or update! Here is the first one:\n${normal_color}"
			grep -A 2 -m 1 'ERROR' ${temp_f}/SQL_insert_and_update1_run_log.txt|perl -pe 's/^/\t\t\t/g'
			echo
			printf "${red_color}PLEASE, check the file '${temp_f}/SQL_insert_and_update1_run_log.txt' and press 'y' to ignore the error and continue or 'n' to abort.\n${normal_color}"
			
			choice="-";while [ ! $(echo $choice|grep '[yn]') ];do printf "\r\t"; read -p "[y/n]?" -n1 -s choice;done;printf "\n"
			if [ ${choice} == "n" ];then printf "${red_color}\n***ABORTING***\n${normal_color}";exit
			fi
		fi
	printf "ok!\n"
	echo
	
	
	printf "B. inserting data in tabulated files:\n"
	#Insert data into the specified database
	#Change from 10/2/2015: no longer calling external script script_insert_wholesite.txt, but rather an inscript function, fnct_auto_insert_main_tab_files , which does the same thing.
	
	#Lines before 10/2/2015
		#Parameters:
		#1. ${temp_f}/psql_commands_2insert_DEV_AUTO.txt: file containing the psql commands, 1 per table
		#2. ${ready2import_tabs_folder}: folder where tabulated files are
		#3. 1=ts_d 2=ts_pi
		#	bash ${req_files_folder}/script_insert_wholesite.txt ${temp_f}/psql_commands_2insert_DEV_AUTO.txt ${ready2import_tabs_folder} $1
	
	#Call the function to insert tabulated files (10/2/2015)
		fnct_auto_insert_main_tab_files
	
	
	
	#If the previous script returned an error during the last tab file insertion, stop here
	if [ $(echo $?) -eq 99 ]
	then
		printf "${red_color}THERE WERE ERRORS DURING INSERTION.\nSCRIPT ABORTED${normal_color}\n\n"
		exit
	fi

	#Insertion succeeded
	printf "ok!\n"
	echo
	
	printf "C. Executing the second SQL script: ${ready2import_tabs_folder}/SQL_insert_and_update2.txt ..."
			psql -d ${database_name} -f "${ready2import_tabs_folder}/SQL_insert_and_update2.txt" > ${temp_f}/SQL_insert_and_update2_run_log.txt 2>&1
		#If there are errors in the execution log
		if [ $(grep 'ERROR' ${temp_f}/SQL_insert_and_update2_run_log.txt|wc -l) -gt 0 ]
		then
			printf "${red_color}ERROR(S)!\n${normal_color}"
			printf "${red_color}\t\tThere were errors during insertion or update! Here is the first one:\n${normal_color}"
			grep -A 2 -m 1 'ERROR' ${temp_f}/SQL_insert_and_update2_run_log.txt|perl -pe 's/^/\t\t\t/g'
			echo
			printf "${red_color}PLEASE, check the file '${temp_f}/SQL_insert_and_update2_run_log.txt' and press 'y' to ignore the error and continue or 'n' to abort.\n${normal_color}"
			
			choice="-";while [ ! $(echo $choice|grep '[yn]') ];do printf "\r\t"; read -p "[y/n]?" -n1 -s choice;done;printf "\n"
			if [ ${choice} == "n" ]
			then
				printf "${red_color}\n***ABORTING***\n${normal_color}"
				exit
			fi
		fi
	printf "ok!\n"
	echo


	printf "D. Executing the third SQL script: ${ready2import_tabs_folder}/SQL_insert_and_update3.txt ..."
			psql -d ${database_name} -f "${ready2import_tabs_folder}/SQL_insert_and_update3.txt" > ${temp_f}/SQL_insert_and_update3_run_log.txt 2>&1
		#If there are errors in the execution log
		if [ $(grep 'ERROR' ${temp_f}/SQL_insert_and_update3_run_log.txt|wc -l) -gt 0 ]
		then
			printf "${red_color}ERROR(S)!\n${normal_color}"
			printf "${red_color}\t\tThere were errors during insertion or update! Here is the first one:\n${normal_color}"
			grep -A 2 -m 1 'ERROR' ${temp_f}/SQL_insert_and_update3_run_log.txt|perl -pe 's/^/\t\t\t/g'
			echo
			printf "${red_color}PLEASE, check the file '${temp_f}/SQL_insert_and_update3_run_log.txt' and press 'y' to ignore the error and continue or 'n' to abort.\n${normal_color}"
			
			choice="-";while [ ! $(echo $choice|grep '[yn]') ];do printf "\r\t"; read -p "[y/n]?" -n1 -s choice;done;printf "\n"
			if [ ${choice} == "n" ]
			then
				printf "${red_color}\n***ABORTING***\n${normal_color}"
				exit
			fi
		fi
	printf "ok!\n"
	echo


	
	
	#Calculate stats after
	printf "\n\tCalculating stats after insertion..."
	psql -d ${database_name} -A -F "___" -t -f "extract_whole_sites_temp/SQL_count_records.txt" |perl -pe 's/___/\t/g' > ${temp_f}/records_count_after_insertion.txt
	dos2unix ${temp_f}/records_count_after_insertion.txt 2> /dev/null
	printf "done.\n"
	
	printf "Insertion stats:\n----------------------\ntable\tlines before\tlines after\n"
	paste ${temp_f}/records_count_before_insertion.txt <(cut -f2 ${temp_f}/records_count_after_insertion.txt)|perl -pe 's/^/\t/g'
	echo

	#Extract data for validation
	printf "\nExtracting data for validation..."
	psql -d ${database_name} -A -F "___" -f "${temp_f}/SQL_extract_data_for_validation.txt" |perl -pe 's/___/\t/g' > ${ready2import_tabs_folder}/validation_data.txt
	dos2unix ${ready2import_tabs_folder}/validation_data.txt 2> /dev/null
	printf "done.\n"
	
	#Show only if insertion is currently done for development version
	if [ $1 -eq 1 ]
	then
		printf "${red_color}Please, compare data in the file ${ready2import_tabs_folder}/validation_data.txt to validate inserted data,\nwith data in the original Excel file,\nbefore inserting into the PRODUCTION version.\n${normal_color}"
		printf "Press \"y\" when comparison is done, or \"n\" to abort the script\n"
		choice="-";while [ ! $(echo $choice|grep '[yn]') ];do printf "\r\t"; read -p " [y/n]?" -n1 -s choice;done;printf "\n"

		if [ ${choice} == "n" ]
		then
			printf "${red_color}\n***ABORTING***\n${normal_color}"
			exit
		fi
	fi

}



function fnct_auto_insert_main_tab_files {
#--------------------------------------------------------------------------------------------------------------------------------------------------------------
#10/2/2015
#Function called by fnct_auto_insert_all_in_db above to automatically insert the main tabulated files for germplasms, germplasms_on_sites, coded_observations_on_germplasms, tree_mensurations
#Replaces the previously called outside script script_insert_wholesite.txt
#No parameters. The variables are inherited from the script.
		#Parameters of the formerly used script
			#psql_commands_filepath= ${temp_f}/psql_commands_2insert_DEV_AUTO.txt: file containing the psql commands, 1 per table
			#tabulated_data_folder= ${ready2import_tabs_folder}: folder where tabulated files are
			#database=1 ( 1=ts_d 2=ts_pi)
			#	bash ${req_files_folder}/script_insert_wholesite.txt ${temp_f}/psql_commands_2insert_DEV_AUTO.txt ${ready2import_tabs_folder} $1

printf "Automated insertion of tabulated data:\n-------------------------------------\n"
echo "Database: ${database_name}"

printf "psql scripts file: ${temp_f}/psql_commands_2insert_DEV_AUTO.txt\n"
echo

for table_and_psql_import_command in $(cat ${temp_f}/psql_commands_2insert_DEV_AUTO.txt)
do
	#Line before 10/2/2015: current_table=$(echo ${psql_import_command}|perl -pe 's/.+copy (\S+) .+/\1/g')	#pas très élégant d'extraire de la sorte. Il faudrait que ces chaînes soient dans le fichier d'origine... modifier ligne ~ 2393 de script_extract_wholesite.txt
	#Line before 10/2/2015: tabulated_file=$(echo $psql_import_command|perl -pe 's/.+___TABS_FOLDER___(\S+.txt).+/\1/g')	#Idem >:-B
	tabulated_file=$(echo ${table_and_psql_import_command}|cut -f1)
	current_table=$(echo ${table_and_psql_import_command}|cut -f2)
	psql_import_command=$(echo ${table_and_psql_import_command}|cut -f3)
	
	
	printf "Table: ${current_table}\n"
	printf "\tTabulated file: ${tabulated_file}\n"
	printf "\tINSERTION..."
	eval $(echo $psql_import_command|perl -pe "s/___DATABASE___/${database_name}/g"|perl -pe "s/___TABS_FOLDER___/${ready2import_tabs_folder}\//g") > ${temp_f}/${current_table}_${database_name}_insert_log.txt 2>&1
	
	#If there are errors in the log
	if [ $(grep 'ERROR' ${temp_f}/${current_table}_${database_name}_insert_log.txt|wc -l) -gt 0 ]
	then
		printf "${red_color}ERROR(S)!\n${normal_color}"
		printf "${red_color}\t\tThere were errors during insertion! Here is the first one:\n${normal_color}"
		grep -A 1 -m 1 'ERROR' ${temp_f}/${current_table}_${database_name}_insert_log.txt|perl -pe 's/^/\t\t\t/g'
		echo
		printf "${red_color}\tPLEASE, verify the file ${tabulated_file}, and retry.\n***ABORTING INSERTION***${normal_color}\n"
		exit 99 #This exit code can be obtained by whatever (script) that executed the current script, using echo $?
	else
		printf "done\n"
	
	fi

done











}


function fnct_convert_to_utf8 {
#--------------------------------------------------------------------------------------------------------------------------------------------------------------
#Function to automatically insert values into the specified database
#Parameter 1: infile complete path
#Parameter 2: outfile complete path. If missing, the modified file will replace the original file.
infilepath=${1}
outfilepath=${2}
#If only infilepath was provided, outfilepath will = infilepath
if [ -z ${2} ];then outfilepath=${1};fi

echo "Checking for encoding of ${infilepath}:"
orig_encoding=$(file -bi ${infilepath}|perl -pe 's/.+charset=(.+)/\1/g')
	#if not utf-8 (i.e. ASCII ou iso-8859-1), convert to utf-8. Note: Ascii will remain ascii even after converion by iconv, but won't cause problem with insertion into the database.
	if [ -z $(echo $orig_encoding|grep -i 'utf-8') ] #if not utf-8
	then
		printf "\tConversion of ${infilepath} from ${orig_encoding} to UTF-8..."
		iconv -f ${orig_encoding} -t utf-8 ${infilepath} > temp.txt
		mv temp.txt ${outfilepath}
		printf "done.\n"
	else
		printf "\tNo need to convert ${infilepath} (already encoded in UTF-8)\n"
		#Even without conversion, copy towards the outfilepath, if it is different from the infilepath
		if [ ${infilepath} != ${outfilepath} ]
		then
			cp ${infilepath} ${outfilepath}
		fi
	fi
}


function fnct_get_and_validate_site_parameters {
#--------------------------------------------------------------------------------------------------------------------------------------------------------------
#Function to get the parameter's value from infile_utf.txt and validate that it belongs to the right data type
param_status=${1}			#required/optional status of the parameter
param_name=${2}			#name of the parameter (e.g. site_code)
param_data_type=${3}		#parameter data type (e.g. date)
param_default_value=${4}	#parameter default value
int_param_min_value=${5}	#for integer data types, minimum value.
int_param_max_value=${6}	#for integer data types, maximum value.
parameters_file=${7}		#file containing the parameters
param_jfile=${8}					#file containing each site parameter's details

#echo
#echo "Parameters (function fnct_get_and_validate_site_parameters):"
#echo "============================================================"
#echo "param_status: ${param_status}"
#echo "param_name: ${param_name}"
#echo "param_data_type: ${param_data_type}" 
#echo "param_default_value: ${param_default_value}"
#echo "int_param_min_value: ${int_param_min_value}"
#echo "int_param_max_value: ${int_param_max_value}"
#echo "parameters_file: ${parameters_file}"
#echo "param_jfile: ${param_jfile}"

#Check validity of FUNCTION parameters (8 above)
	#Check if the required/optional status is specified, otherwise abort
	if [ -z $(echo ${param_status}|grep -Ei '^R$|^O$') ]
	then
		printf "\n\t\t${red_color}Incorrect or missing required/optional status for parameter \"${param_name}\"!\n"
		return 1	#Error in FUNRCTION PARAMETER. Will be aborted.
	fi

	#Check if parameter name is empty
	if [ -z $(echo ${param_name}|tr -d ' ') ] || [ "${param_name}" == "(unknown)" ]
	then
		printf "\n\t\t${red_color}Parameter name cannot be empty!\n"
		return 1	#Error in FUNRCTION PARAMETER. Will be aborted.
	fi

	#Check parameter data type
	if [ -z $(echo ${param_data_type}|tr -d ' ') ] || [ "${param_data_type}" == "(unknown)" ]
	then
		printf "\n\t\t${red_color}Parameter data type cannot be empty (${param_name})!${normal_color}\n"
		return 1	#Error in FUNRCTION PARAMETER. Will be aborted.
	fi

	#Check that a default value exists if parameter is optional
	if  ( [ -z $(echo ${param_default_value}|tr -d ' ') ] || [ "${param_default_value}" == "0" ] ) && [ "${param_status}" == "O" ]
	then
		printf "\n\t\t${red_color}Parameter default value cannot be empty for optional parameters (${param_name})!\n"
		return 1	#Error in FUNRCTION PARAMETER. Will be aborted.
	fi

#Put the required/optional status and parameter name in the journal file
printf "${param_status}\t${param_name}\t" >> ${param_jfile}


#Get the column number for current parameter
	parameter_col_no=$(grep "\b${param_name}\b" ${temp_f}/infile_utf.txt|perl -pe 's/\t/\n/g'|cat -n|grep "\b${param_name}\b"|cut -f1|tr -d ' ')
	#echo parameter_col_no $parameter_col_no

#If the parameter was found in the file
if [ ! -z ${parameter_col_no} ]
then
	#Parameter value is obtained by finding the value in the line just below (option -A1) the line of the parameter name, for the same column ($parameter_col_no).
	#Leading and trailing spaces are also removed automatically.
	parameter_value=$(grep -A1 "\b${param_name}\b" ${temp_f}/infile_utf.txt|cut -f${parameter_col_no}|tail -1|perl -pe 's/^ +//g'|perl -pe 's/ +$//g')
	#echo parameter_value ___${parameter_value}---

	#Test if parameter value is empty, after removal of all spaces.
	if [ -z $(echo $parameter_value|tr -d ' ') ]
	then
		printf "(empty)\t-\t" >> ${param_jfile}
		printf "\n\t\t${red_color}The parameter value for \"${param_name}\" CANNOT BE EMPTY!${normal_color}\n"
		return 2	#Error in SITE PARAMETER, will either be aborted if that parameter is required (R), or a default value will be put if that parameter is optional (O).
	fi

	#Put the value in the journal file
	printf "${parameter_value}\t" >> ${param_jfile}
	#Show onscreen
	printf "${blue_color}${parameter_value}${normal_color}..."

	#If a date, check if correctly formatted
	if [ ${param_data_type} == "date" ] && [ -z $(echo ${parameter_value}|grep -E '^[[:digit:]]{4}-[[:digit:]]{2}-[[:digit:]]{2}$') ]
	then
		printf "bad data type!\t" >> ${param_jfile}
		printf "\n\t\t${red_color}The parameter \"${param_name}\" MUST BE FORMATTED AS A DATE (YYYY-MM-DD)!\n\t\tACTUAL VALUE \"${parameter_value}\" IS INCORRECT.${normal_color}\n"
		return 2	#Error in SITE PARAMETER, will either be aborted if that parameter is required (R), or a default value will be put if that parameter is optional (O).
	fi

	#If an integer, check if correctly formatted
	if [ ${param_data_type} == "integer" ] 
	then
	
		#Check if an integer
		if [ -z $(echo ${parameter_value}|grep -E '^[[:digit:]]+$') ]
		then
			printf "bad data type!\t" >> ${param_jfile}
			printf "\n\t\t${red_color}The parameter \"${param_name}\" MUST BE AN INTEGER!\n\t\tACTUAL VALUE \"${parameter_value}\" IS INCORRECT.${normal_color}\n"
			return 2	#Error in SITE PARAMETER, will either be aborted if that parameter is required (R), or a default value will be put if that parameter is optional (O).
		else #it is an integer, check min/max values
			if [ ${parameter_value} -lt ${int_param_min_value} ] || [ ${parameter_value} -gt ${int_param_max_value} ] 
			then
				printf "bad range!\t" >> ${param_jfile}
				printf "\n\t\t${red_color}The parameter \"${param_name}\" MUST BE BETWEN ${int_param_min_value} AND ${int_param_max_value}!\n\t\tACTUAL VALUE \"${parameter_value}\" IS INCORRECT.${normal_color}\n"
				return 2	#Error in SITE PARAMETER, will either be aborted if that parameter is required (R), or a default value will be put if that parameter is optional (O).
			fi
		fi
	fi

	#If a boolean, check if correctly formatted
	if [ ${param_data_type} == "boolean" ] && [ -z $(echo ${parameter_value}|grep -Ei '^TRUE$|^FALSE$|^NULL$') ]
	then
		printf "bad data type!" >> ${param_jfile}
		printf "\n\t\t${red_color}The parameter \"${param_name}\" MUST BE A BOOLEAN (TRUE/FALSE/NULL)!\n\t\tACTUAL VALUE \"${parameter_value}\" IS INCORRECT.${normal_color}\n"
		return 2	#Error in SITE PARAMETER, will either be aborted if that parameter is required (R), or a default value will be put if that parameter is optional (O).
	fi

	#If a numeric, check if correctly formatted
	if [ ${param_data_type} == "numeric" ] && [ -z $(echo ${parameter_value}|grep -E '^[[:digit:]]+\.?[[:digit:]]*$') ]
	then
		printf "bad data type!" >> ${param_jfile}
		printf "\n\t\t${red_color}The parameter \"${param_name}\" MUST BE NUMERIC!\n\t\tACTUAL VALUE \"${parameter_value}\" IS INCORRECT.${normal_color}\n"
		return 2	#Error in SITE PARAMETER, will either be aborted if that parameter is required (R), or a default value will be put if that parameter is optional (O).
	fi

else
	printf "N/A\t(not found)\t" >> ${param_jfile}
	printf "\n\t\t${red_color}The parameter \"${param_name}\" couldn't be found in the file ${infile}!${normal_color}\n" 
	return 2	#Error in SITE PARAMETER, will either be aborted if that parameter is required (R), or a default value will be put if that parameter is optional (O).
fi

#Ok, everything found and conform.
printf "Ok.\t-\n" >> ${param_jfile}

}



function fnct_clean_filler_tree_famprovs {
#Function to cleanup and make unique the entries in the ${req_files_folder}/REQ_filler_tree_famprovs.txt reference file
#Basically, removes duplicate entries, spaces, and keeps the header at its place
	grep "^#" ${req_files_folder}/REQ_filler_tree_famprovs.txt > /dev/shm/saved_filler_tree_famprovs_header.txt
	grep -vE "^#|^$" ${req_files_folder}/REQ_filler_tree_famprovs.txt|sort -u > /dev/shm/tempf.txt
	cat /dev/shm/saved_filler_tree_famprovs_header.txt /dev/shm/tempf.txt > ${req_files_folder}/REQ_filler_tree_famprovs.txt
	rm /dev/shm/saved_filler_tree_famprovs_header.txt /dev/shm/tempf.txt

}






#==================================================================================================================================
#MAIN PROGRAM
#==================================================================================================================================


#echo ${journal_f}
#Start of capture in the log
{
#echo ${journal_f}
####################################################################
################################SKIP_TAG##############################
####################################################################
#THIS IS THE START OF THE CODE TO BE SKIPPED, IF skip=="yes"
if [ ${skip} == "no" ]
then
####################################################################
####################################################################
####################################################################


#------------------------------------------------------------------------------------------------------------------------------------------------------------
#1. Header
#------------------------------------------------------------------------------------------------------------------------------------------------------------
echo;echo
echo "================================================"
echo "Whole site data extraction script (${0})"
echo "By SÃ©bastien ClÃ©ment, Version: ${version}"
echo "Run: $(date)"
uname -a
echo "Site file: ${1}"
echo "================================================"


#------------------------------------------------------------------------------------------------------------------------------------------------------------
#2. Check for file dependencies
#------------------------------------------------------------------------------------------------------------------------------------------------------------
#Cycle through the required files array
printf "Checking for required files..."

#Get the maximum element position in the array, knowing that the first position is 0. 
#${#required_files[@]} represents the total number of elements in the array. 
#[new 20210707]
let max_element_number=$(echo ${#required_files[@]})-1

#line before 2021-07-07: for element in $(seq 0 $(echo "scale=0;${#required_files[@]}-1"|bc)). We got rid of bc because it is not installed by default on Cygwin.
for element in $(seq 0 $max_element_number)

do 
	#echo ${required_files[$element]}
	if [ ! -e ${required_files[$element]} ]
	then
		printf "${red_color}Required file ${required_files[$element]} could not be found!\n"
		printf "SCRIPT ABORTED\n\n${normal_color}"
		exit
	fi
	dos2unix ${required_files[$element]} 2> /dev/null	
done
printf "done.\n"

#exit #****************

#Make a copy of critical (those to which lines are regularly added) required files on the P
#Note: other can be found in P:\000_scripts\UNIX\EXTRACT_WHOLESITES\REQs_script_extract_wholesite20150114
if [ ${no_network} -eq 2 ] #13/7/2016: only if network is available.
then
	cp -f ${req_files_folder}/REQ_filler_tree_famprovs.txt ${scripts_path_shared}/bkps/REQ_filler_tree_famprovs_$(date +%Y%m%d)_bkp.txt
	cp -f ${req_files_folder}/REQ_site-specific_values_substitution_utf.txt ${scripts_path_shared}/bkps/REQ_site-specific_values_substitution_$(date +%Y%m%d)_bkp.txt
	cp -f ${req_files_folder}/REQ_values_to_remove.txt ${scripts_path_shared}/bkps/REQ_values_to_remove_$(date +%Y%m%d)_bkp.txt
fi

#------------------------------------------------------------------------------------------------------------------------------------------------------------
#3. Script parameters
#------------------------------------------------------------------------------------------------------------------------------------------------------------
printf "Testing parameters passed to the script..."

infile=$1
#test=$2: now specified at beginning of script, no longer a parameter.

#Check if infile is missing
if [ -z ${infile} ]; then printf "${red_color}Site file (parameter #1) is missing!\nABORTING\n${normal_color}";exit 0;fi
if [ ! -e ${infile} ]; then printf "${red_color}The file ${infile} doesn't exist!\nABORTING\n${normal_color}";exit 0;fi

#Check if test mode is correct
if [ -z $(echo ${test}|grep '^[12]$') ]; then printf "${red_color}Incorrect test mode: 1=yes, 2=no!\nABORTING\n${normal_color}";exit 0;fi

printf "done.\n"


#convert to Unix the infile
	dos2unix ${infile} 2> /dev/null
	fnct_convert_to_utf8 ${infile} ${temp_f}/infile_utf.txt


#------------------------------------------------------------------------------------------------------------------------------------------------------------
#4. Create reference files
#	Note: 
#	Some reference files require informations (e.g. site_code) from parameters, and are thus extracted after or in section 4.
#		
#	Some other reference files ARE required for section 4:
#		${temp_f}/TreeSource_sites_code_and_plantation_dates_and_years.txt
#		${temp_f}/TreeSource_data_source_ids.txt
#------------------------------------------------------------------------------------------------------------------------------------------------------------
printf "Extracting reference files from the database..."


#[NEW! 27/2/2015]: Extract all organisms from TreeSource, because an up-to-date version might be needed if a SPECIES column is present. 
	psql -d ts_pi -A -t -F "___" -c "SELECT * FROM organisms ORDER BY name" |iconv -f utf-8 -t iso-8859-1//IGNORE|perl -pe 's/___/\t/g' > ${all_species_filepath}
	dos2unix ${all_species_filepath} 2> /dev/null

#Create files from existing required (REQ) files
	#REQ_obs_codes_other_than_tree_status.txt
		grep -vE '^#|^$' ${req_files_folder}/REQ_data_types.txt |awk -F "\t" '{if ($14 == "obs_code" && $15 != "1") print $1}'|sort -u > ${req_files_folder}/REQ_obs_codes_other_than_tree_status.txt


#Extract list of observation codes from TreeSource (30/8/2013: replaced with the line below)
	#psql -d ts_pi -A -t -c "SELECT code FROM observation_codes ORDER BY 1" > ${temp_f}/TreeSource_all_obs_codes.txt
	#dos2unix ${temp_f}/TreeSource_all_obs_codes.txt 2>/dev/null


#Extract all observation codes (prefixes, values, full code, description) from TreeSource
	psql -d ts_pi -A -t -F "___" -f "${req_files_folder}/SQL_extract_observation_codes.txt" |perl -pe 's/___/\t/g' > ${temp_f}/TreeSource_all_obs_codes_and_descriptions.txt
	dos2unix ${temp_f}/TreeSource_all_obs_codes_and_descriptions.txt 2> /dev/null

#Extract only the codes from the previous list
	cut -f4 ${temp_f}/TreeSource_all_obs_codes_and_descriptions.txt|sort > ${temp_f}/TreeSource_all_obs_codes_only.txt

#From the previous list, keep only those that are going to be used in Excel files coming from field work.
	awk -F "\t" '{if ($5 == "t") print $1"\t"$3"\t"$4"\t"$6}' ${temp_f}/TreeSource_all_obs_codes_and_descriptions.txt |sort -t $'\t' -k1,1 -k2,2 |cat ${req_files_folder}/REQ_obs_codes_conv_table2_header.txt - > ${req_files_folder}/REQ_obs_codes_conv_table2_sel_codes.txt


#Extract the list of sites (code and establishment_year)
	psql -d ts_pi -A -t -F "___" -c "select code,plantation_establishment_date_from,plantation_establishment_date_to,establishment_year from sites where code NOT LIKE '(%' order by code" |perl -pe 's/___/\t/g' > ${temp_f}/TreeSource_sites_code_and_plantation_dates_and_years.txt
	dos2unix ${temp_f}/TreeSource_sites_code_and_plantation_dates_and_years.txt 2> /dev/null

#Extract the list of data_source_id's
	psql -d ts_pi -A -t -c "select id FROM data_sources ORDER BY 1" > ${temp_f}/TreeSource_data_source_ids.txt
	dos2unix ${temp_f}/TreeSource_data_source_ids.txt 2> /dev/null

#[NEW 20210709] Create the a conversion table between INFO_TYPE headers and actual table fields
#Previously, this was kept in REQ_replacement_headers.txt, but it is now generated on-the-fly here.
#This avoids to keep two files (REQ_data_types.txt and REQ_replacement_headers.txt) synchronized.
grep -vE '^#|^$|^-' ${req_files_folder}/REQ_data_types.txt|awk -F "\t" '{print $1"\t"$14"\t"$13}'|sort -k1,1 > ${temp_f}/replacement_headers.txt



printf "done.\n"


#------------------------------------------------------------------------------------------------------------------------------------------------------------
#5. Site parameters
#------------------------------------------------------------------------------------------------------------------------------------------------------------
#Site parameters validation -- 11/9/2014
#	User values in Excel files
#	Default values and validation info from REQ_parameters_default_values_and_types_utf.txt.
#---------------------------------------------------------------------------
printf "Looking for parameters in the file ${infile}:\n"

#Add to journal file the header for analysis info to come
printf "R/O\tParameter\tExcel file value\tinfo\taction\n" > ${param_jfile}

#Pass each line of the parameters file (1 line = 1 site parameter)
for param_file_line in $(grep -v "^#" ${parameters_file})
do
	param_status=$(echo $param_file_line|cut -f1)
		#If empty string, set it to "unknown", otherwise the function will ignore this parameter
		if [ "$param_status" == "" ];then param_status="U";fi
	param_name=$(echo $param_file_line|cut -f2)
		#If empty string, set it to "unknown", otherwise the function will ignore this parameter
		if [ "$param_name" == "" ];then param_name="(unknown)";fi
	param_data_type=$(echo $param_file_line|cut -f3)
		#If empty string, set it to "unknown", otherwise the function will ignore this parameter
		if [ "$param_data_type" == "" ];then param_data_type="(unknown)";fi
	param_default_value=$(echo $param_file_line|cut -f4)
		#If empty string, set it to 0, otherwise the function will ignore this parameter
		if [ "$param_default_value" == "" ];then param_default_value=0;fi
	int_param_min_value=$(echo $param_file_line|cut -f5)
		#If empty string, set it to 0, otherwise the function will ignore this parameter
		if [ "$int_param_min_value" == "" ];then int_param_min_value=0;fi
	int_param_max_value=$(echo $param_file_line|cut -f6)
		#If empty string, set it to 0, otherwise the function will ignore this parameter
		if [ "$int_param_max_value" == "" ];then int_param_max_value=0;fi

	#Show parameter name onscreen
	printf "\t${param_name}..."
	
	
	#echo "Parameters to be passed to the function:"
	#echo "============================="
	#echo param_status: $param_status
	#echo param_name: $param_name
	#echo param_data_type: $param_data_type
	#echo param_default_value: $param_default_value
	#echo int_param_min_value: $int_param_min_value
	#echo int_param_max_value: $int_param_max_value
	#echo parameters_file: $parameters_file
	#echo param_jfile: $param_jfile

#Execute function to validate current SITE parameter
	fnct_get_and_validate_site_parameters ${param_status} ${param_name} ${param_data_type} ${param_default_value} ${int_param_min_value} ${int_param_max_value} ${parameters_file} ${param_jfile}

#Once the parameter has been verified, check the response sent by the function
#Vérifier le code de retour (1, 2 ou aucun)
case $(echo $?) in
	1)	#Paramètre de fonction incorrect.
		printf "\t\t${red_color}PLEASE, CORRECT THE PARAMETERS FILE ${parameters_file} AND RESTART THE SCRIPT.${normal_color}\n"
		exit
		;;
	
	2)	#Paramètre de site incorrect.
		if [ ! -z $(echo ${param_status}|grep -Ei 'R') ]
		then 
			printf "ABORTED!\n" >> ${param_jfile}
			printf "\t\t${red_color}PLEASE CHECK THE FILE ${infile} AND RESTART THE SCRIPT.${normal_color}\n"
			exit
		else
			printf "\t\tDo you want to put the default value ($param_default_value) for parameter ${param_name}?\n\t\t(No will abort).\n"
			choice="-"
			while [ -z $(echo $choice|grep '[yn]') ]
			do
				printf "\r"
				read -p "                [y/n]?" -n1 -s choice
			done
			printf "\n"
			
			if [ $choice == "y" ]
			then
				printf "SET TO DEFAULT: ${param_default_value}.\n" >> ${param_jfile}
				chosen_value=${param_default_value}
			else
				printf "ABORTED!\n" >> ${param_jfile}
				printf "\t\t${red_color}PLEASE CHECK THE FILE ${infile} AND RESTART THE SCRIPT.${normal_color}\n"
				exit
			fi
		fi
		;;
	
	*)	#Aucune erreur.
		printf "Ok.\n"
		chosen_value=${parameter_value}
		;;
esac

#Set the variable its validated and chosen (if applicable) value
	declare ${param_name}="${chosen_value}"
	
	#Special case: parameter_name="plantation_team_employees": get team_id from it.
	#8/1/2015
	if [ ${param_name} == "plantation_team_employees" ]
	then
		plantation_team_id_pt_dev=$(psql -d ts_d -t -A -c "SELECT fnct_get_team_from_employees('${plantation_team_employees}');")
		plantation_team_id_pt_prod=$(psql -d ts_pi -t -A -c "SELECT fnct_get_team_from_employees('${plantation_team_employees}');")
	
		#Make sure the result for team_id fetching is the same in the two databases, otherwise, that means that they are not synchronized for team ids. Warn and exit.
		if [ ! ${plantation_team_id_pt_dev} == ${plantation_team_id_pt_prod} ]
		then
			printf "\n\t${red_color}PROBLEM WITH OBTAINING TEAM_ID FOR THE plantation_team_employees PARAMETER.\n"
			printf "\t'${plantation_team_employees}' point to 2 distinct team_id values in each database version:\n"
			printf "\t\tts_d: '${plantation_team_id_pt_dev}'\n"
			printf "\t\tts_pi: '${plantation_team_id_pt_prod}'\n"
			printf "\tPLEASE SYNCHRONIZE THE TWO DATABASES FOR THE TEAMS TABLE AND RESTART THE SCRIPT.\n"
			printf "ABORTED!${normal_color}\n"
			exit
		else	#If they are the same, just give plantation_team_id the value found for the dev version
			plantation_team_id=${plantation_team_id_pt_dev}
			
			if [ -z ${plantation_team_id} ]
			then
				plantation_team_id="\\\N"
			fi
		fi
	fi
	
	#echo
	#echo "SELECTED VALUES"
	#echo "-------------------------"
	#echo param_name ${param_name}
	#echo parameter_value ${parameter_value}
	#echo chosen_value ${chosen_value}

	#echo $site_code
	#echo
	#echo
done
printf "Have a look at the file $param_jfile for details.\n\n"


#Look for site code in DB -- 12/9/2014
#---------------------------------------------------------------------------
#Capitalize site code letters
site_code=$(echo ${site_code}|tr '[:lower:]' '[:upper:]')

printf "Looking for the existence of site code ${site_code} in the database..."
site_found_in_db=$(echo ${site_code}|join -t $'\t' - <(sort -k1,1 ${temp_f}/TreeSource_sites_code_and_plantation_dates_and_years.txt) ) 

#Check that the site code exists in the database
if [ -z ${site_found_in_db} ]
then
	printf "NOT FOUND!\n"
	printf "\t${red_color}The site ${site_code} does not exist in the database.\n\tPLEASE ADD IT TO THE sites table and restart the script.${normal_color}\n"
	exit

else
	printf "found.\n"
fi

#The site exists, check the plantation date(s)/year -- 12/9/2014
#---------------------------------------------------------------------------
printf "Looking for the plantation date/year of ${site_code} in the database..."

	plantation_date_from=$(echo ${site_found_in_db}|cut -f2)
	plantation_date_to=$(echo ${site_found_in_db}|cut -f3)
	plantation_year=$(echo ${site_found_in_db}|cut -f4)

	#echo $plantation_date_from
	#echo $plantation_date_to
	#echo $plantation_year


	#If sites.plantation_date_from (DB) contains a value, set plantation_date to this value
	if [ ! -z $plantation_date_from ]
	then
		printf "found a date.\n"
		#echo "sites.plantation_date_from=${plantation_date_from}"
		plantation_date=${plantation_date_from}
		plantation_date_is_approximate="FALSE"
	else
		#Else, if sites.plantation_date_to (DB) contains a value, set plantation_date to this value
		if [ ! -z $plantation_date_to ]
		then
			printf "found a date.\n"
			#echo "sites.plantation_date_to=${plantation_date_to}"
			plantation_date=${plantation_date_to}
			plantation_date_is_approximate="FALSE"
		else
			#Else, if sites.plantation_year (DB) contains a value, set plantation_date to a computed date using plantation_year
			if [ ! -z $plantation_year ] # should not be empty ***************************
			then
				printf "found a year, converted it to a date.\n"
				#echo "sites.plantation_year=${plantation_year}"
				plantation_date=${plantation_year}"-05-15"
				plantation_date_is_approximate="TRUE"
			#None of the 3 fields contain a value
			else
				printf "NOT FOUND!\n"
				printf "\t${red_color}The site ${site_code} does not have a plantation date or year.\n\tPLEASE ADD INFORMATION ON PLANTATION DATE/YEAR TO THE sites TABLE AND RESTART THE SCRIPT.${normal_color}\n"
				exit
			fi
		fi
	fi


#Look for data_source_id DB -- 12/9/2014
#---------------------------------------------------------------------------
printf "Looking for the existence of data_source_id=${data_source_id} in the database..."

if [ -z $(echo ${data_source_id}|join -t $'\t' - <(sort -k1,1 ${temp_f}/TreeSource_data_source_ids.txt) ) ]
then
	printf "NOT FOUND!\n"
	printf "\t${red_color}The data_source_id ${data_source_id} does not exist in the database.\n\tPLEASE ADD IT TO THE data_sources table and restart the script.${normal_color}\n"
	exit
fi
printf "found.\n"


#Extract reference files from the database, related to the site (14/1/2015)
#---------------------------------------------------------------------------
printf "Extracting ${site_code}-specific reference files from the database..."

#Extract the distinct coded observation years for that site
	psql -d ts_pi -A -t -c "SELECT DISTINCT extract(year FROM coog.date) FROM mv_current_germplasms_site_and_container mv JOIN coded_observations_on_germplasms coog USING (germplasm_id) WHERE mv.last_site_code='${site_code}'"|sort > ${temp_f}/TreeSource_coded_observation_years_${site_code}.txt
	dos2unix ${temp_f}/TreeSource_coded_observation_years_${site_code}.txt 2> /dev/null

#Extract the distinct measurement years for that site
	psql -d ts_pi -A -t -c "SELECT DISTINCT growth_year_at_msmt FROM mv_tree_mensurations_and_plantation_details WHERE site_code='${site_code}'"|sort > ${temp_f}/TreeSource_measurement_years_${site_code}.txt
	dos2unix ${temp_f}/TreeSource_measurement_years_${site_code}.txt 2> /dev/null
printf "done.\n"


#If there are already measurements and/or observations, additional_data=1, else additional_data=0.
if [ $(cat ${temp_f}/TreeSource_coded_observation_years_${site_code}.txt ${temp_f}/TreeSource_measurement_years_${site_code}.txt|sort -u|wc -l) -gt 0 ]; then additional_data=1;else additional_data=0;fi
#echo $additional_data


#------------------------------------------------------------------------------------------------------------------------------------------------------------
#5.A. Check the file REQ_site-specific_values_substitution_utf.txt for invalid entries (NEW! 27-28/7/2016)
#------------------------------------------------------------------------------------------------------------------------------------------------------------

printf "Checking the ${req_files_folder}/REQ_site-specific_values_substitution_utf.txt for errors..."

#col 1: sites codes - check that each code exists in  
#join <(grep -vE '^[[:space:]]*#|^[[:space:]]*$' ${req_files_folder}/REQ_site-specific_values_substitution_utf.txt|cut -f1|sort|uniq|grep -v 'ALL') <(cut -f1 ${temp_f}/TreeSource_sites_code_and_plantation_dates_and_years.txt|sort) -v1 > ${temp_f}/REQ_site-specific_values_substitution_site_codes_ERRORS.txt
join -t $'\t' <(grep -vE '^[[:space:]]*#|^[[:space:]]*$' ${req_files_folder}/REQ_site-specific_values_substitution_utf.txt|awk -F "\t" '{if($1 != "ALL") print}'|sort -k1,1) <(cut -f1 ${temp_f}/TreeSource_sites_code_and_plantation_dates_and_years.txt|sort) -v1 > ${temp_f}/REQ_site-specific_values_substitution_site_codes_ERRORS.txt

#If some of the site codes found in ${temp_f}/REQ_site-specific_values_substitution_site_codes.txt DO NOT exist in TreeSource:
if [ -s ${temp_f}/REQ_site-specific_values_substitution_site_codes_ERRORS.txt ]
then 
	printf "${red_color}\nThe site codes (1st column below) for the following lines, from ${req_files_folder}/REQ_site-specific_values_substitution_utf.txt, DO NOT EXIST in TreeSource.\n"
	perl -pe 's/^/\t/g' ${temp_f}/REQ_site-specific_values_substitution_site_codes_ERRORS.txt
	#Note: the current site code existence was confirmed in section 5. Site parameters, no need to re-check here.
	printf "None of these lines are related to the current site (${site_code}).\n"
	choice="-";while [ ! $(echo $choice|grep '[yn]') ];do printf "${green_color}\r"; read -p "Do you still want to continue [y/n]?" -n1 -s choice;done;printf "\n${normal_color}"
	if [ $choice == "n" ]; then printf "${red_color}***ABORTED***${normal_color}\n";exit;fi

fi



#col 2: type (entête): comparer avec toutes les entêtes possibles et sortir celles n'existant pas dans cette liste.
join -t $'\t' <(grep -vE '^[[:space:]]*#|^[[:space:]]*$' ${req_files_folder}/REQ_site-specific_values_substitution_utf.txt|awk -F "\t" '{print $2"\t"$0}'|sort -k1,1) <(grep -v '^[[:space:]]*#' ${req_files_folder}/REQ_data_types.txt|cut -f1|sort -u) -v1|cut -f1 --complement > ${temp_f}/REQ_site-specific_values_substitution_data_type_header_ERRORS.txt
#If some of the data type headers found in ${temp_f}/REQ_site-specific_values_substitution_site_codes.txt DO NOT exist in ${req_files_folder}/REQ_data_types.tx:
if [ -s ${temp_f}/REQ_site-specific_values_substitution_data_type_header_ERRORS.txt ]
then 
	printf "${red_color}\nThe data type headers (2nd column) of the following lines, from ${req_files_folder}/REQ_site-specific_values_substitution_utf.txt, DO NOT EXIST in ${req_files_folder}/REQ_data_types.txt\n"
	perl -pe 's/^/\t/g' ${temp_f}/REQ_site-specific_values_substitution_data_type_header_ERRORS.txt 

	
	#if there are some lines for the current site, warn and exit
	if [ ! -z $(cut -f1 ${temp_f}/REQ_site-specific_values_substitution_data_type_header_ERRORS.txt |sort -u|grep "${site_code}") ]
	then
		printf "${red_color}Since there is at least one line for the current site, ${site_code}, it MUST BE CORRECTED before the script can be run.\n"
		printf "Please correct them in the ${req_files_folder}/REQ_site-specific_values_substitution_utf.txt and restart the script${normal_color}\n"
		printf "${red_color}\n\n***ABORTED***${normal_color}\n"
		exit
	else #warn and offer to continue
		printf "None of these lines are related to the current site (${site_code}).\n"
		choice="-";while [ ! $(echo $choice|grep '[yn]') ];do printf "${green_color}\r"; read -p "Do you still want to continue [y/n]?" -n1 -s choice;done;printf "\n${normal_color}"
			if [ $choice == "n" ]; then printf "${red_color}***ABORTED***${normal_color}\n";exit;fi
	
	fi
fi




#col 3: année
grep -vE '^[[:space:]]*#|^[[:space:]]*$' ${req_files_folder}/REQ_site-specific_values_substitution_utf.txt|awk -F "\t" '{if($3 != "ALL") print}'|awk -F "\t" -v cy=${current_year} '{if ($3 !~ /^[0-9]{4}$/ || ($3 ~ /^[0-9]{4}$/ && $3 < 1500) || ($3 ~ /^[0-9]{4}$/ && $3 > cy)) print }' > ${temp_f}/REQ_site-specific_values_substitution_year_ERRORS.txt
#If some of the years found in ${temp_f}/REQ_site-specific_values_substitution_site_codes.txt DO NOT have the 0000 format:
if [ -s ${temp_f}/REQ_site-specific_values_substitution_year_ERRORS.txt ]
then 
	printf "${red_color}\nThe years (3rd column) of the following lines, from ${req_files_folder}/REQ_site-specific_values_substitution_utf.txt, ARE NOT VALID.\n"
	perl -pe 's/^/\t/g' ${temp_f}/REQ_site-specific_values_substitution_year_ERRORS.txt

	#if there are some lines for the current site, warn and exit
	if [ ! -z $(cut -f1 ${temp_f}/REQ_site-specific_values_substitution_year_ERRORS.txt|sort -u|grep "${site_code}") ]
	then
		printf "${red_color}Since there is at least one line for the current site, ${site_code}, it MUST BE CORRECTED before the script can be run.\n"
		printf "Please correct them in the ${req_files_folder}/REQ_site-specific_values_substitution_utf.txt and restart the script${normal_color}\n"
		printf "${red_color}\n\n***ABORTED***${normal_color}\n"
		exit

	else #warn and offer to continue
		printf "None of these lines are related to the current site (${site_code}).\n"
		choice="-";while [ ! $(echo $choice|grep '[yn]') ];do printf "${green_color}\r"; read -p "Do you still want to continue [y/n]?" -n1 -s choice;done;printf "\n${normal_color}"
			if [ $choice == "n" ]; then printf "${red_color}***ABORTED***${normal_color}\n";exit;fi
	fi	
fi



#col 4: code original:

#Cas 1: Remplacement des valeurs avant la validation
#	- On ne vérifie pas cette colonne, car ces codes à remplacer avant la validation peuvent être erronés (c'est d'ailleurs pourquoi ils sont substitués).
#	- Par contre, la colonne 5 peut être vérifiée dans le cas de codes d'observations (E, F, D, etc.), car ce sont les codes valides qui remplaceront ceux de la colonne 4.

#Cas 2: Remplacement des valeurs après la validation
#	- On ne fait pas de vérifications, car ces valeurs peuvent être trop variables (ex: famprov 999 ---> XXXXX, GEN_METH 1 ---> GER, TREE 99 ---> NULL).

#Cas 3: substitution dans le fichier tabulé
#Note: ces modifications sont faites UNIQUEMENT sur les fichiers tabulés de CODES D'OBSERVATIONS.
#	...toute autre champ spécifié pour une modification de type 3 sera exclu.
#- Le code de la colonne 4 doit exister dans TreeSource, avant d'être remplacé dans le fichier tabulé. 
#	Note: le code de la colonne 5, qui doit remplacer celui spécifié dans la colonne 4, NE DOIT PAS être vérifié, 
#	...car il est très possible qu'il n'existe pas encore dans TreeSource. Ceci est vérifié à la section 18.



#Vérifier que les remplacements de type 3 (sur fichier tabulé) se font uniquement sur des champs de codes d'observation (E, F, D, etc.)
join -t $'\t' <(grep -vE '^[[:space:]]*#|^[[:space:]]*$' ${req_files_folder}/REQ_obs_codes_conv_table2_sel_codes.txt|cut -f1|sort -u) <(grep -vE '^[[:space:]]*#|^[[:space:]]*$' ${req_files_folder}/REQ_site-specific_values_substitution_utf.txt|awk -F "\t" '{if ($6 == "3") print $2"\t"$0}'|sort -k1,1) -v2|cut -f1 --complement > ${temp_f}/REQ_site-specific_values_substitution_option_3_data_type_ERRORS.txt

#If some of the field types found in ${temp_f}/REQ_site-specific_values_substitution_site_codes.txt for option 3 (substitution in the tabulated file)...
#	...are not observation code types.
if [ -s ${temp_f}/REQ_site-specific_values_substitution_option_3_data_type_ERRORS.txt ]
then 
	printf "${red_color}\nThe data type in the 2nd column below, from ${req_files_folder}/REQ_site-specific_values_substitution_utf.txt, are NOT ALLOWED for tabulated files substitutions (option 3 on column #6). Only observation codes can be substituted for option 3.\n"
	perl -pe 's/^/\t/g' ${temp_f}/REQ_site-specific_values_substitution_option_3_data_type_ERRORS.txt
	
	#if there are some lines for the current site, warn and exit
	if [ ! -z $(cut -f1 ${temp_f}/REQ_site-specific_values_substitution_option_3_data_type_ERRORS.txt|sort -u|grep "${site_code}") ]
	then
		printf "${red_color}Since there is at least one line for the current site, ${site_code}, it MUST BE CORRECTED before the script can be run.\n"
		printf "Please correct them in the ${req_files_folder}/REQ_site-specific_values_substitution_utf.txt and restart the script${normal_color}\n"
		printf "${red_color}\n\n***ABORTED***${normal_color}\n"
		exit

	else #warn and offer to continue
		printf "None of these lines are related to the current site (${site_code}).\n"
		choice="-";while [ ! $(echo $choice|grep '[yn]') ];do printf "${green_color}\r"; read -p "Do you still want to continue [y/n]?" -n1 -s choice;done;printf "\n${normal_color}"
			if [ $choice == "n" ]; then printf "${red_color}***ABORTED***${normal_color}\n";exit;fi
	
	fi
	
fi



#Liste des codes complets (colonne #4, ex: E03, F_7, etc.) de REQ_site-specific_values_substitution_utf.txt pour l'option #3 (substitution dans fichier tabulé)...
#	...qui n'existent pas dans TreeSource.
join -t $'\t' <(grep -vE '^[[:space:]]*#|^[[:space:]]*$' ${req_files_folder}/REQ_site-specific_values_substitution_utf.txt|awk -F "\t" '{if($6 == 3) print $4"\t"$0}'|sort -k1,1) <(sort ${temp_f}/TreeSource_all_obs_codes_only.txt) -v 1|cut -f1 --complement|sort -k1,1 -k4,4 > ${temp_f}/REQ_site-specific_values_substitution_option_3_code_before_ERRORS.txt

#If some of the before-substitution codes found in ${temp_f}/REQ_site-specific_values_substitution_site_codes.txt DO NOT ALREADY EXIST in TreeSource (${temp_f}/TreeSource_all_obs_codes_only.txt), for option 3 (substitution in the tabulated file):
if [ -s ${temp_f}/REQ_site-specific_values_substitution_option_3_code_before_ERRORS.txt ]
then 
	printf "${red_color}\nThe codes in the 4th column below, from ${req_files_folder}/REQ_site-specific_values_substitution_utf.txt, DO NOT EXIST in TreeSource\n"
	perl -pe 's/^/\t/g' ${temp_f}/REQ_site-specific_values_substitution_option_3_code_before_ERRORS.txt
	
	#if there are some lines for the current site, warn and exit
	if [ ! -z $(cut -f1 ${temp_f}/REQ_site-specific_values_substitution_option_3_code_before_ERRORS.txt|sort -u|grep "${site_code}") ]
	then
		printf "${red_color}Since there is at least one line for the current site, ${site_code}, it MUST BE CORRECTED before the script can be run.\n"
		printf "Please correct them in the ${req_files_folder}/REQ_site-specific_values_substitution_utf.txt and restart the script${normal_color}\n"
		printf "${red_color}\n\n***ABORTED***${normal_color}\n"
		exit

	else #warn and offer to continue
		printf "None of these lines are related to the current site (${site_code}).\n"
		choice="-";while [ ! $(echo $choice|grep '[yn]') ];do printf "${green_color}\r"; read -p "Do you still want to continue [y/n]?" -n1 -s choice;done;printf "\n${normal_color}"
			if [ $choice == "n" ]; then printf "${red_color}***ABORTED***${normal_color}\n";exit;fi
	
	fi
	
fi



#col 5: code remplacé, lorsqu'il s'agit d'un remplacement avant la validation. Le code de remplacement doit être déjà dans TreeSource
#Note: cela s'applique uniquement pour les codes d'observation (E, F, D, etc.)

#Lignes de ${req_files_folder}/REQ_site-specific_values_substitution_utf.txt qui concernent uniquement les CATÉGORIES de codes d'observations (E, D, F, etc.)
join -t $'\t' <(grep -vE '^[[:space:]]*#|^[[:space:]]*$' ${req_files_folder}/REQ_obs_codes_conv_table2_sel_codes.txt|cut -f1|sort -u) <(grep -vE '^[[:space:]]*#|^[[:space:]]*$' ${req_files_folder}/REQ_site-specific_values_substitution_utf.txt|awk -F "\t" '{print $2"\t"$0}'|sort -k1,1)|cut -f1 --complement > ${temp_f}/REQ_site-specific_values_substitution_obs_codes_only.txt

#De cette liste, extraire uniquement les lignes où la colonne 6 est à 1 (remplacement des valeurs avant la validation).
join -t $'\t' <(awk -F "\t" '{if ($6 == "1") print $2"_"$5"\t"$0}' ${temp_f}/REQ_site-specific_values_substitution_obs_codes_only.txt|sort -k1,1) <(grep -vE '^[[:space:]]*#|^[[:space:]]*$' ${req_files_folder}/REQ_obs_codes_conv_table2_sel_codes.txt|awk -F "\t" '{print $1"_"$2}'|sort) -v1|cut -f1 --complement > ${temp_f}/REQ_site-specific_values_substitution_option_1_code_after_ERRORS.txt


#If some of the after-substitution codes found in ${temp_f}/REQ_site-specific_values_substitution_site_codes.txt DO NOT ALREADY EXIST in TreeSource (${temp_f}/TreeSource_all_obs_codes_only.txt), for option 1 (substitution before validation):
if [ -s ${temp_f}/REQ_site-specific_values_substitution_option_1_code_after_ERRORS.txt ]
then 
	printf "${red_color}\nThe codes in the 5th column below, from ${req_files_folder}/REQ_site-specific_values_substitution_utf.txt, DO NOT EXIST in TreeSource\n"
	perl -pe 's/^/\t/g' ${temp_f}/REQ_site-specific_values_substitution_option_1_code_after_ERRORS.txt
	
	#if there are some lines for the current site, warn and exit
	if [ ! -z $(cut -f1 ${temp_f}/REQ_site-specific_values_substitution_option_1_code_after_ERRORS.txt|sort -u|grep "${site_code}") ]
	then
		printf "${red_color}Since there is at least one line for the current site, ${site_code}, it MUST BE CORRECTED before the script can be run.\n"
		printf "Please correct them in the ${req_files_folder}/REQ_site-specific_values_substitution_utf.txt and restart the script${normal_color}\n"
		printf "${red_color}\n\n***ABORTED***${normal_color}\n"
		exit
	
	else #warn and offer to continue
		printf "None of these lines are related to the current site (${site_code}).\n"
		choice="-";while [ ! $(echo $choice|grep '[yn]') ];do printf "${green_color}\r"; read -p "Do you still want to continue [y/n]?" -n1 -s choice;done;printf "\n${normal_color}"
			if [ $choice == "n" ]; then printf "${red_color}***ABORTED***${normal_color}\n";exit;fi
	
	fi
	
fi

#Col 6: doit être 1, 2 ou 3.
grep -vE '^[[:space:]]*#|^[[:space:]]*$' ${req_files_folder}/REQ_site-specific_values_substitution_utf.txt|awk -F "\t" '{if ($6 !~ /^[1-3]$/) print}' > ${temp_f}/REQ_site-specific_values_substitution_replacement_option_ERRORS.txt

#If there are values different thant 1, 2 or 3 in ${req_files_folder}/REQ_site-specific_values_substitution_utf.txt, column 6:
if [ -s ${temp_f}/REQ_site-specific_values_substitution_replacement_option_ERRORS.txt ]
then 
	printf "${red_color}The values in the 6th column below, from ${req_files_folder}/REQ_site-specific_values_substitution_utf.txt, are INCORRECT (allowed range: 1-3)\n"
	perl -pe 's/^/\t/g' ${temp_f}/REQ_site-specific_values_substitution_replacement_option_ERRORS.txt
	
	#if there are some lines for the current site, warn and exit
	if [ ! -z $(cut -f1 ${temp_f}/REQ_site-specific_values_substitution_replacement_option_ERRORS.txt|sort -u|grep "${site_code}") ]
	then
		printf "${red_color}Since there is at least one line for the current site, ${site_code}, it MUST BE CORRECTED before the script can be run.\n"
		printf "Please correct them in the ${req_files_folder}/REQ_site-specific_values_substitution_utf.txt and restart the script${normal_color}\n"
	
	else #warn and offer to continue
		printf "None of these lines are related to the current site (${site_code}).\n"
		choice="-";while [ ! $(echo $choice|grep '[yn]') ];do printf "${green_color}\r"; read -p "Do you still want to continue [y/n]?" -n1 -s choice;done;printf "\n${normal_color}"
			if [ $choice == "n" ]; then printf "${red_color}***ABORTED***${normal_color}\n";exit;fi
	
	fi
	
fi

printf "done.\n"



#------------------------------------------------------------------------------------------------------------------------------------------------------------
#6. Variable definitions based on parameters
#------------------------------------------------------------------------------------------------------------------------------------------------------------

#Set the status observation code (e.g. "E", "QcE") corresponding to the data origin (e.g. 1=CFS, 2=MRNF)
#If no type was specified, assume it to be 1 (CFS).
case $data_origin in
	1)
		tree_status_obs_code="E"
		;;
	2)
		tree_status_obs_code="QcE"
		;;
	3)
		tree_status_obs_code="NOT YET DEFINED"
		;;
	4)
		tree_status_obs_code="NOT YET DEFINED"
		;;
	*)
		tree_status_obs_code="E"
		;;
esac

journal_f="extract_${site_code}_${timestamp}_log.txt"
final_folder=${site_code}_${timestamp}


#------------------------------------------------------------------------------------------------------------------------------------------------------------
#7. Locate marked rows
#------------------------------------------------------------------------------------------------------------------------------------------------------------
printf "Locating line markers:\n"
echo "-----------------------------"

#TODO: USE DECLARE INSTEAD OF THAT.
#Create commands to define variables with line number from REQ_line_markers.txt file
awk -F "\t" '{print $1"=$(grep -n \"^"$2"\" ${temp_f}/infile_utf.txt|perl -pe '\''s/^(\\d+)\\:.+/\\1/g'\'')"}' ${req_files_folder}/REQ_line_markers.txt|grep -v "^#" > ${temp_f}/line_markers_var_set.txt
#Line before 10/4/2013: awk -F "\t" '{print $1"=$(grep -n \"^"$2"\" $1|perl -pe '\''s/^(\\d+)\\:.+/\\1/g'\'')"}' ${req_files_folder}/REQ_line_markers.txt|grep -v "^#" > ${temp_f}/line_markers_var_set.txt
	#line_markers_var_set.txt
		#orig_header_row=$(grep -n "^ORIG_HEADER" $1|perl -pe 's/^(\d+)\:.+/\1/g')
		#date_row=$(grep -n "^DATE" $1|perl -pe 's/^(\d+)\:.+/\1/g')
		#info_type_row=$(grep -n "^INFO_TYPE" $1|perl -pe 's/^(\d+)\:.+/\1/g')
		#units_row=$(grep -n "^UNITS" $1|perl -pe 's/^(\d+)\:.+/\1/g')
		#data_1st_row=$(grep -n "^DATA_1ST_ROW" $1|perl -pe 's/^(\d+)\:.+/\1/g')

. ${temp_f}/line_markers_var_set.txt

#Create commands to extract variable values
awk -F "\t" '{print "echo $"$1}' ${req_files_folder}/REQ_line_markers.txt|grep -v '#' > ${temp_f}/line_marker_var_show.txt
	#line_marker_var_show.txt
		#echo $orig_header_row
		#echo $date_row
		#echo $info_type_row
		#echo $units_row
		#echo $data_1st_row
	
#Put variable name + value into a file:
paste <(cut -f1 ${req_files_folder}/REQ_line_markers.txt|grep -v '^#'|perl -pe 's/\n/:\n/g') <(. ${temp_f}/line_marker_var_show.txt ) > ${temp_f}/line_marker_variables+values.txt
	#line_marker_variables+values.txt
		#orig_header_row:	6
		#date_row:	3
		#info_type_row:	4
		#units_row:	5
		#data_1st_row:	



#rm ${temp_f}/line_markers_var_set.txt
#rm ${temp_f}/line_marker_var_show.txt


#Check if markers are missing
if [ $(grep '[[:space:]]$' ${temp_f}/line_marker_variables+values.txt|wc -l) -gt 0 ]
then 
	printf "${red_color}The following line marker(s) are missing from the ${infile} file:\n"
	awk -F "\t" '{if ($2 == "") print "\t"$1}' ${temp_f}/line_marker_variables+values.txt|perl -pe 's/://g'
	printf "Please add these markers to the file and restart the script\n${normal_color}"
	fnct_aborting
fi

#Ok, all marker fields found
cat ${temp_f}/line_marker_variables+values.txt


#------------------------------------------------------------------------------------------------------------------------------------------------------------
#8. 	Check for the presence of valid field names, required and optional fields, correct date formats,
#	Also put column position for those fields into a variable
#------------------------------------------------------------------------------------------------------------------------------------------------------------
printf "Locating required fields & formats:\n"
echo "-----------------------------"
	#8A. Create a list of headers by putting side-to-side into a file: A. original headers B. type of info C. date D. units E. type of info + date
		#HARDCODED:
		#orig_header_row
		#info_type_row
		#date_row
		#units_row
		paste <(sed -n ${orig_header_row},${orig_header_row}p ${temp_f}/infile_utf.txt|cut -f1 --complement|perl -pe 's/\t/\n/g') <(sed -n ${info_type_row},${info_type_row}p ${temp_f}/infile_utf.txt|cut -f1 --complement|perl -pe 's/\t/\n/g') <(sed -n ${date_row},${date_row}p ${temp_f}/infile_utf.txt|cut -f1 --complement|perl -pe 's/\t/\n/g') <(sed -n ${units_row},${units_row}p ${temp_f}/infile_utf.txt|cut -f1 --complement|perl -pe 's/\t/\n/g')|awk -F "\t" '{if ($3!="") print $0"\t"$2"_"$3;else print $0"\t"$2}'|cat -n|perl -pe 's/^ +(\d+)\t/\1\t/g' > ${temp_f}/all_headers.txt
		
		#NOTE THAT THE COLUMN POSITION BELOW IS BASED ON THE FILE WHERE THE FIRST, EMPTY COLUMN, WAS REMOVED
		
		#all_headers.txt
		#1	SEQ	SEQUENCE			
		#2	X	-			
		#3	Y	-			
		#4	PARC	COMPARTMENT			
		#5	FAM	FAMPROV			
		#6	ST	TREE_ALT_NAME			
		#7	R	BLOCK			
		#8	AR	TREE			
		#9	E75	E	1975-09-01		E_1975-09-01
		#10	H75	height	1975-09-01		height_1975-09-01
			#Col1: column #
			#Col2: original header name
			#Col3: info type
			#Col4: date
			#Col5: units
			#Col6: info type + date (concatenation)

		#Number of columns
		all_columns_count=$(cat ${temp_f}/all_headers.txt|wc -l)

		#Get all different dates. Anything that doesn't contain 2 consecutive digits is ignored.
		cut -f4 ${temp_f}/all_headers.txt |sort -u|grep -v '^$'|grep '[0-9][0-9]' > ${temp_f}/distinct_dates.txt
	
	#8B. Check if every column in INFO_TYPE is a recognizable one (present in REQ_data_types.txt)
		#join the infile data types with those of REQ_data_types.txt
		join <(cut -f3 ${temp_f}/all_headers.txt |sort -u) <(grep -v "^#" ${req_files_folder}/REQ_data_types.txt |cut -f1|sort -u) -v1 > ${temp_f}/unrecognized_infile_data_types.txt

		if [ -s ${temp_f}/unrecognized_infile_data_types.txt ]
		then
			echo "${red_color}The following data types (INFO_TYPE) in ${infile} (${temp_f}/infile_utf.txt) were not recognized:"
			cat ${temp_f}/unrecognized_infile_data_types.txt|perl -pe 's/^$/(empty)/g'|perl -pe 's/^/\t/g'
			printf "${normal_color}"
			fnct_aborting
		fi

	#8C. Check if dates are of the right format
		awk '{if ($1 !~ /^[12][0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9]$/) print}' ${temp_f}/distinct_dates.txt > ${temp_f}/incorrect_dates.txt
		if [ -s ${temp_f}/incorrect_dates.txt ]
		then
			printf "${red_color}SOME SPECIFIED DATES ARE INCORRECTLY FORMATTED!\n"
			printf "The following dates are incorrect\n"
			cat ${temp_f}/incorrect_dates.txt|perl -pe 's/^/\t/g'
			printf "PLEASE CORRECT THE FILE ${infile} (${temp_f}/infile_utf.txt)\n${normal_color}"
			fnct_aborting
		fi


	#8D. Create a file to put the team employees for each date
		#***TEAMCODE_ok_20150107***
		if [ ! -e ${temp_f}/team_employees_per_date.txt ]
		then
			perl -pe 's/\n/\tNULL\n/g' ${temp_f}/distinct_dates.txt > ${temp_f}/team_employees_per_date.txt
		fi
		echo "Please, provide full employee names for each measurement date in the file ${temp_f}/team_employees_per_date.txt and press any key when done."
		echo "Note: if unknown, leave the NULL value."
		read -n1 -s
	
		#Convert to Unix
		dos2unix ${temp_f}/team_employees_per_date.txt
		
		#Join by date the user-modified file  and the original distinct_dates.txt file, and keep only columns 1 &2, to remove anything else
		join -t $'\t' ${temp_f}/distinct_dates.txt ${temp_f}/team_employees_per_date.txt|cut -f1,2 > temp.txt
		mv temp.txt ${temp_f}/team_employees_per_date.txt
		
		#Test if user-modified file has anything else than 2 columns
		if [ $(awk -F "\t" '{print NF}' ${temp_f}/team_employees_per_date.txt|sort -nu|head -1) -ne 2 ]
		then
			printf "${red_color}THE FILE ${temp_f}/team_employees_per_date.txt HAS AN INCORRECT NUMBER OF COLUMNS!\n"
			printf "PLEASE RESTART THE SCRIPT AND PROVIDE A CORRECT FILE WHEN ASKED.\n"
			printf "\n***ABORTING***\n${normal_color}"
			exit
		fi
		
		#Test if user-modified file still contains the original dates.
		if [ $(cat ${temp_f}/distinct_dates.txt|wc -l) -ne $(cat ${temp_f}/team_employees_per_date.txt|wc -l) ]
		then
			printf "${red_color}THE ORIGINAL DATES CANNOT BE FOUND OR ARE DUPLICATED IN FILE ${temp_f}/team_employees_per_date.txt!\n"
			printf "PLEASE RESTART THE SCRIPT AND PROVIDE A CORRECT FILE WHEN ASKED.\n"
			printf "\n***ABORTING***\n${normal_color}"
			exit
		fi
		
		#Convert to utf-8
		fnct_convert_to_utf8 ${temp_f}/team_employees_per_date.txt ${temp_f}/team_employees_per_date_utf.txt
				
		#Get correspoding id from ts_d and ts_pi
		awk -F "\t" '{print "SELECT '\''"$1"'\'','\''"$2"'\'',fnct_get_team_from_employees('\''"$2"'\'');"}' ${temp_f}/team_employees_per_date_utf.txt > ${temp_f}/SQL_get_team_id_from_employee_names.txt
		psql -d ts_d -t -A -F "___" -f "${temp_f}/SQL_get_team_id_from_employee_names.txt"|perl -pe 's/___/\t/g' > ${temp_f}/date_employees_teamid_pt_dev.txt
		psql -d ts_pi -t -A -F "___" -f "${temp_f}/SQL_get_team_id_from_employee_names.txt"|perl -pe 's/___/\t/g' > ${temp_f}/date_employees_teamid_pt_prod.txt

		#Make sure the result for team_id fetching is the same in the two databases, otherwise, that means that they are not synchronized for team ids. Warn and exit.
		if [ $(diff ${temp_f}/date_employees_teamid_pt_dev.txt ${temp_f}/date_employees_teamid_pt_prod.txt|wc -l) -ne 0 ]
		then
			printf "\n\t${red_color}PROBLEM WITH OBTAINING TEAM_IDs FOR THE ${temp_f}/team_employees_per_date.txt FILE.\n"
			printf "\tAt least one team points to 2 distinct team_id values in each database version (development vs production):\n"
			sdiff -s ${temp_f}/date_employees_teamid_pt_dev.txt ${temp_f}/date_employees_teamid_pt_prod.txt|perl -pe 's/^/\t\t/g'
			printf "\tPLEASE SYNCHRONIZE THE TWO DATABASES FOR THE TEAMS TABLE AND RESTART THE SCRIPT.\n"
			printf "ABORTED!${normal_color}\n"
			exit
		else	#If they are the same, just copy date_employees_teamid.txt from date_employees_teamid_pt_dev.txt
			cp ${temp_f}/date_employees_teamid_pt_dev.txt ${temp_f}/date_employees_teamid.txt
		fi
		dos2unix ${temp_f}/date_employees_teamid.txt



#exit
	printf "\nChecking the required fields for tree name calculation.\n"
	#8E. Check if required fields are there
		nb_required_fields=$(grep '^R' ${req_files_folder}/REQ_germplasm_name_fields.txt|wc -l)
		join <(cut -f3 ${temp_f}/all_headers.txt |sort -u) <( grep '^R' ${req_files_folder}/REQ_germplasm_name_fields.txt|cut -f2|sort ) > ${temp_f}/found_required_fields.txt
		nb_found_required_fields=$(cat ${temp_f}/found_required_fields.txt|wc -l)

		if [ $nb_found_required_fields -eq $nb_required_fields ]
		then
			printf "\tThe ${nb_required_fields} required fields for base germplasm name calculation were found:\n"
			cat ${temp_f}/found_required_fields.txt|perl -pe 's/^/\t\t/g'
		else
			printf "\t${red_color}ONLY ${nb_found_required_fields}/${nb_required_fields} required fields were found!\n"
			printf "\tThe following required field(s) are missing\n"
			join <(grep '^R' ${req_files_folder}/REQ_germplasm_name_fields.txt|cut -f2|sort) <(sort ${temp_f}/found_required_fields.txt) -v1|perl -pe 's/^/\t\t/g'
			printf "\tPLEASE CHECK THE FILE ${infile} (${temp_f}/infile_utf.txt) AGAIN AND ADD THE MISSING FIELD(S)\n${normal_color}"
			fnct_aborting
		fi

	#8F. Find required + optional field positions for germplasm name calculation
		#a. left join all germplasm name fields (required + optional) with their names & positions in all_headers.txt
		#b. add the "0" position to those not in all_headers.txt (e.g. SEQUENCE)
		join -t $'\t' <(grep -E '^R|^O' ${req_files_folder}/REQ_germplasm_name_fields.txt|cut -f2|sort) <(awk -F "\t" '{print $3"\t"$1}' ${temp_f}/all_headers.txt|sort ) -a 1|perl -pe 's/^(\S+)\n/\1\t0\n/g' > ${temp_f}/germplasm_name_fields_pos.txt
			#germplasm_name_fields_pos.txt
				#SEQUENCE        1
				#FAMPROV 5
				#BLOCK   7
				#TREE    8

		#Warn if optional field(s) are missing. We can assume that any field missing will be an optional field, since the script should have stopped above if any required fields were missing
		#Missing optional fields have a position=0 in ${temp_f}/germplasm_name_fields_pos.txt
		grep "[[:space:]]0$" ${temp_f}/germplasm_name_fields_pos.txt |cut -f1 > ${temp_f}/germplasm_name_missing_optional_fields.txt

		if [ $(cat ${temp_f}/germplasm_name_missing_optional_fields.txt|wc -l) -gt 0 ]; then printf "\tThe following optional fields are missing, they won't be included in the germplasm name:\n";cat ${temp_f}/germplasm_name_missing_optional_fields.txt|perl -pe 's/^/\t\t/g';fi


	#8G. Extract all possible fields in germplasms_on_sites from REQ_data_types.txt
		grep -v '^#' ${req_files_folder}/REQ_data_types.txt|awk -F "\t" '{print $13"\t"$1}'|grep 'germplasms_on_sites'|cut -f2|sort > ${temp_f}/germplasms_on_sites_fields.txt
	
	#8H. Find field positions for germplasms_on_sites and join them with those for germplasm names
		#When the field is not present in the file, put a 0 besides it as "position"
		#Add field positions for germplasm names fields (germplasm_name_fields_pos.txt)
		join -t $'\t' <(grep -v '^#' ${temp_f}/germplasms_on_sites_fields.txt|sort) <(awk -F "\t" '{print $3"\t"$1}' ${temp_f}/all_headers.txt|sort ) -a 1|perl -pe 's/^(\S+)\n/\1\t0\n/g'|cat - ${temp_f}/germplasm_name_fields_pos.txt|sort -u > ${temp_f}/all_possible_fields_pos.txt

	#8I. Create commands to set the variable names
		awk -F "\t" '{print $1"_col="$2}' ${temp_f}/all_possible_fields_pos.txt > ${temp_f}/all_possible_fields_var_set.txt
		#all_possible_fields_var_set.txt
			#BLOCK_col=3
			#COMPARTMENT_col=0
			#COMP_SEQ_col=0
			#FAMPROV_col=2
			#gos_comments_col=0
			#REPCOMPNO_col=28
			#SEQUENCE_col=1
			#SUBBLOCK_col=0
			#TREE_col=4
			#X_col=0
			#Y_col=0

		#Execute the file
		. ${temp_f}/all_possible_fields_var_set.txt

	#8J. Generate a list of replacement headers (column name vs real table field name)
	 
	 #NOT YET IN FUNCTION: since not all fields are present in the REQ_data_types.txt table, some improperly named (i.e. columns in original Excel files) columns would not be transformed into their real table field name counterparts
	 #grep -v '^#' ${req_files_folder}/REQ_data_types.txt|awk -F "\t" '{print $13"\t"$1"\t"$14}'|sort -k1,1 -k2,2|grep -v '^-' > ${temp_f}/replacement_headers.txt
		
#[NEW 20210707]
#Verify that  all fields required to calculate the tree name are present, depending on the name format selected in ${tree_name_format}.
printf "\tAdditional fields required for the tree name format (${tree_name_format}) selected.\n"
case $tree_name_format in
	
	"S_B_F_T_S")
		#Block, family/provenance and tree fields are required, so no need to check them here
		printf "\t\tnone.\n"
		;;
	
	"S_B_BR_BC_F")

		
		#If one of the two fields is missing
		if [ ${block_row_no_col} -eq 0 ] || [ ${block_col_no_col} -eq 0 ]
		then
			if [ ${block_row_no_col} -eq 0 ]
			then
				printf "\t${red_color}A BLOCK ROW field is required for this format, but it is missing in the original Excel file.${normal_color}\n"
			fi
			
			if [ ${block_col_no_col} -eq 0 ]
			then
				printf "\t${red_color}A BLOCK COLUMN field is required for this format, but it is missing in the original Excel file.${normal_color}\n"
			fi
			
			fnct_aborting
		fi	
		printf "\t\tblock_row_no\n"
		printf "\t\tblock_col_no\n"
		;;
	
	"S_B_F_T_C")
		#If compartment column is missing
		if [ ${COMPARTMENT_col} -eq 0 ]
		then	
			printf "\t${red_color}A COMPARTMENT field is required for this format, but it is missing in the original Excel file.${normal_color}\n"
			fnct_aborting
		fi
		printf "\t\tCOMPARTMENT\n"
		
		;;
	*)
		echo "none"
		;;
esac
printf "\tOk!\n"



#exit






#------------------------------------------------------------------------------------------------------------------------------------------------------------
#9. Create a cleaned file
#------------------------------------------------------------------------------------------------------------------------------------------------------------
#a. remove first, empty column
#b. keep only the calculated header name (column #6 from all_headers.txt)
#c. remove empty lines, [NEW 19/7/2016] even when they contains regular spaces or tabs.
printf "Creating a cleaned file:\n"
echo "---------------------------"
cat <(cut -f6 ${temp_f}/all_headers.txt |perl -pe 's/\n/\t/g'|perl -pe 's/\t$/\n/g') <(tail -n+${data_1st_row} ${temp_f}/infile_utf.txt|cut -f1 --complement)|grep -v '^[[:space:]]*$' > ${temp_f}/cleaned_infile.txt
	#cleaned_infile.txt
	#SEQUENCE	-	-	COMPARTMENT	FAMPROV	TREE_ALT_NAME	BLOCK	TREE	E_1975-09-01	height_1975-09-01	QT_1975-09-01	RT_1975-09-01	D_1975-09-01	D_1975-09-01	D_1975-09-01	F_1975-09-01	F_1975-09-01	F_1975-09-01	E_1981-09-01	height_1981-09-01	dbh_1981-09-01	QT_1981-09-01	RT_1981-09-01	D_1981-09-01	D_1981-09-01	F_1981-09-01	F_1981-09-01	_1981-09-01	E_1986-09-01	height_1986-09-01	dbh_1986-09-01	QT_1986-09-01	RT_1986-09-01	D_1986-09-01	F_1986-09-01	F_1986-09-01	E_1998-09-01	dbh_1998-09-01	height_1998-09-01	D_1998-09-01	D_1998-09-01	D_1998-09-01	F_1998-09-01	F_1998-09-01	-		-	-	E_2000-09-01	dbh_2000-09-01	_2000-09-01	-	-	E_2003-09-01	dbh_2003-09-01	height_2003-09-01	-	-	-	-	-	-	-	-	-	-	
	#417	17	1	33	5193	1601	1	1	1	131	6	6	4	0	0	0	0	0	1	290	40	8	7	4	0	0	0		1	350	65	5	6	8	2	0	7	0	0	5	6	8	0	2	0		0		7	0	 - 	0	0	77	0	0		0	0	0						
	#418	17	2	33	5193	1601	1	2	1	131	6	6	4	0	0	0	0	0	1	360	55	6	7	4	0	0	0		1	570	90	6	7	0	0	0	7	0	0	6	7	0	0	0	0		0		7	0	 - 	0	0	77	0	0		0	0	0						
	#419	17	3	33	5193	1601	1	3	1	204	4	5	4	0	0	2	0	0	1	570	80	6	6	4	0	2	0		1	810	120	6	6	0	0	0	1	190	1570	6	6	0	0	0	100		0,184375417		9	22	1	0,03801336	9,025017891	9	0	0		220	0	0	63	1760	0,279047164			
	#420	17	4	33	5193	1601	1	4	1	259	7	7	4	0	0	0	0	0	1	605	85	8	7	4	0	0	0	AS 80108	1	850	135	8	8	0	0	0	1	220	1680	8	8	0	0	0	100		0,266188252		1	23	0	0,04154766	9,864120794	1	24	180	24	230	240	1800	69			0,04523904	10,74051716	5193
	#421	17	5	33	5193	1601	1	5	1	173	6	7	4	0	0	0	0	0	1	485	60	7	7	4	0	0	0		1	790	100	7	7	0	0	0	1	140	1480	7	7	0	0	0	100		0,092486668		1	15	1	0,0176715	4,195514516	7	0	0		150	0	0	58	1653	0,119630031			
	#422	17	6	33	5193	1601	1	6	1	222	5	5	4	0	0	2	0	0	1	560	90	5	6	4	0	2	0		1	870	145	6	6	0	2	0	1	200	1520	6	6	0	0	2	100		0,1980679		1	23	0	0,04154766	9,864120794	1	23	177	50	230	230	1770	54			0,04154766	9,864120794	5193
	#423	17	7	33	5193	1601	1	7	1	183	6	5	4	0	0	0	0	0	1	480	80	6	6	4	0	0	0		1	820	120	6	6	0	0	0	1	200	1630	6	6	0	0	0	100		0,2126803		1	22	1	0,03801336	9,025017891	7	0	0		220	0	0	68	1833	0,290700553			
	#424	17	8	33	5193	1601	1	8	1	195	8	8	4	0	0	0	0	0	1	500	75	7	7	4	0	0	0		1	820	120	7	7	0	0	0	1	200	1580	7	7	0	0	0	100		0,2060383		1	21	0	0,03463614	8,22320845	1	22	174	32	210	220	1740	63			0,03801336	9,025017891	5193
	#425	17	9	33	5193	1601	1	9	1	164	6	6	4	0	0	0	0	0	1	385	60	7	8	4	0	1	0		1	610	110	7	7	0	0	0	7	0	0	7	7	0	0	0	0		0		7	0	 - 	0	0	77	0	0		0	0	0						
	#(...)

#d. uniformize column numbers. This is done because sometimes Excel doesn't count as a column an empty value in the last column.
bash ${req_files_folder}/uniformize_col_no.txt ${temp_f}/cleaned_infile.txt 1 n temp.txt
mv temp.txt ${temp_f}/cleaned_infile.txt



#------------------------------------------------------------------------------------------------------------------------------------------------------------
#10. Check for longer than expected family numbers (before substitution). If found, warning only, no automatic abort.
#------------------------------------------------------------------------------------------------------------------------------------------------------------
echo
echo "Checking the format and length of family/provenance numbers:"
echo "-----------------------------------------------"

#Extract the different formats encountered (implemented on 12/3/2014)
	printf "Here are the counts for the different family/provenance number formats encoutered (0 = any digit, A = any other character):\n"
	cut -f${FAMPROV_col} ${temp_f}/cleaned_infile.txt|tail -n+2|perl -pe 's/\d/0/g'|perl -pe 's/[^\d\s]/A/g'|sort|uniq -c|perl -pe 's/^ +(\d+) (.+)/\2\t\1/g' > ${temp_f}/family_formats.txt
	cat ${temp_f}/family_formats.txt|perl -pe 's/^/\t/g'

#Max number of characters (implemented on 12/3/2014)
	printf "This is the maximum character length found in ${infile} for a family/provenance number:\n"
	cut -f1 ${temp_f}/family_formats.txt|awk '{print length($1)}'|sort -n|tail -1|perl -pe 's/^/\t/g'

#Extract a list of fams/provs that are longer than the set maximal length for the field in the germplasm_name (famprovchars).
	awk -F "\t" -v awk_famprovchars=${famprovchars} -v awk_FAMPROV_col=${FAMPROV_col} '{if(length($awk_FAMPROV_col) > awk_famprovchars) print $awk_FAMPROV_col}' <(tail -n+2 ${temp_f}/cleaned_infile.txt) |sort -u > ${temp_f}/FAMPROV_too_long.txt

	if [ -s ${temp_f}/FAMPROV_too_long.txt ]
	then
		printf "${red_color}WARNING: The following family/provenance numbers are longer than the set maximal character length (famprovchars=${famprovchars}):${normal_color}\n"
		perl -pe 's/^/\t/g' ${temp_f}/FAMPROV_too_long.txt
		printf "${red_color}If these values are normal, or will be substituted as specified in REQ_site-specific_values_substitution_utf.txt, press y to continue. If not, press n to abort.\n${normal_color}"
		choice="-";while [ ! $(echo $choice|grep '[yn]') ];do printf "\r\t"; read -p "Accept [y/n]?" -n1 -s choice;done;printf "\n"
		if [ $choice == "n" ]
		then 
			printf "Please, check the original Excel file for family/provenance numbers > ${famprovchars}.\n"
			printf "${red_color}***ABORTED***${normal_color}\n"
			exit
		fi

	else
		echo "done."
	fi

#exit

#------------------------------------------------------------------------------------------------------------------------------------------------------------
#11. Check for replicated compartments (same block-family-tree numbers)
#------------------------------------------------------------------------------------------------------------------------------------------------------------
#22/3/2013
echo
echo "Checking for replicated compartments:"
echo "-----------------------------------------------"

	#Copy ${temp_f}/cleaned_infile.txt to ${temp_f}/cleaned_infile0.txt
	#Note: 
	#	if replicated compartment numbers are added below, ${temp_f}/cleaned_infile0.txt will be overwritten.
	#	if not, cleaned_infile0.txt will simply be the copy of cleaned_infile.txt made above.
	cp ${temp_f}/cleaned_infile.txt ${temp_f}/cleaned_infile0.txt



#If old data already exists (additional_data = 1), no use to look for replicated compartments: germplasms_on_sites records have already been inserted in TreeSource
if [ ${additional_data} -eq 1 ]
then
	echo
	printf "No need to check for replicated compartments: tree positioning data already exists in germplasms_on_sites.\n"


else #No data was previously inserted into TreeSource for that site, proceed with replicated compartments check

	if [ $REPCOMPNO_col -ne 0 ]
	then
		printf "A REPCOMPNO column is already in the original Excel file, at position ${REPCOMPNO_col}.\n"
		printf "Note that values in this existing column will have precedence over calculated ones.\n"
	fi




	#echo BLOC: $BLOCK_col FAMILLE: $FAMPROV_col ARBRE: $TREE_col # parcelle répliquée: $REPCOMPNO_col


	#11-01. Sortir la liste des parcelles répliquées, ou "replicated compartments" (bloc-famille-arbre répétés), dans le fichier original "cleaned_infile.txt"
	#	S'il n'y a pas de colonne de # de parcelle répliquée (REPCOMPNO_col=0), il ne considérera que bloc, famille et arbre dans la recherche de réplicas
	#	S'il la colonne repcompno existe et qu'il y a déjà des valeurs dedans indiquant un # de parcelle répliquée, ces parcelles répliquées seront exclues de replicated_BFA.txt...
	#		...car elles ne sont plus dupliquées selon le sort|uniq -d
	#	Note, le fichier replicated_BFA.txt aura:
	#		3 colonnes si REPCOMPNO_col=0
	#		4 colonnes si REPCOMPNO_col <> 0
	awk -F "\t" -v awk_block_col=$BLOCK_col -v awk_famprov_col=$FAMPROV_col -v awk_tree_col=$TREE_col -v awk_repcompno_col=$REPCOMPNO_col '{if (awk_repcompno_col != 0) print $awk_block_col"\t"$awk_famprov_col"\t"$awk_tree_col"\t"$awk_repcompno_col; else print $awk_block_col"\t"$awk_famprov_col"\t"$awk_tree_col}' ${temp_f}/cleaned_infile.txt|sort|uniq -d > ${temp_f}/replicated_BFA.txt
		#1	121	1	
		#1	121	2	
		#1	139	3	
		#1	9999	99	

	#11-02. S'il y a des parcelles répliquées (numéros de  bloc-famille/provenance-arbre qui reviennent), afficher les familles/provenances et demander 
	if [ -s ${temp_f}/replicated_BFA.txt ]
	then
		#List unique famprovs for replicated BFA compartments
		cut -f2 ${temp_f}/replicated_BFA.txt |sort -nu > ${temp_f}/replicated_BFA_famprovs.txt
			#121
			#139
			#9999
		printf "${red_color}WARNING: The following families/provenances have repeated Block, family/provenance and tree number combinations in distinct compartments:${normal_color}\n"
		perl -pe 's/^/\t/g' ${temp_f}/replicated_BFA_famprovs.txt
		printf "${red_color}If these are ALL made of NORMAL trees, press 1.\n"
		printf "If these are ALL made of FILLER trees, press 2.\n"
		printf "If they are a mix of NORMAL and FILLER trees, press 3.\n"
		printf "HINT: use the following query to list all families/provenances planted on the current site (${site_code}):\n"
		printf "${blue_color}\tSELECT * FROM v_planted_seedlots_short WHERE site_code='${site_code}';${normal_color}\n"
		choice="-";while [ ! $(echo $choice|grep '[123]') ];do printf "\r"; read -p "Press 1, 2 or 3" -n1 -s choice;done;printf "\n"
		


		#All normal trees, will proceed to auto-assignation of replicated compartment number (repcompno) below
		if [ $choice -eq 1 ]
		then
			printf "A new REP_COMP_NO_FINAL column will be added to the original data file specifying the replicated compartment numbers (1, 2, ...) in order of appearance.\n"
		fi

		#All filler trees, will add the famprov number(s) to the ${req_files_folder}/REQ_filler_tree_famprovs.txt reference file
		if [ $choice -eq 2 ]
		then
			printf "Those filler tree-specific family/provenance numbers will be added to the ${req_files_folder}/REQ_filler_tree_famprovs.txt reference file for the current site (${site_code}), and not be considered for replicated compartments.\n"
			perl -pe "s/^/${site_code}\t/g"  ${temp_f}/replicated_BFA_famprovs.txt >> ${req_files_folder}/REQ_filler_tree_famprovs.txt
			#Clean up the reference file containing filler tree family/provenance numbers
			fnct_clean_filler_tree_famprovs
		fi

		#Some normal, some filler trees, ask for each one.
		if [ $choice -eq 3 ]
		then
			printf "Please indicate for each family/provenance number if it is a normal number (n) or a filler tree (f):\n"
			#Ask which are filler trees and which aren't
			for famprov_no in $(cat ${temp_f}/replicated_BFA_famprovs.txt)
			do
				#printf "${famprov_no}: "
				choice_famprov="-"
				while [ ! $(echo $choice_famprov|grep '[fn]') ]
				do 
					printf "\r\t"
					read -p "${famprov_no}:" -n1 choice_famprov
				done
				
				if [ "${choice_famprov}" == "f" ]
				then
					printf "${site_code}\t${famprov_no}\n" >> ${req_files_folder}/REQ_filler_tree_famprovs.txt

				fi
				printf "\n"
				
			done

			#Clean up the reference file containing filler tree family/provenance numbers
			fnct_clean_filler_tree_famprovs
		
		fi

	fi


	#11-03. 
	#Remove filler trees from replicated_BFA.txt
	#Note: encore une fois, 3 colonnes si REPCOMPNO_col=0, 4 si REPCOMPNO_col <> 0
	join -v2 -t $'\t' <(grep "\b${site_code}\b" ${req_files_folder}/REQ_filler_tree_famprovs.txt|cut -f2|sort) <(awk -F "\t" '{print $2"\t"$0}' ${temp_f}/replicated_BFA.txt|sort -k1,1)|cut -f1 --complement > ${temp_f}/replicated_BFA_no_filler.txt
		#1	121	1	
		#1	121	2	
		#1	139	3	

	#11-04. 
	#Ajouter le code BFA (ex: 1_121_4) pour chaque combinaison de BFA répliqués
	#4 colonnes (REPCOMPNO_col=0) ou 5 (REPCOMPNO_col <> 0). Pas grave, car plus bas, uniquement la première colonne est utilisée (cut -f1).
	awk -F "\t" '{print $1"_"$2"_"$3"\t"$0}' ${temp_f}/replicated_BFA_no_filler.txt|sort -k1,1 > ${temp_f}/replicated_BFA_no_filler_with_code.txt
		#1_121_1	1	121	1	
		#1_121_2	1	121	2	
		#1_139_3	1	139	3	





	#If some repeated BFA, not from filler trees  (choice 1 and 3 above) 
	if [ $choice -ne 2 ]
	then
		 
		 #11-05.
		 #Modifier le fichier original (${temp_f}/cleaned_infile.txt):
		 #	1. ajouter le code BFA au début
		 #	2. ajouter colonne de numéros de lignes devant
		 #	3. copier ces 2 premières colonnes à la fin du fichier
		 #	4. Enlever la première colonne (muméros de lignes)
		 #	
		awk -F "\t" -v awk_block_col=$BLOCK_col -v awk_famprov_col=$FAMPROV_col -v awk_tree_col=$TREE_col '{print $awk_block_col"_"$awk_famprov_col"_"$awk_tree_col"\t"$0}' ${temp_f}/cleaned_infile.txt|nl|awk -F "\t" '{print $0"\t"$1"\t"$2}'|cut -f1 --complement|sort -k1,1 > ${temp_f}/cleaned_infile_with_BFA_code.txt
		#1_100_7 1       100     7       z       222                  3  1_100_7
		#1_121_1 1       121     1       a       333                  4  1_121_1
		#1_121_1 1       121     1       b       333                  5  1_121_1
		#1_121_1 1       121     1       c       333                  6  1_121_1
		#1_121_2 1       121     2       d       333                  7  1_121_2
		#1_121_2 1       121     2       e       333                  8  1_121_2
		#1_121_2 1       121     2       f       333                  9  1_121_2
		#1_139_2 1       139     2       g       333     1           10  1_139_2
		#1_139_2 1       139     2       h       333     2           11  1_139_2
		#1_139_2 1       139     2       i       333     3           12  1_139_2
		#1_139_3 1       139     3       j       333                 13  1_139_3
		#1_139_3 1       139     3       k       333                 14  1_139_3
		#1_139_3 1       139     3       l       333                 15  1_139_3
		#1_99_1  1       99      1       z       222                  2  1_99_1
		#1_9999_99       1       9999    99      z       111                 16  1_9999_99
		#1_9999_99       1       9999    99      z       111                 17  1_9999_99
		#1_9999_99       1       9999    99      z       111                 18  1_9999_99
		#1_99999_99      1       99999   99      z       111                 19  1_99999_99
		#B_F_A   B       F       A       ZZZ     KKK     REPCOMPNO            1  B_F_A





		#11-06.
		 #Extract the lines of above file where there is replicated BFA, and remove the BFA code (1st column)
		 #get lines with BFA codes (only) from the list of replicated BFA (replicated_BFA_no_filler_with_code.txt)
		 
		 #join -t $'\t' <(cut -f1 ${temp_f}/replicated_BFA_no_filler_with_code.txt) ${temp_f}/cleaned_infile_with_BFA_code.txt|cut -f1 --complement > ${temp_f}/cleaned_infile_replicated_BFA.txt
		 join -t $'\t' <(cut -f1 ${temp_f}/replicated_BFA_no_filler_with_code.txt) ${temp_f}/cleaned_infile_with_BFA_code.txt > ${temp_f}/cleaned_infile_replicated_BFA.txt
			 #1       121     1       a       333                  1  1_121_1
			#1       121     1       b       333                  2  1_121_1
			#1       121     1       c       333                  3  1_121_1
			#1       121     2       d       333                  4  1_121_2
			#1       121     2       e       333                  5  1_121_2
			#1       121     2       f       333                  6  1_121_2
			#1       139     3       j       333                 10  1_139_3
			#1       139     3       k       333                 11  1_139_3
			#1       139     3       l       333                 12  1_139_3
			
			#GOOD ONE
			#1_121_1	1       121     1       a       333                  1  1_121_1
			#1_121_1	1       121     1       b       333                  2  1_121_1
			#1_121_1	1       121     1       c       333                  3  1_121_1
			#1_121_2	1       121     2       d       333                  4  1_121_2
			#1_121_2	1       121     2       e       333                  5  1_121_2
			#1_121_2	1       121     2       f       333                  6  1_121_2
			#1_139_3	1       139     3       j       333                 10  1_139_3
			#1_139_3	1       139     3       k       333                 11  1_139_3
			#1_139_3	1       139     3       l       333                 12  1_139_3
		
		#11-07.
		#Copy existing line numbers as first column 
		#Sort by
		#	1. BFA code (col 2)
		#	2. line number (col 1)
		#Note: this ensures that replicated BFA are numbered according to their ordering in the original file.
		#Note2: putting the sorted columns at positions 1 & 2 in the file is essential otherwise the sort command doesn't work if sorting columns too far in the file.
		bfa_code_col_no=$(awk -F "\t" '{print NF}' ${temp_f}/cleaned_infile_replicated_BFA.txt|sort -u)
		let line_numbering_col_no=${bfa_code_col_no}-1
		
		paste <(cut -f${line_numbering_col_no} ${temp_f}/cleaned_infile_replicated_BFA.txt) ${temp_f}/cleaned_infile_replicated_BFA.txt|sort -k2,2 -k1,1 > ${temp_f}/cleaned_infile_replicated_BFA_sorted.txt
			#1	1_121_1	1       121     1       a       333                  1  1_121_1
			#2	1_121_1	1       121     1       b       333                  2  1_121_1
			#3	1_121_1	1       121     1       c       333                  3  1_121_1
			#4	1_121_2	1       121     2       d       333                  4  1_121_2
			#5	1_121_2	1       121     2       e       333                  5  1_121_2
			#6	1_121_2	1       121     2       f       333                  6  1_121_2
			#10	1_139_3	1       139     3       j       333                 10  1_139_3
			#11	1_139_3	1       139     3       k       333                 11  1_139_3
			#12	1_139_3	1       139     3       l       333                 12  1_139_3
		
		#Remove columns 1 & 2
		cut -f1,2 --complement ${temp_f}/cleaned_infile_replicated_BFA_sorted.txt > ${temp_f}/cleaned_infile_replicated_BFA_sorted2.txt
			#1       121     1       a       333                  1  1_121_1
			#1       121     1       b       333                  2  1_121_1
			#1       121     1       c       333                  3  1_121_1
			#1       121     2       d       333                  4  1_121_2
			#1       121     2       e       333                  5  1_121_2
			#1       121     2       f       333                  6  1_121_2
			#1       139     3       j       333                 10  1_139_3
			#1       139     3       k       333                 11  1_139_3
			#1       139     3       l       333                 12  1_139_3
		
		#11-08.
		#Add the replicated compartment number (last column)
		#awk -F "\t" -v OFS="\t" 'prev!=$NF{first=1;prev=$NF} {$(NF+1)=first;first++} $0' ${temp_f}/cleaned_infile_replicated_BFA.txt > ${temp_f}/cleaned_infile_replicated_BFA_calc_repcompno.txt
		awk -F "\t" -v OFS="\t" 'prev!=$NF{first=1;prev=$NF} {$(NF+1)=first;first++} $0' ${temp_f}/cleaned_infile_replicated_BFA_sorted2.txt > ${temp_f}/cleaned_infile_replicated_BFA_calc_repcompno.txt
			#1       121     1       a       333                  1  1_121_1 	1
			#1       121     1       b       333                  2  1_121_1 	2
			#1       121     1       c       333                  3  1_121_1 	3
			#1       121     2       d       333                  4  1_121_2	 1
			#1       121     2       e       333                  5  1_121_2 	2
			#1       121     2       f       333                  6  1_121_2 	3
			#1       139     3       j       333                 10  1_139_3 	1
			#1       139     3       k       333                 11  1_139_3 	2
			#1       139     3       l       333                 12  1_139_3 	3



		#11-09.
		 #Extract the lines of cleaned file where there is NO replicated BFA
		join -v2 -t $'\t' <(cut -f1 ${temp_f}/replicated_BFA_no_filler_with_code.txt) ${temp_f}/cleaned_infile_with_BFA_code.txt|cut -f1 --complement > ${temp_f}/cleaned_infile_non_replicated_BFA.txt
			#1       100     7       z       222                  3  1_100_7
			#1       139     2       g       333     1           10  1_139_2
			#1       139     2       h       333     2           11  1_139_2
			#1       139     2       i       333     3           12  1_139_2
			#1       99      1       z       222                  2  1_99_1
			#1       9999    99      z       111                 16  1_9999_99
			#1       9999    99      z       111                 17  1_9999_99
			#1       9999    99      z       111                 18  1_9999_99
			#1       99999   99      z       111                 19  1_99999_99
			#B       F       A       ZZZ     KKK     REPCOMPNO            1  B_F_A


		#11-10.
		#Add an empty value for repcompno column for those lines without replicated BFA
		perl -pe 's/\n/\t\n/g' ${temp_f}/cleaned_infile_non_replicated_BFA.txt > ${temp_f}/cleaned_infile_non_replicated_BFA_empty_repcompno.txt
			#1       100     7       z       222                  3  1_100_7	(<--tab)
			#1       139     2       g       333     1           10  1_139_2	(<--tab)
			#1       139     2       h       333     2           11  1_139_2	(<--tab)
			#1       139     2       i       333     3           12  1_139_2	(<--tab)
			#1       99      1       z       222                  2  1_99_1	(<--tab)
			#1       9999    99      z       111                 16  1_9999_99	(<--tab)
			#1       9999    99      z       111                 17  1_9999_99	(<--tab)
			#1       9999    99      z       111                 18  1_9999_99	(<--tab)
			#1       99999   99      z       111                 19  1_99999_99	(<--tab)
			#B       F       A       ZZZ     KKK     REPCOMPNO            1  B_F_A	(<--tab)


		#11-11.
		#Combine the two, and choose from original file or calculated repcompno depending on which has a value in it.
		#After the commands below, the file will have the following 4 columns at the end: 
		#	line numbers
		#	BFA code
		#	calculated repcompno
		#	final repcompno
		#Note: if it is the line # 1 (header), will add REP_COMP_NO_FINAL at the end. THIS ASSUMES THAT THERE WILL ALWAYS BE A HEADER IN THE INPUT FILE.
		#cat tonumber_replicated_BFA_calc_repcompno.txt tonumber_non_replicated_BFA_empty_repcompno.txt|awk -F "\t" -v awk_repcompno_col=$REPCOMPNO_col '{if(awk_repcompno_col != 0) print $0"\t"$awk_repcompno_col; else print $0"\t"$NF}' > tonumber_calc_repcompno.txt
		
		#If there is a repcompno col in original file
		if [ $REPCOMPNO_col -ne 0 ]
		then
			#REPCOMPNO column exists, take its value when there is one, instead of the calculated one. Otherwise, take the calculated one. In both cases, copy the value to a new column at the end.
			cat ${temp_f}/cleaned_infile_replicated_BFA_calc_repcompno.txt ${temp_f}/cleaned_infile_non_replicated_BFA_empty_repcompno.txt|awk -F "\t" -v awk_repcompno_col=$REPCOMPNO_col '{if($(NF-2) == 1) print $0"\tREP_COMP_NO_FINAL"; else if($awk_repcompno_col != "") print $0"\t"$awk_repcompno_col; else print $0"\t"$NF}' > ${temp_f}/cleaned_infile_calc_repcompno.txt
		else
			#REPCOMPNO column doesn't exist, always take the calculated one and copy it in a new column at the end.
			cat ${temp_f}/cleaned_infile_replicated_BFA_calc_repcompno.txt ${temp_f}/cleaned_infile_non_replicated_BFA_empty_repcompno.txt|awk -F "\t" '{if($(NF-2) == 1) print $0"\tREP_COMP_NO_FINAL"; else print $0"\t"$NF}' > ${temp_f}/cleaned_infile_calc_repcompno.txt

		
		fi


			#1       121     1       a       333                  4  1_121_1 1       1
			#1       121     1       b       333                  5  1_121_1 2       2
			#1       121     1       c       333                  6  1_121_1 3       3
			#1       121     2       d       333                  7  1_121_2 1       1
			#1       121     2       e       333                  8  1_121_2 2       2
			#1       121     2       f       333                  9  1_121_2 3       3
			#1       139     3       j       333                 13  1_139_3 1       1
			#1       139     3       k       333                 14  1_139_3 2       2
			#1       139     3       l       333                 15  1_139_3 3       3
			#1       100     7       z       222                  3  1_100_7
			#1       139     2       g       333     1           10  1_139_2         1
			#1       139     2       h       333     2           11  1_139_2         2
			#1       139     2       i       333     3           12  1_139_2         3
			#1       99      1       z       222                  2  1_99_1
			#1       9999    99      z       111                 16  1_9999_99
			#1       9999    99      z       111                 17  1_9999_99
			#1       9999    99      z       111                 18  1_9999_99
			#1       99999   99      z       111                 19  1_99999_99
			#B       F       A       ZZZ     KKK     REPCOMPNO            1  B_F_A           REP_COMP_NO_FINAL




		#11-12.
		#Calculate the column position for the line numbers
		#Reminder: 4 last columns
		#	line numbers (header = 1)
		#	BFA code (B_F_A)
		#	calculated repcompno (no header)
		#	final repcompno (REP_COMP_NO_FINAL)
		
		let final_repcompno_col=$(awk -F "\t" '{print NF}' ${temp_f}/cleaned_infile_calc_repcompno.txt|sort -u)	#The REP_COMP_NO_FINAL column position (always the last file column)
		let line_number_col=${final_repcompno_col}-3	#line number
		let last_col_to_remove=${final_repcompno_col}-1 #calculated repcompno

		#Put line number column in front, remove added columns except that for calculated repcompno, sort by line number, remove line numbers
		#Only remaining column from the new added in this block of code: REP_COMP_NO_FINAL (last column)
		
		paste <(cut -f${line_number_col} ${temp_f}/cleaned_infile_calc_repcompno.txt) <(cut -f${line_number_col}-${last_col_to_remove} --complement ${temp_f}/cleaned_infile_calc_repcompno.txt)|sort -k1,1n|cut -f1 --complement > ${temp_f}/cleaned_infile0.txt
		#paste <(cut -f${line_number_col} ${temp_f}/cleaned_infile_calc_repcompno.txt) <(cut -f${line_number_col}-${last_col_to_remove} --complement ${temp_f}/cleaned_infile_calc_repcompno.txt)|sort -k1,1n|cut -f1 --complement > aaa.txt
			#B       F       A       ZZZ     KKK     REPCOMPNO       REP_COMP_NO_FINAL
			#1       99      1       z       222
			#1       100     7       z       222
			#1       121     1       a       333             1
			#1       121     1       b       333             2
			#1       121     1       c       333             3
			#1       121     2       d       333             1
			#1       121     2       e       333             2
			#1       121     2       f       333             3
			#1       139     2       g       333     1       1
			#1       139     2       h       333     2       2
			#1       139     2       i       333     3       3
			#1       139     3       j       333             1
			#1       139     3       k       333             2
			#1       139     3       l       333             3
			#1       9999    99      z       111
			#1       9999    99      z       111
			#1       9999    99      z       111
			#1       99999   99      z       111


		#11-13.
		#Recalculate the new column position for REP_COMP_NO_FINAL
		let final_repcompno_col=$(awk -F "\t" '{print NF}' ${temp_f}/cleaned_infile0.txt|sort -u)

		printf "The newly added REP_COMP_NO_FINAL column is at position ${final_repcompno_col}.\n"
		if [ $REPCOMPNO_col -ne 0 ]
		then
			printf "It will be the new reference REPCOMPNO column. The original was at position ${REPCOMPNO_col}.\nThe original REPCOMPNO will be renamed REPCOMPNO_OLD.\n"
		fi

		#11-14.
		#Check that cleaned_infile.txt and cleaned_infile0.txt are the same but for the REP_COMP_NO_FINAL column added in the latter as the last column
		differences=$(diff ${temp_f}/cleaned_infile.txt <(cut -f${final_repcompno_col} --complement ${temp_f}/cleaned_infile0.txt)|wc -l)
		printf "\tNumber of different lines between cleaned_infile.txt and cleaned_infile0.txt (after removal of REP_COMP_NO_FINAL in the latter):\n"
		printf "\t\t${differences}\n"
		if [ $differences -ne 0 ]
		then
			printf "${red_color}There are differences between the ${temp_f}/cleaned_indile.txt and the one with added replicated compartment number (${temp_f}/cleaned_infile0.txt).\n"
			printf "Please compare the two and verify the \"Check for replicated compartments (same block-family-tree numbers)\" section of the script.\n"
			printf "***ABORTED***${normal_color}\n"
			exit
		fi


		#11-15.
		#Rename the old REPCOMPNO column to REPCOMPNO_OLD and REP_COMP_NO_FINAL to REPCOMPNO in cleaned_infile0.txt
		if [ $REPCOMPNO_col -ne 0 ]
		then

			#Copy ${temp_f}/cleaned_infile0.txt to ${temp_f}/cleaned_infile0_bkp.txt for script verifications if needed
			cp ${temp_f}/cleaned_infile0.txt ${temp_f}/cleaned_infile0_bkp.txt
			
			#Rename the original REPCOMNO column to REPCOMPNO_OLD, then rename the new column REP_COMP_NO_FINAL to REPCOMPNO.
			#Renaming is crucial because for the tabulated file creation for germplasms_on_sites, the header name is REPCOMPNO (see replacement_headers.txt)
			perl -pe 's/REPCOMPNO/REPCOMPNO_OLD/g' ${temp_f}/cleaned_infile0.txt|perl -pe 's/REP_COMP_NO_FINAL/REPCOMPNO_-/g' > ${temp_f}/cleaned_infile0_temp.txt
			mv ${temp_f}/cleaned_infile0_temp.txt ${temp_f}/cleaned_infile0.txt
			
			#Modify ${temp_f}/all_headers.txt
				#Rename REPCOMPNO to REPCOMPNO_OLD
				perl -pe 's/REPCOMPNO/REPCOMPNO_OLD/g' ${temp_f}/all_headers.txt > ${temp_f}/all_headers_temp.txt
				
				#Add the final calculated repcompno column as the new REPCOMPNO column in all_headers.txt
				cat ${temp_f}/all_headers_temp.txt <(printf "${final_repcompno_col}\tREPCOMPNO\tREPCOMPNO\t-\t-\tREPCOMPNO_-\n") > ${temp_f}/all_headers.txt

				#Redefine the file all_possible_fields_pos.txt, because based on all_headers.txt
				#It was defined first in section 8H, lists column positions for reference files, and serves to define variables with them.
				#It will be used also to filter rows to be removed (section 12) and to create 1 file per field for tabulated files creation (section 21).
				join -t $'\t' <(grep -v '^#' ${temp_f}/germplasms_on_sites_fields.txt|sort) <(awk -F "\t" '{print $3"\t"$1}' ${temp_f}/all_headers.txt|sort ) -a 1|perl -pe 's/^(\S+)\n/\1\t0\n/g'|cat - ${temp_f}/germplasm_name_fields_pos.txt|sort -u > ${temp_f}/all_possible_fields_pos.txt

				#Then redefine the variable REPCOMPNO_col, to set it to the last column
				REPCOMPNO_col=${final_repcompno_col}

		fi
	fi

fi


#exit #**************************************************************


#------------------------------------------------------------------------------------------------------------------------------------------------------------
#12.  Remove lines based on certain field values
#------------------------------------------------------------------------------------------------------------------------------------------------------------
echo
echo "Removal of lines based on certain field values:"
echo "-----------------------------------------------"
echo "The lines containing the following field values will be removed from the original file"
perl -pe 's/\t/=/g' ${req_files_folder}/REQ_values_to_remove.txt|perl -pe 's/^/\t/g'

#Create a list of columns with values to remove
join -t $'\t' <(sort -t $'\t' -k1,1 ${req_files_folder}/REQ_values_to_remove.txt) <(sort -t $'\t' -k1,1 ${temp_f}/all_possible_fields_pos.txt)> ${temp_f}/values_to_remove.txt
#BLOCK	96	2
#FAMPROV	0	18
#FAMPROV	99999	18



printf "" > ${temp_f}/removed_lines.txt

for filter_line in $(cat ${temp_f}/values_to_remove.txt)
do
	column2filter=$(echo $filter_line|cut -f3)
	value2filter=$(echo $filter_line|cut -f2)

	#Keep the removed lines in a separate file
	awk -F "\t" -v awk_column2filter=$column2filter -v awk_value2filter=$value2filter '{if ($awk_column2filter == awk_value2filter) print}' ${temp_f}/cleaned_infile0.txt >> ${temp_f}/removed_lines.txt
	
	#Remove them from the original file
	awk -F "\t" -v awk_column2filter=$column2filter -v awk_value2filter=$value2filter '{if ($awk_column2filter != awk_value2filter) print}' ${temp_f}/cleaned_infile0.txt > ${temp_f}/temp_cleaned_infile0.txt
	mv ${temp_f}/temp_cleaned_infile0.txt ${temp_f}/cleaned_infile0.txt 
done

if [ -s ${temp_f}/removed_lines.txt ]
then
	echo
	printf "$(cat ${temp_f}/removed_lines.txt|wc -l) lines were removed from the original file!\n"
	printf "Here are the removed lines (${temp_f}/removed_lines.txt):\n"
	perl -pe 's/^/\t/g' ${temp_f}/removed_lines.txt
	echo
fi




#------------------------------------------------------------------------------------------------------------------------------------------------------------
#13. Create specific header files
#------------------------------------------------------------------------------------------------------------------------------------------------------------
echo
echo "Create specific header files:"
echo "-----------------------------"

	#Create a list of headers (all_headers.txt) with data type for each type of header (REQ_data_types.txt):
	join -t $'\t' <(awk -F "\t" '{print $3"\t"$1"\t"$4"\t"$5"\t"$6}' ${temp_f}/all_headers.txt|sort -t $'\t' -k1,1) <( grep -v '^#' ${req_files_folder}/REQ_data_types.txt|sort -t $'\t' -k1,1 )|awk -F "\t" '{print $2"\t"$0}'|cut -f3 --complement|sort -t $'\t' -k1,1n > ${temp_f}/all_headers+data_types.txt
	#Line before: 27/3/2013: join -t $'\t' <(awk -F "\t" '{print $3"\t"$1"\t"$4"\t"$5"\t"$6}' ${temp_f}/all_headers.txt|sort -k1,1) <( sort -k1,1 ${req_files_folder}/REQ_data_types.txt )|awk -F "\t" '{print $2"\t"$0}'|cut -f3 --complement|sort -k1,1n > ${temp_f}/all_headers+data_types.txt
	
		#all_headers+data_types.txt
		#-----------------------------------------------------------
		#1       SEQUENCE                        SEQUENCE        1       1       2       2       -       2       -       2
		#2       -                       -       3       2       1       2       -       3       -       1
		#3       -                       -       3       2       1       2       -       3       -       1
		#4       COMPARTMENT                     COMPARTMENT     1       1       1       2       -       2       -       1
		#5       FAMPROV                 FAMPROV 1       1       2       2       -       2       -       1
		#6       TREE_ALT_NAME                   TREE_ALT_NAME   3       1       1       2       -       2       -       1
		#7       BLOCK                   BLOCK   1       1       2       2       -       1       -       1
		#8       TREE                    TREE    1       1       2       2       -       1       -       1
		#9       E       1975-09-01            E_1975-09-01  1       1       1       2       -       1       REQ_obs_codes_conv_table2_sel_codes.txt      1
		#10      height  1975-09-01    cm      height_1975-09-01     1       1       1       1       5-300   2       -       1

	#Create a list of all observation fields (E, F, D, RT, QT) based on those present in ${req_files_folder}/REQ_obs_codes_conv_table2_sel_codes.txt
	cat ${temp_f}/all_headers+data_types.txt|awk -F "\t" '{print $2"\t"$1"\t"$3"\t"$12}'|sort -t $'\t' -k1,1|join -t $'\t' - <(cut -f1 ${req_files_folder}/REQ_obs_codes_conv_table2_sel_codes.txt |sort -u|grep -vE '^$|^#')|sort -t $'\t' -k3,3 -k1,1 > ${temp_f}/all_obs_fields.txt
	#Line before 30/8/2013: cat ${temp_f}/all_headers+data_types.txt|awk -F "\t" '{print $2"\t"$1"\t"$3"\t"$12}'|sort -k1,1|join -t $'\t' - <(cut -f1 ${req_files_folder}/REQ_obs_codes_conv_table.txt |sort -u|grep -vE '^$|^#')|sort -k3,3 -k1,1 > ${temp_f}/all_obs_fields.txt
		#all_obs_fields.txt
		#-----------------------------------------------------------
		#E       9       1975-09-01    REQ_obs_codes_conv_table2_sel_codes.txt
		#F       16      1975-09-01    REQ_obs_codes_conv_table2_sel_codes.txt
		#F       17      1975-09-01    REQ_obs_codes_conv_table2_sel_codes.txt
		#F       18      1975-09-01    REQ_obs_codes_conv_table2_sel_codes.txt
		#D       13      1975-09-01    REQ_obs_codes_conv_table2_sel_codes.txt
		#D       14      1975-09-01    REQ_obs_codes_conv_table2_sel_codes.txt
		#D       15      1975-09-01    REQ_obs_codes_conv_table2_sel_codes.txt
		#QT      11      1975-09-01    REQ_obs_codes_conv_table2_sel_codes.txt
		#RT      12      1975-09-01    REQ_obs_codes_conv_table2_sel_codes.txt
		#E       19      1981-09-01    REQ_obs_codes_conv_table2_sel_codes.txt



	#Create a list of all measurement fields (height, dbh) based on those present in ${req_files_folder}/REQ_reference_measurement_units.txt
	join -t $'\t' <(awk -F "\t" '{print $3"\t"$1"\t"$4"\t"$5}' ${temp_f}/all_headers.txt |sort -t $'\t' -k1,1) <(grep -v '^#' ${req_files_folder}/REQ_reference_measurement_units.txt|sort -t $'\t' -k1,1 )|sort -t $'\t' -k2,2 > ${temp_f}/all_meas_fields.txt
		#all_meas_fields.txt
		#-----------------------------------------------------------
		#height	10	1975-09-01	cm	cm
		#height	20	1981-09-01	cm	cm
		#dbh	21	1981-09-01	mm	mm
		#height	30	1986-09-01	cm	cm
		#dbh	31	1986-09-01	mm	mm
		#dbh	38	1998-09-01	mm	mm
		#height	39	1998-09-01	cm	cm
		#dbh	50	2000-09-01	cm	mm
		#dbh	55	2003-09-01	cm	mm
		#height	56	2003-09-01	dm	cm

printf "ok\n";




#------------------------------------------------------------------------------------------------------------------------------------------------------------
#14. Check if there already are measurement and/or observation data for those years in TreeSource
#------------------------------------------------------------------------------------------------------------------------------------------------------------
echo
echo "Checking if coded observations and/or measurements have already been integrated into TreeSource for those years"
echo "-----------------------------------------------"


#Coded observations
#List common years between Excel file coded observation columns and TreeSource coded observations for that site
join -t $'\t' <(cut -f3 ${temp_f}/all_obs_fields.txt|awk -F "-" '{print $1}'|sort -u) ${temp_f}/TreeSource_coded_observation_years_${site_code}.txt > ${temp_f}/file_and_db_common_obs_years.txt

#If some years repeated, warn and offer choice to try to import nevertheless, as this may work if these are new codes for the same years. If codes are the same, for the same DATES, it will fail at insertion.
if [ $(cat ${temp_f}/file_and_db_common_obs_years.txt|wc -l) -gt 0 ]
then
	printf "${red_color}There already are coded observations data in TreeSource for the following year(s):\n"
	perl -pe 's/^/\t/g' ${temp_f}/file_and_db_common_obs_years.txt
	printf "Do you still want to try to import them to the database ? Note: any duplicate in coded observation for a given date will cause an error at insertion time.${normal_color}\n"
	already_imp_data_choice="-";while [ ! $(echo $already_imp_data_choice|grep '[Yn]') ];do printf "\r\t"; read -p "[Y/n]?" -n1 -s already_imp_data_choice;done;printf "\n"
	if [ ${already_imp_data_choice} == "n" ];then printf "${red_color}\n***ABORTING***\n${normal_color}";exit;fi
else
	printf "Coded observations: ok, all are new.\n"
fi


#Measurements
#List common years between Excel file measurement columns and TreeSource measurements for that site
join -t $'\t' <(cut -f3 ${temp_f}/all_meas_fields.txt|awk -F "-" '{print $1}'|sort -u) ${temp_f}/TreeSource_measurement_years_${site_code}.txt > ${temp_f}/file_and_db_common_meas_years.txt

if [ $(cat ${temp_f}/file_and_db_common_meas_years.txt|wc -l) -gt 0 ]
then
	printf "${red_color}There already are measurements data in TreeSource for the following year(s):\n"
	perl -pe 's/^/\t/g' ${temp_f}/file_and_db_common_meas_years.txt
	printf "This would cause an error while trying to insert. Please review the $infile file and restart the script.${normal_color}\n"
	printf "${red_color}\n***ABORTING***\n${normal_color}"
	exit
else
	printf "Measurements: ok, all are new.\n"
fi


#------------------------------------------------------------------------------------------------------------------------------------------------------------
#15. Check if specified units exist in the reference table
#------------------------------------------------------------------------------------------------------------------------------------------------------------
#Put into a file units from input file that are not into the units conversion table
echo
echo "Specified units:"
echo "-----------------------------"

#Line before 18/4/2012: Was not taking into account empty units! cut -f5 ${temp_f}/all_headers.txt |sort -u|grep -v '^$'|join - <(cut -f1 ${req_files_folder}/REQ_units_conversion_table.txt |sort -u) -v1 > ${temp_f}/infile_units_not_in_table.txt


#List those units not present in the reference table, including blanks
cut -f4 ${temp_f}/all_meas_fields.txt|sort -u|join - <(cut -f1 ${req_files_folder}/REQ_units_conversion_table.txt |sort -u) -v1 > ${temp_f}/infile_units_not_in_table.txt

#Warn and exit if any of the units provided in the input file are not recognized
#Check if some units were left blank
if [ $(grep '^$' ${temp_f}/infile_units_not_in_table.txt|wc -l) -ne 0 ]; then printf "${red_color}There are blank values in the specified units\n";awk -F "\t" '{if ($4 == "") print $2"\t"$1"\t"$3}' ${temp_f}/all_meas_fields.txt|sort -t $'\t' -k1,1n|cat <(printf "Col\tmeas.\tdate\n") -|perl -pe 's/^/\t/g';printf "These units should be used:\n";cut -f1 ${req_files_folder}/REQ_units_conversion_table.txt |sort -u|perl -pe 's/^/\t/g';printf "${normal_color}";fnct_aborting;fi

#If not, chechk for other wrong units
if [ $(grep -v '^$' ${temp_f}/infile_units_not_in_table.txt|wc -l) -ne 0 ];then printf "${red_color}The following units were not recognized:\n";cat ${temp_f}/infile_units_not_in_table.txt|perl -pe 's/^/\t/g'; printf "These units should be used:\n";cut -f1 ${req_files_folder}/REQ_units_conversion_table.txt |sort -u|perl -pe 's/^/\t/g';printf "${normal_color}";fnct_aborting;fi

printf "ok\n"


#------------------------------------------------------------------------------------------------------------------------------------------------------------
#16. Check if every column has a valid date assigned to it
#------------------------------------------------------------------------------------------------------------------------------------------------------------
echo
echo "Specified dates:"
echo "-----------------------------"
#Among the columns that should have a date, list those that don't
#First part of the join is the listing of all column types that are listed in REQ_data_types.txt as requiring a date (e.g. E, D, F, RT, QT, height, dbh, COMMENT, etc.)
#2nd part of the join is listing those column types from the all_headers.txt that have actually no date with them
join -t $'\t' <(grep -v '^#' ${req_files_folder}/REQ_data_types.txt |cut -f1,10|grep '1$'|cut -f1|sort -u) <(awk -F "\t" '{print $3"\t"$4"\t"$1"\t"$2}' ${temp_f}/all_headers.txt |sort -t $'\t' -k1,1)|awk -F "\t" '{if ($2 !~ /^[12][0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9]$/) print}'|awk -F "\t" '{print $4"\t"3"\t"$2"\t"$1}'|sort -t $'\t' -k2,2 > ${temp_f}/columns_with_missing_dates.txt

if [ -s ${temp_f}/columns_with_missing_dates.txt ]
then
	printf "${red_color}The following columns have incorrect dates assigned:\n"
	
	cat <(printf "name\tcol#\tdate\ttype\n----\t----\t----\t----\n") ${temp_f}/columns_with_missing_dates.txt|perl -pe 's/^/\t/g'
	printf "\nThere should be a date for all those column types:\n"
	cat <(grep -v '^#' ${req_files_folder}/REQ_data_types.txt |cut -f1,10|grep '1$'|cut -f1|sort -u)|perl -pe 's/^/\t/g'
	printf "${normal_color}"
	fnct_aborting
	
else 
	printf "ok\n"

fi


#------------------------------------------------------------------------------------------------------------------------------------------------------------
#17. Check for replicated columns:
#------------------------------------------------------------------------------------------------------------------------------------------------------------
echo
echo "Replicated columns:"
echo "-----------------------------"
#
#find any column having data_type+date duplicated, except for those starting with -, F_ or D_ which are often replicated, and put the results in a file
#Line before 9/3/2012: join -t $'\t' <(cut -f6 ${temp_f}/all_headers.txt|grep -vE "^$|^${skip_char}|^F_|^D_"|sort|uniq -d) <(awk -F "\t" '{print $6"\t"$0}' ${temp_f}/all_headers.txt|sort -k1,1)|cut -f1 --complement|sort -k1,1 > ${temp_f}/replicated_columns.txt

#From the list of codes + dates, keep only those codes that are not allowed to appear more than once per date (gotten from REQ_data_types.txt, column 12), using an exclusive join (with the -v 1 option)
#If some of these codes appear more than once for a given date, fetch the rest of the info (column #, original header name, date, etc.) from all_headers.txt with a join and store them in ${temp_f}/replicated_columns.txt and WARN
join -t $'\t' <(cut -f3,4 ${temp_f}/all_headers.txt|sort -t $'\t' -k1,1) <(grep -v '^#' ${req_files_folder}/REQ_data_types.txt|awk -F "\t" '{if ($12 == "1") print $1}'|sort) -v1|sort|uniq -d|awk -F "\t" '{print $1"_"$2}'|sort|join -t $'\t' - <(paste <(cut -f6 ${temp_f}/all_headers.txt) ${temp_f}/all_headers.txt |sort -t $'\t' -k1,1)|cut -f1,7 --complement|sort -t $'\t' -k1,1 > ${temp_f}/replicated_columns.txt


if [ $(cat ${temp_f}/replicated_columns.txt|wc -l) -gt 0 ]
then
	printf "${red_color}The following columns are replicated :\n"
	cat <(printf "col#\tname\ttype\tdate\tunits\n----\t----\t----\t----\t------\n") <(cut -f1-5 ${temp_f}/replicated_columns.txt)|perl -pe 's/^/\t/g'
	printf "${normal_color}"
	fnct_aborting
else
	printf "none\n"
fi







#------------------------------------------------------------------------------------------------------------------------------------------------------------
#18. Prepare files for value substitutions before or after validation
#------------------------------------------------------------------------------------------------------------------------------------------------------------

#Added 20/3/2013
echo
echo "Preparing files for value substitutions specified in REQ_site-specific_values_substitution_utf.txt:"
echo "-----------------------------------------------------------------------------------------------"
	#A. From all_headers+data_types.txt, create a list of concatenated field names and years, and their column # in the original file
	#This will be used to screen for valid combinations of field name + year in the REQ_site-specific_values_substitution_utf.txt file
		cut -f1-3 ${temp_f}/all_headers+data_types.txt|perl -pe 's/\t(\d{4})-\d\d-\d\d/\t\1/g'|awk -F "\t" '{print $2"_"$3"\t"$0}'|sort -t $'\t' -k1,1 > ${temp_f}/all_headers_years_columnno_for_values_substitutions.txt
		#cut -f1-3 ${temp_f}/all_headers+data_types.txt|perl -pe 's/\t(\d{4})-\d\d-\d\d/\t\1/g'|awk -F "\t" '{if ($3 !~ /[0-9][0-9][0-9][0-9]$/) print $1"\t"$2"\tALL"; else print $0}'|awk -F "\t" '{print $2"_"$3"\t"$0}'|sort -k1,1 > ${temp_f}/all_headers_years_columnno_for_values_substitutions.txt

			#${temp_f}/all_headers_years_columnno_for_values_substitutions.txt (comes from all_headers+data_types.txt)
				#-_ALL	1	-	-
				#BLOCK_ALL	3	BLOCK	-
				#E_1999	7	E	1999
				#E_2000	11	E	2000
				#E_2001	13	E	2001
				#E_2007	17	E	2007
				#E_2012	21	E	2012
				#FAMPROV_ALL	5	FAMPROV	-

	#B. From REQ_site-specific_values_substitution_utf.txt extract lines for the current site or for ALL sites
		#Note: 28/7/2016: we now specify columns 1 through 6 instead of using $0, to get rid of potential additional comment columns at the end.
		grep -vE "^#|^$" ${req_files_folder}/REQ_site-specific_values_substitution_utf.txt|awk -F "\t" '{if($4 != "") print $1"\t"$2"\t"$3"\t"$4"\t"$5"\t"$6}'|grep -E "^${site_code}[[:space:]]|^ALL[[:space:]]"|cut -f1 --complement > ${temp_f}/value_substitutions_for_current_site.txt
			#${temp_f}/value_substitutions_for_current_site.txt (comes from REQ_site-specific_values_substitution_utf.txt)
				#FAMPROV	ALL	999	XXXXX	2
				#FAMPROV	ALL	9999	XXXXX	2
				#FAMPROVs	ALL	99999	XXXXX	2
				#D	2001	6	6d	1
				#D	2000	6	-	1
				#D	2007	6	6d	1
				#QT	2007,2008	6	6f	1
				#F	ALL	5	4b	1

	#C. From the ones in step B, get those for all years:
		awk -F "\t" -v IGNORECASE=1 '{if ($2 ~ /all/) print}' ${temp_f}/value_substitutions_for_current_site.txt|sort -t $'\t' -k1,1 > ${temp_f}/value_substitutions_for_current_site_all_years.txt
			#${temp_f}/value_substitutions_for_current_site_all_years.txt (comes from REQ_site-specific_values_substitution_utf.txt).
				#F	ALL	5	4b	1
				#FAMPROV	ALL	999	XXXXX	2
				#FAMPROV	ALL	9999	XXXXX	2
				#FAMPROVs	ALL	99999	XXXXX	2

		#Get the column position and years for those fields
			join -t $'\t'  ${temp_f}/value_substitutions_for_current_site_all_years.txt <(awk -F "\t" '{print $3"\t"$2"\t"$4}' ${temp_f}/all_headers_years_columnno_for_values_substitutions.txt|sort -t $'\t' -k1,1) > ${temp_f}/value_substitutions_for_current_site_all_years_existing_fields.txt
				#${temp_f}/value_substitutions_for_current_site_all_years_existing_fields.txt (combination of REQ_site-specific_values_substitution_utf.txt and all_headers+data_types.txt)
					#F	ALL	5	4b	1	10	1999
					#F	ALL	5	4b	1	16	2001
					#F	ALL	5	4b	1	20	2007
					#F	ALL	5	4b	1	25	2012
					#FAMPROV	ALL	999	XXXXX	2	5	-
					#FAMPROV	ALL	9999	XXXXX	2	5	-


		#Get the lines where field name is invalid (from REQ_site-specific_values_substitution_utf.txt)
		#28/7/2016: NO LONGER USED, AS REQ_site-specific_values_substitution_utf.txt values are validated above in section 5.A.
			#join -t $'\t'  ${temp_f}/value_substitutions_for_current_site_all_years.txt <(awk -F "\t" '{print $3"\t"$2"\t"$4}' ${temp_f}/all_headers_years_columnno_for_values_substitutions.txt|sort -t $'\t' -k1,1) -v1 > ${temp_f}/value_substitutions_for_current_site_all_years_NONEXISTENT_fields.txt
				#${temp_f}/value_substitutions_for_current_site_all_years_NONEXISTENT_fields.txt
					#FAMPROVs	ALL	99999	XXXXX	2
	
			#if [ -s ${temp_f}/value_substitutions_for_current_site_all_years_NONEXISTENT_fields.txt ]
			#then
			#	printf "\n"
			#	printf "${red_color}The following lines in the REQ_site-specific_values_substitution_utf.txt file have invalid field names!\nIf you choose to continue, they will be ignored.${normal_color}\n"
			#	perl -pe 's/^/\t/g' ${temp_f}/value_substitutions_for_current_site_all_years_NONEXISTENT_fields.txt
			#	choice="-";while [ ! $(echo $choice|grep '[yn]') ];do printf "\r\t"; read -p "Continue [y/n]?" -n1 -s choice;done;printf "\n"
			#	if [ $choice == "n" ]; then printf "${red_color}***ABORTED***${normal_color}\n";exit;fi
			#fi

	#D. From the valid ones in step B, get those for specific years:
		awk -F "\t" -v IGNORECASE=1 '{if ($2 !~ /all/) print $1"_"$2"\t"$0}' ${temp_f}/value_substitutions_for_current_site.txt|sort -t $'\t' -k1,1 > ${temp_f}/value_substitutions_for_current_site_specific_years.txt
		#${temp_f}/value_substitutions_for_current_site_specific_years.txt (comes from REQ_site-specific_values_substitution_utf.txt)
			#D_2000	D	2000	6	-	1
			#D_2001	D	2001	6	6d	1
			#D_2007	D	2007	6	6d	1
			#QT_2007,2008	QT	2007,2008	6	6f	1

		#Get the column position and years for those fields
			join -t $'\t'  ${temp_f}/value_substitutions_for_current_site_specific_years.txt <(awk -F "\t" '{print $1"\t"$2"\t"$4}' ${temp_f}/all_headers_years_columnno_for_values_substitutions.txt|sort -t $'\t' -k1,1) > ${temp_f}/value_substitutions_for_current_site_specific_years_existing_fields_and_years.txt
			#${temp_f}/value_substitutions_for_current_site_specific_years_existing_fields_and_years.txt (combination of REQ_site-specific_values_substitution_utf.txt and all_headers+data_types.txt)
				#D_2001	D	2001	6	6d	1	15	2001
				#D_2007	D	2007	6	6d	1	19	2007

		#Get the lines where field name or year are invalid
		#28/7/2016: NO LONGER USED, AS REQ_site-specific_values_substitution_utf.txt values are validated above in section 5.A.
			#join -t $'\t'  ${temp_f}/value_substitutions_for_current_site_specific_years.txt <(awk -F "\t" '{print $1"\t"$2"\t"$4}' ${temp_f}/all_headers_years_columnno_for_values_substitutions.txt|sort -t $'\t' -k1,1) -v1 > ${temp_f}/value_substitutions_for_current_site_specific_years_NONEXISTENT_fields_or_years.txt
				#${temp_f}/value_substitutions_for_current_site_specific_years_NONEXISTENT_fields_or_years.txt (combination of REQ_site-specific_values_substitution_utf.txt and all_headers+data_types.txt)
					#D_2000	D	2000	6	-	1
					#QT_2007,2008	QT	2007,2008	6	6f	1
	
			#if [ -s ${temp_f}/value_substitutions_for_current_site_specific_years_NONEXISTENT_fields_or_years.txt ]
			#then
			#	printf "\n"
			#	printf "${red_color}The following lines in the REQ_site-specific_values_substitution_utf.txt file have invalid field names AND/OR years, or they are simply not present in \nIf you choose to continue, they will be ignored.${normal_color}\n"
			#	perl -pe 's/^/\t/g' <(cut -f1 --complement ${temp_f}/value_substitutions_for_current_site_specific_years_NONEXISTENT_fields_or_years.txt)
			#	choice="-";while [ ! $(echo $choice|grep '[yn]') ];do printf "\r\t"; read -p "Continue [y/n]?" -n1 -s choice;done;printf "\n"
			#	if [ $choice == "n" ]; then printf "${red_color}***ABORTED***${normal_color}\n";exit;fi
			#fi


	#E. Join all + specific years
		cat <(awk -F "\t" '{print $1"\t"$6"\t"$7"\t"$5"\t"$3"\t"$4}' ${temp_f}/value_substitutions_for_current_site_all_years_existing_fields.txt) <(awk -F "\t" '{print $2"\t"$7"\t"$8"\t"$6"\t"$4"\t"$5}' ${temp_f}/value_substitutions_for_current_site_specific_years_existing_fields_and_years.txt)|sort -u|sort -t $'\t' -k1,1 -k2,2 > ${temp_f}/value_substitutions_for_current_site_0_all.txt
			#${temp_f}/value_substitutions_for_current_site_0_all.txt
				#F	10	1999	1	5	4b
				#F	16	2001	1	5	4b
				#F	20	2007	1	5	4b
				#F	25	2012	1	5	4b
				#FAMPROV	5	-	2	999	XXXXX
				#FAMPROV	5	-	2	9999	XXXXX
				#D	15	2001	1	6	6d
				#D	19	2007	1	6	6d

	#F. Split in different files according to the when_to_replace parameter (1 = before validation, 2 = after validation)
		awk -F "\t" '{if ($4 == "1") print}' ${temp_f}/value_substitutions_for_current_site_0_all.txt|cut -f4 --complement > ${temp_f}/value_substitutions_for_current_site_1_before_validation.txt
		awk -F "\t" '{if ($4 == "2") print}' ${temp_f}/value_substitutions_for_current_site_0_all.txt|cut -f4 --complement > ${temp_f}/value_substitutions_for_current_site_2_after_validation.txt
		awk -F "\t" '{if ($4 == "3") print}' ${temp_f}/value_substitutions_for_current_site_0_all.txt|cut -f4 --complement > ${temp_f}/value_substitutions_for_current_site_3_on_final_tabulated_file.txt


	#G. verify that only observation codes were used for modification of the final tabulated file (option 3)
	#28/7/2016: this is now checked for in section 5.A.
		#Put anything other than an observation code into an error list
		#Line before 30/8/2013: join -t $'\t' <(grep -vE '^#|^$' ${req_files_folder}/REQ_obs_codes_conv_table.txt|cut -f1|sort -u) <(sort -k1,1 ${temp_f}/value_substitutions_for_current_site_3_on_final_tabulated_file.txt) -v2 > ${temp_f}/value_substitutions_for_current_site_3_on_final_tabulated_file_NOT_A_CODED_OBS.txt
		#join -t $'\t' <(grep -vE '^#|^$' ${req_files_folder}/REQ_obs_codes_conv_table2_sel_codes.txt|cut -f1|sort -u) <(sort -t $'\t' -k1,1 ${temp_f}/value_substitutions_for_current_site_3_on_final_tabulated_file.txt) -v2 > ${temp_f}/value_substitutions_for_current_site_3_on_final_tabulated_file_NOT_A_CODED_OBS.txt
				
		#		if [ -s ${temp_f}/value_substitutions_for_current_site_3_on_final_tabulated_file_NOT_A_CODED_OBS.txt ]
		#		then
		#			printf "\n"
		#			printf "${red_color}The following lines (for final file substitutions) in the REQ_site-specific_values_substitution_utf.txt file DO NOT CONTAIN VALID OBSERVATION CODES!\nIf you choose to continue, they will be ignored.${normal_color}\n"
		#			perl -pe 's/^/\t/g'  <(cut -f2 --complement ${temp_f}/value_substitutions_for_current_site_3_on_final_tabulated_file_NOT_A_CODED_OBS.txt)
		#			choice="-";while [ ! $(echo $choice|grep '[yn]') ];do printf "\r\t"; read -p "Continue [y/n]?" -n1 -s choice;done;printf "\n"
		#			if [ $choice == "n" ]; then printf "${red_color}***ABORTED***${normal_color}\n";exit;fi
		#		fi

		
		#Keep only observation codes
		#Line before: 30/8/2013: join -t $'\t' <(grep -vE '^#|^$' ${req_files_folder}/REQ_obs_codes_conv_table.txt|cut -f1|sort -u) <(sort -k1,1 ${temp_f}/value_substitutions_for_current_site_3_on_final_tabulated_file.txt) > ${temp_f}/value_substitutions_for_current_site_3_on_final_tabulated_file_temp.txt
		#join -t $'\t' <(grep -vE '^#|^$' ${req_files_folder}/REQ_obs_codes_conv_table2_sel_codes.txt|cut -f1|sort -u) <(sort -t $'\t' -k1,1 ${temp_f}/value_substitutions_for_current_site_3_on_final_tabulated_file.txt) > ${temp_f}/value_substitutions_for_current_site_3_on_final_tabulated_file_temp.txt
		#mv ${temp_f}/value_substitutions_for_current_site_3_on_final_tabulated_file_temp.txt ${temp_f}/value_substitutions_for_current_site_3_on_final_tabulated_file.txt

	
	#H. For option 3, check that final observation codes, after substitution, already exist in the REQ_obs_codes_conv_table2_sel_codes.txt
	
	#List "after substitution" codes that are not in observation_codes.code:
	join <(cut -f5 ${temp_f}/value_substitutions_for_current_site_3_on_final_tabulated_file.txt|sort -u) <(sort ${temp_f}/TreeSource_all_obs_codes_only.txt) -v1 > ${temp_f}/nonexistent_codes.txt

		if [ -s ${temp_f}/nonexistent_codes.txt ]
		then
			printf "\n"
			printf "${red_color}The following codes (for final file substitutions) in the REQ_site-specific_values_substitution_utf.txt file DO NOT EXIST IN TREESOURCE!\nIf you accept them, you will be asked to provide a description for those new codes.\nIf you don't accept them, the script will be interrupted.${normal_color}\n"
			perl -pe 's/^/\t/g'  ${temp_f}/nonexistent_codes.txt
			choice="-";while [ ! $(echo $choice|grep '[yn]') ];do printf "${red_color}\r"; read -p "Accept [y/n]?" -n1 -s choice;done;printf "\n${normal_color}"
			if [ $choice == "n" ]; then printf "${red_color}***ABORTED***${normal_color}\n";exit;fi


			#User accepted the new codes, add them to TreeSource
			printf "" > ${temp_f}/SQL_insert_new_codes.txt
			
			
			echo
			echo "NEW CODES CREATION:"
			echo "-------------------"
			for new_code in $(cat ${temp_f}/nonexistent_codes.txt)
			do
				echo "${new_code}"
				echo "-----------"
				read -p "Description:" code_description
				code_description_dsquotes=$(echo ${code_description}|perl -pe "s/'/''/g")
				
				code_tree_is_dead="-";while [ ! $(echo $code_tree_is_dead|grep '[tf\?]') ];do printf "\r\t"; read -p "Does the code imply that tree is dead [t/f/?]?" -n1 -s code_tree_is_dead;done;printf "\n"
				if [ $code_tree_is_dead == "t" ]
				then
					code_tree_is_dead="TRUE"
				else
					if [ $code_tree_is_dead == "f" ];then code_tree_is_dead="FALSE";else code_tree_is_dead="NULL";fi
				fi
				
				
				code_tree_growth_is_affected="-";while [ ! $(echo $code_tree_growth_is_affected|grep '[tf\?]') ];do printf "\r\t"; read -p "Does the code imply that tree growth is affected [t/f/?]?" -n1 -s code_tree_growth_is_affected;done;printf "\n"
				if [ $code_tree_growth_is_affected == "t" ]
				then
					code_tree_growth_is_affected="TRUE"
				else
					if [ $code_tree_growth_is_affected == "f" ];then code_tree_growth_is_affected="FALSE";else code_tree_growth_is_affected="NULL";fi
				fi
				code_category_choice="-";while [ ! $(echo $code_category_choice|grep '[123]') ];do printf "\r\t"; read -p "Category: 1) Tree status  2) Tree damages  3) Tree deformations" -n1 -s code_category_choice;done;printf "\n"
				
				
				
					case $code_category_choice in
					1)
						code_category="Ãtat"
						;;
					2)
						code_category="Dommages"
						;;
					3)
						code_category="DÃ©formations"
						;;
					4)
						code_category="NOT YET DEFINED"
						;;
					*)
						code_category="NOT YET DEFINED"
						;;
					esac
				
				code_work_type="DendromÃ©trie"
				code_data_source_id=${data_source_id}
				
				printf "\tINSERT INTO observation_codes(code,description_fr,tree_is_dead,tree_growth_is_affected,category,work_type,data_source_id) VALUES('${new_code}','${code_description_dsquotes}',${code_tree_is_dead},${code_tree_growth_is_affected},'${code_category}','${code_work_type}',${code_data_source_id});\n" >> ${temp_f}/SQL_insert_new_codes.txt
				printf "\tNOTE: SQL commands to add those new codes into the database will appear in ${ready2import_tabs_folder}/SQL_insert_and_update1.txt\n"
				echo
				
			done
					
			#cat ${temp_f}/SQL_insert_new_codes.txt
		fi
	
	printf "ok\n"

	#Create a substitutions journal
	printf "" > ${temp_f}/subst_j.txt


##############################################
##############################################
##############################################
#exit 
##############################################
##############################################
##############################################




#------------------------------------------------------------------------------------------------------------------------------------------------------------
#19. Value substitutions BEFORE VALIDATION
#------------------------------------------------------------------------------------------------------------------------------------------------------------

#Wether or not there are substitutions before validation, copy cleaned_file0.txt to cleaned_infile1.txt
cp  ${temp_f}/cleaned_infile0.txt ${temp_f}/cleaned_infile1.txt


#If there are substitutions to be done before validation
if [ -s ${temp_f}/value_substitutions_for_current_site_1_before_validation.txt ]
then
	#Add a line to the substitutions journal
	printf "1. Substitution BEFORE validations (${temp_f}/cleaned_infile.txt ---> ${temp_f}/cleaned_infile1.txt):\n-----------------------------------\nType\tYear\tCol#\tSubst.\tRows\n----\t----\t----\t------\t----\n" >> ${temp_f}/subst_j.txt
	
	for before_validation_line in $(cat ${temp_f}/value_substitutions_for_current_site_1_before_validation.txt)
	do
		#echo $before_validation_line
		column_no=$(echo $before_validation_line|cut -f2)
		value_in=$(echo $before_validation_line|cut -f4)
		value_out=$(echo $before_validation_line|cut -f5)

		awk -F "\t" -v search_str=${value_in} -v replace_str=${value_out} -v awk_col_no=${column_no} -v OFS="\t" '{gsub("^"search_str"$",replace_str,$awk_col_no);print}' ${temp_f}/cleaned_infile1.txt > ${temp_f}/cleaned_infile_with_subst_before_val_temp.txt
	
		#Compte le nombre de substitutions effectuÃ©es en comparant par sdiff le fichier avant et aprÃ¨s substitutions
		substitutions_count=$(sdiff -s <(cut -f${column_no} ${temp_f}/cleaned_infile1.txt) <(cut -f${column_no} ${temp_f}/cleaned_infile_with_subst_before_val_temp.txt )|wc -l)
			#echo $substitutions_count
		#Ajoute au journal les paramÃ¨tres et le nombre de substitutions
		echo ${before_validation_line}|awk -F "\t" -v subst_count=${substitutions_count} '{print $1"\t"$3"\t"$2"\t"$4" ==> "$5":\t"subst_count}' >> ${temp_f}/subst_j.txt
		
		#Recopie le fichier temporaire sur le fichier principal
		mv ${temp_f}/cleaned_infile_with_subst_before_val_temp.txt ${temp_f}/cleaned_infile1.txt
	done

fi



#------------------------------------------------------------------------------------------------------------------------------------------------------------
#20. Validate data quality (measurements & observations)
#------------------------------------------------------------------------------------------------------------------------------------------------------------
echo
echo "Analyze each column's values:"
echo "----------------------------------------"
	
	#If a previous version of the column validation summary exists, drop it
	if [ -e ${validation_temp_f}/summary.txt ];then rm -r ${validation_temp_f}/summary.txt; fi

	#If a previous version of the column validation journal exists, drop it
	if [ -e ${validation_temp_f}/journal.txt ];then rm -r ${validation_temp_f}/journal.txt; fi


#Pass each line of all_headers+data_types.txt (i.e. means every column of the original file)
for infile_column in $(cat ${temp_f}/all_headers+data_types.txt)
do

#Setting variables to pass to validate_column.txt, extracted from all_headers+data_types.txt (all_headers.txt + REQ_data_types.txt)
	column_no=$(echo $infile_column|cut -f1)
	header_name=$(echo $infile_column|cut -f2)
	date=$(echo $infile_column|cut -f3)
	units=$(echo $infile_column|cut -f4); if [ -z $units ];then units="-";fi;units=$(echo $units|perl -pe 's/ //g') #space removal to avoid error message caused by validate_columns.txt (16/7/2012)
	header_name_date=$(echo $infile_column|cut -f5)
	data_type=$(echo $infile_column|cut -f6)
	
	ignore_zeros=$(echo $infile_column|cut -f7)
	nulls_allowed=$(echo $infile_column|cut -f8)
	calcs_on_numeric=$(echo $infile_column|cut -f9)
	min_max=$(echo $infile_column|cut -f10)
	show_value_list=$(echo $infile_column|cut -f11)
	ref_values_file=$(echo $infile_column|cut -f12)
	allow_repeats=$(echo $infile_column|cut -f13)
	column_name=$(printf "%03d_$header_name_date\n" $column_no)

	#If the header is ${skip_char}, that column will not be analyzed
	if [ $header_name == ${skip_char} ];then skip=1;else skip=0;fi
	
	
	#If a file is specified for reference values, add the REQ path before
	if [ $ref_values_file != "-" ];then ref_values_file="${req_files_folder}/${ref_values_file}";fi
	#echo $ref_values_file


	#echo "column_no='$column_no', header_name='$header_name', date='$date', units='${units}', header_name_date='$header_name_date', data_type='$data_type', ignore_zeros='$ignore_zeros', nulls_allowed='$nulls_allowed', calcs_on_numeric='$calcs_on_numeric', min_max='$min_max', show_value_list='$show_value_list', ref_values_file='$ref_values_file', allow_repeats='$allow_repeats'"

	#Extract the current column from cleaned_infile1.txt into a separate file
	cut -f${column_no} ${temp_f}/cleaned_infile1.txt > ${temp_f}/current_column.txt

	#Validate for the data type using validate_column.txt
	bash ${req_files_folder}/validate_column.txt ${temp_f}/current_column.txt ${column_name} 1 ${data_type} ${ignore_zeros} ${nulls_allowed} ${calcs_on_numeric} ${min_max} ${show_value_list} ${units} ${ref_values_file} ${header_name} ${on_yellow_alert} ${on_red_alert} ${validation_temp_f} ${skip} ${display_summary} ${allow_repeats} ${display_full_info}
	

	#If in test mode, stops after 10 analyzed columns
	if [ $test -eq 1 ]
	then
		if [ $column_no -eq 1 ]; then break;fi
	fi

done


ok_columns_count=$(grep 'OK' ${validation_temp_f}/summary.txt |cut -f1|sort -u|wc -l)
failed_columns_count=$(grep 'FAIL' ${validation_temp_f}/summary.txt |cut -f1|sort -u|wc -l)
check_columns_count=$(grep 'CHECK' ${validation_temp_f}/summary.txt |cut -f1|sort -u|wc -l)

echo ;echo
echo "-----------------------------------------------------------"
echo "Validation of the ${all_columns_count} columns in file ${1}"
echo "-----------------------------------------------------------"
echo
echo "Please read the main journal: ${validation_temp_f}/journal.txt"
echo "and the summary: ${validation_temp_f}/summary.txt"
echo
printf "STATUS\tCOUNT\n-----\t------\n"
printf "OK\t${ok_columns_count}\n"
printf "FAILED\t${failed_columns_count}\n"
printf "CHECK\t${check_columns_count}\n"
echo


if [ ${check_columns_count} -gt 0 ]
then
	printf "${red_color}The following columns have warnings, please read the journal file.${normal_color}\n"
	grep 'CHECK' wholesite_columns_validation_temp_folder/summary.txt |cut -f1,11|perl -pe 's/^/\t/g'
	printf "\n${red_color}If you have read the journal file and want to ignore the warnings, press \"y\" to continue\n${normal_color}"
	choice="-";while [  ! $(echo $choice|grep '[yn]') ];do printf "\r\t"; read -p "Continue [y/n]?" -n1 -s choice;done;printf "\n"
	if [ $choice == "n" ]; then printf "${red_color}***ABORTING***${normal_color}\n";exit;fi
fi


if [ ${failed_columns_count} -gt 0 ]
then
	printf "\n"
	printf "${red_color}The following columns have errors, please read the journal file.${normal_color}\n"
	grep 'FAIL' wholesite_columns_validation_temp_folder/summary.txt |cut -f1,11|perl -pe 's/^/\t/g'
	
	printf "${red_color}\nYOU MUST CORRECT THE ERRORS IN THE ORIGINAL FILE AND RESTART THIS SCRIPT AGAIN.\n"
	printf "***ABORTING***${normal_color}\n"
	exit 

fi


#------------------------------------------------------------------------------------------------------------------------------------------------------------
#21. Value substitutions AFTER validation
#------------------------------------------------------------------------------------------------------------------------------------------------------------

#Wether or not there are substitutions after validation, copy cleaned_infile1.txt to cleaned_infile2.txt
cp  ${temp_f}/cleaned_infile1.txt ${temp_f}/cleaned_infile2.txt


#If there are substitutions to be done before validation
if [ -s ${temp_f}/value_substitutions_for_current_site_2_after_validation.txt ]
then
	#Add a line to the substitutions journal
	printf "\n2. Substitution AFTER validations (${temp_f}/cleaned_infile1.txt ---> ${temp_f}/cleaned_infile2.txt):\n-----------------------------------\nType\tYear\tCol#\tSubst.\tRows\n----\t----\t----\t------\t----\n" >> ${temp_f}/subst_j.txt


	for after_validation_line in $(cat ${temp_f}/value_substitutions_for_current_site_2_after_validation.txt)
	do
		#echo $after_validation_line
		column_no=$(echo $after_validation_line|cut -f2)
		value_in=$(echo $after_validation_line|cut -f4)
		value_out=$(echo $after_validation_line|cut -f5)

		awk -F "\t" -v search_str=${value_in} -v replace_str=${value_out} -v awk_col_no=${column_no} -v OFS="\t" '{gsub("^"search_str"$",replace_str,$awk_col_no);print}' ${temp_f}/cleaned_infile2.txt > ${temp_f}/cleaned_infile_with_subst_after_val_temp.txt

		#Counts the number of substitutions by comparing (sdiff) the files before and after substitutions
		substitutions_count=$(sdiff -s <(cut -f${column_no} ${temp_f}/cleaned_infile2.txt) <(cut -f${column_no} ${temp_f}/cleaned_infile_with_subst_after_val_temp.txt )|wc -l)

		#Ajoute au journal les paramÃ¨tres et le nombre de substitutions
		echo ${after_validation_line}|awk -F "\t" -v subst_count=${substitutions_count} '{print $1"\t"$3"\t"$2"\t"$4" ==> "$5":\t"subst_count}' >> ${temp_f}/subst_j.txt
		#awk -F "\t" -v subst_count=${substitutions_count} '{print $1"\t"$3"\t"$2"\t"$4" ==> "$5":\t"subst_count}' ${temp_f}/value_substitutions_for_current_site_2_after_validation.txt >> ${temp_f}/subst_j.txt

		#Copy back temporary file on main file
		mv ${temp_f}/cleaned_infile_with_subst_after_val_temp.txt ${temp_f}/cleaned_infile2.txt
	done

fi






#------------------------------------------------------------------------------------------------------------------------------------------------------------
#22. Calculate the germplasm_name
#------------------------------------------------------------------------------------------------------------------------------------------------------------
echo
echo "Calculating germplasm names:"
echo "-----------------------------"

#[NEW 2010707] Note: presence of required fields (e.g. BLOCK, FAMPROV, TREE, COMPARTMENT, etc.) to calculate the germplasm name based on the tree_name_format parameter has already been checked around line #2100
#---> See required/optional columns in REQ_germplasm_name_fields.txt).
#---> Their column # in the original file is listed in: ${temp_f}/all_possible_fields_var_set.txt
#---> All values for each file is present in: BLOCK.txt, FAMPROV.txt and TREE.txt


#Extract to a single file every column in all_possible_fields_pos.txt, and clean the file.
#Example: FAMPROV.txt
#	20908
#	20908
#	20908
#	20916
#	20916
#	20916
#	20914
#	(...)
for req_opt_field_info in $(cat ${temp_f}/all_possible_fields_pos.txt )
do 
	header=$(echo $req_opt_field_info|cut -f1)
	col_no=$(echo $req_opt_field_info|cut -f2)
	column_filename="${header}.txt"
	#Get the ignore_zeros parameter for that data type from REQ_data_types.txt
	ignore_zeros=$(grep "^${header}[[:space:]]" ${req_files_folder}/REQ_data_types.txt|cut -f3)

	#echo
	#echo $column_filename, ignore_zeros=${ignore_zeros}
	#echo "-----------"
	
	#If the column exists in the cleaned file...
	if [ $col_no -ne 0 ]
	then
		#...extract it, remove header and put in a file named as the header_name + ".txt", for example BLOCK.txt
		cut -f${col_no} ${temp_f}/cleaned_infile2.txt|tail -n+2 > ${temp_f}/${column_filename}
		#head ${temp_f}/${column_filename}
		#Clean the file (remove leading/trailing spaces; if zeros ignored, replace with NULL; replace empty strings with NULL) 
		#...the resulting file is saved as the original one (e.g. BLOCK.txt)
		fnct_clean_values ${temp_f}/${column_filename} ${ignore_zeros}
	fi
done

#If there is no sequence, generate a SEQUENCE.txt file filled with 0's
if [ $SEQUENCE_col -eq 0 ]
then
	echo "NOTE: No sequence numbers for that site. The sequence number part in the tree name will be '00000'."
	nb_of_trees=$(cat ${temp_f}/BLOCK.txt|wc -l)
	for i in $(seq ${nb_of_trees}); do echo 0;done > ${temp_f}/SEQUENCE.txt
fi




#Left-padding of certain columns to appear in the tree names
#------------------------------------------------------------------------------------------------------
#Format some columns first, specifically those that may either contain text or numbers, to avoid complex conditional statements when left-padding is done within the awk/printf statement below.

#Example:
#	TREE.txt 				3, \N		--->for germplasms_on_sites.tree_number. 	No change in the original file. If an integer, will be imported as is. If NULL, it will be automatically replaced by a \N in the tabulated file for import.
#	TREE_left-padded.txt: 	003, XXX	---> for germplasm_names.name. 			If TREE is a number, will be left-padded. If anything else, will be replaced by "X+".

#Note: values in these files may have been substituted as specified in REQ_site-specific_values_substitution_utf.txt.



	#String of X's to be used as replacement when the value of a column below is unknown.
	string_of_xs="XXXXXXXXXX"
	
	
	
	#Determine the max number of digits found in fields that are REQUIRED and common to all name formats:
	#-----------------------------------------------------------------------------------------------------------------------------
	#Note: these are base values. For each name format below, these variables may be overwritten.
	
	#BLOCK
		#[NEW 20210708]: Determine the max number of digits for that field :
		max_digits_block_field=$(awk '{if ($1 ~ /^[0-9]+$/) print length($1)}' ${temp_f}/BLOCK.txt |sort -un|tail -1)
	
	#TREE: 		
		#[NEW 20210708]: Determine the max number of digits for that field :
		max_digits_tree_field=$(awk '{if ($1 ~ /^[0-9]+$/) print length($1)}' ${temp_f}/TREE.txt |sort -un|tail -1)
		
	
	#Required field wich will be padded the same way for all tree name formats below
	#--------------------------------------------------------------------------------------------------
	#Note: this could be changed to make like the others, and the famprovchars parameter could be dropped.

	#FAMPROV: if numeric, left-pad (e.g. 93 ---> 00093), otherwise leave as is (e.g. XXXXX stays the same).
		awk -v awk_famprovchars=$famprovchars '{if ($1 ~ /^[0-9]+$/) printf "%0"awk_famprovchars"d\n",$1;else print $1}' ${temp_f}/FAMPROV.txt > ${temp_f}/FAMPROV_left-padded.txt


#tree_name_format="S_B_F_T_S"	#for test only

#[NEW 20210707]: depending on the name format chosen, calculate the tree name.
#Note: all the required fields for those name formats have been verified above (line 2104+), so here is just the combination of those fields.
case $tree_name_format in
	
	"S_B_F_T_S")
		
		#Example: E410D2_05_00425_002_06194_00
		#05 = block #, 00425 = family #, 002 = tree #, 06194 = sequence #, 00 = clone #
		
		#NOTE: The left-padding, although unnecessary long for some fields (e.g. 3 digits for compartment tree numbers!) is kept for uniformity with names already existing in TreeSource, to make sure new measurements extracted with the current script will have the same tree names as measurements already in TreeSource.
		
		#Left-pad fields with number of digits specific to this tree name format
		#-------------------------------------------------------------------------------------
		#BLOCK
			max_digits_block_field=2	#set manually, overrides the value set automatically above
			#[NEW 20210708]: Set a variable containing an equivalent number of X's for unknown blocks
			unknown_block_no_replacer=$(echo ${string_of_xs:0:$max_digits_block_field})
			#[NEW 20210708]: left-pad with max_digits_block_field digits, or put the equivalent number of "X" when no block number is there.
			awk -v awk_mdbf=$max_digits_block_field -v awk_ubnr=$unknown_tree_no_replacer '{if ($1 ~ /^[0-9]+$/) printf "%0"awk_mdbf"d\n",$1;else print awk_ubnr}' ${temp_f}/BLOCK.txt > ${temp_f}/BLOCK_left-padded.txt
		
		
		
		#SEQUENCE
			#[NEW 20210708]: Determine the max number of digits for that field :
			
			#max_digits_seq_field=$(awk '{if ($1 ~ /^[0-9]+$/) print length($1)}' ${temp_f}/SEQUENCE.txt |sort -un|tail -1)	#Set automatically
			max_digits_seq_field=5	#Set manually
			
			#[NEW 20210708]: Set a variable containing an equivalent number of X's for unknown sequences
			unknown_seq_no_replacer=$(echo ${string_of_xs:0:$max_digits_seq_field})
			
			#[NEW 20210708]: left-pad with the max number of digits encountered in the sequence numbers, or put the equivalent number of "X" when no sequence number is there.
			awk -v awk_mdsf=$max_digits_seq_field -v awk_usnr=$unknown_seq_no_replacer '{if ($1 ~ /^[0-9]+$/) printf "%0"awk_mdsf"d\n",$1;else print awk_usnr}' ${temp_f}/SEQUENCE.txt > ${temp_f}/SEQUENCE_left-padded.txt

		#TREE:
			max_digits_tree_field=3	#set manually, overrides the value set automatically above
			#[NEW 20210708]: Set a variable containing an equivalent number of X's for unknown trees
			unknown_tree_no_replacer=$(echo ${string_of_xs:0:$max_digits_tree_field})
		
			#[NEW 20210708]: left-pad with the max number of digits encountered in the tree numbers, or put the equivalent number of "X" when no tree number is there.
			#Line before 20210708: awk '{if ($1 ~ /^[0-9]+$/) printf "%03d\n",$1;else print "XXX"}' ${temp_f}/TREE.txt > ${temp_f}/TREE_left-padded.txt
			awk -v awk_mdtf=$max_digits_tree_field -v awk_utnr=$unknown_tree_no_replacer '{if ($1 ~ /^[0-9]+$/) printf "%0"awk_mdtf"d\n",$1;else print awk_utnr}' ${temp_f}/TREE.txt > ${temp_f}/TREE_left-padded.txt
		
			
		
		#1. Site code is passed as a string from ${site_code} parameter
		#2. Block number from BLOCK_left-padded.txt, already left-padded above, passed here as a string (%s)
		#3. Family/provenance number is from FAMPROV_left-padded.txt file, already left-padded above, passed here as a string (%s).
		#4. Tree number is from TREE_left-padded.txt file, already left-padded above, passed here as a string (%s).
		#5. Sequence number from SEQUENCE_left-padded.txt, already left-padded above, passed here as a string (%s)
		#6. Clone number is passed as a string from ${clone_number} parameter, left-padded to 2 characters (%02d). SHOULD BE REMOVED AS IT NEVER SERVES.
		
		#Line before 20130903: paste ${temp_f}/${block_or_subblock_file} ${temp_f}/FAMPROV.txt ${temp_f}/TREE.txt ${temp_f}/SEQUENCE.txt |awk -F "\t" -v awk_SITE_CODE=$site_code -v awk_famprovchars=$famprovchars -v awk_clone_number=$clone_number  '{if ($2 ~ /^[0-9]+$/) printf awk_SITE_CODE"_%02d_%0"awk_famprovchars"d_%03d_%05d_%02d\n",$1,$2,$3,$4,awk_clone_number; else printf awk_SITE_CODE"_%02d_%s_%03d_%05d_%02d\n",$1,$2,$3,$4,awk_clone_number}'|cat <(printf "temp_germplasm_name\n") - > ${temp_f}/germplasm_names.txt
		paste ${temp_f}/BLOCK_left-padded.txt ${temp_f}/FAMPROV_left-padded.txt ${temp_f}/TREE_left-padded.txt ${temp_f}/SEQUENCE_left-padded.txt|awk -F "\t" -v awk_SITE_CODE=$site_code -v awk_clone_number=$clone_number  '{printf awk_SITE_CODE"_%s_%s_%s_%s_%02d\n",$1,$2,$3,$4,awk_clone_number}'|cat <(printf "temp_germplasm_name\n") - > ${temp_f}/germplasm_names.txt
		;;
	
	
	"S_B_BR_BC_F")
		
		#Example: E188B_1_11_4_134_5
		#1 = block, 11 = block row #, 4 = block col #, 134_5 = clone #
		#Notes: 
		#	the 134_5 portion (clone number) fits into the family/provenance (F) column. The "_" between "134" and "5" makes it confusing.
		#	In these names from New Brunswick tests, NO LEFT-PADDING is used.
		
		#1. Site code is passed as a string from ${site_code} parameter
		#2. Block number is from BLOCK.txt file, passed here as a string (%s).
		#3. Block row number is from block_row_no.txt, passed here as a string (%s).
		#4. Block col number is from block_col_no.txt, passed here as a string (%s).
		#3. Family/provenance number is from FAMPROV.txt file, passed here as a string (%s).
		paste ${temp_f}/BLOCK.txt ${temp_f}/block_row_no.txt ${temp_f}/block_col_no.txt ${temp_f}/FAMPROV.txt |awk -F "\t" -v awk_SITE_CODE=$site_code '{printf awk_SITE_CODE"_%s_%s_%s_%s\n",$1,$2,$3,$4}'|cat <(printf "temp_germplasm_name\n") - > ${temp_f}/germplasm_names.txt
		;;
	
	
	"S_B_F_T_C")
		
		#Example: SLC30498_14_20921_3_0750
		#14 = block #, 20921 = family/provenance #, 3 = compartment tree #, 0750 = compartment #

		#Left-pad fields with number of digits specific to this tree name format
		#-------------------------------------------------------------------------------------
			
			#BLOCK		
				max_digits_block_field=2	#set manually, overrides the value set automatically above
				#[NEW 20210708]: Set a variable containing an equivalent number of X's for unknown blocks
				unknown_block_no_replacer=$(echo ${string_of_xs:0:$max_digits_block_field})
				#[NEW 20210708]: left-pad with max_digits_block_field digits, or put the equivalent number of "X" when no block number is there.
				awk -v awk_mdbf=$max_digits_block_field -v awk_ubnr=$unknown_tree_no_replacer '{if ($1 ~ /^[0-9]+$/) printf "%0"awk_mdbf"d\n",$1;else print awk_ubnr}' ${temp_f}/BLOCK.txt > ${temp_f}/BLOCK_left-padded.txt

			#FAMPROV
				#See above
				
			#TREE
				#max_digits_tree_field: determined automatically above
				unknown_tree_no_replacer=$(echo ${string_of_xs:0:$max_digits_tree_field})
			
				#[NEW 20210708]: left-pad with the max number of digits encountered in the tree numbers, or put the equivalent number of "X" when no tree number is there.
				awk -v awk_mdtf=$max_digits_tree_field -v awk_utnr=$unknown_tree_no_replacer '{if ($1 ~ /^[0-9]+$/) printf "%0"awk_mdtf"d\n",$1;else print awk_utnr}' ${temp_f}/TREE.txt > ${temp_f}/TREE_left-padded.txt
	
			#COMPARTMENT
				#[NEW 20210708]: Determine automatically the max number of digits for that field :
				max_digits_comp_field=$(awk '{if ($1 ~ /^[0-9]+$/) print length($1)}' ${temp_f}/COMPARTMENT.txt |sort -un|tail -1)
				#max_digits_comp_field=5	#if want to set manually
				#[NEW 20210708]: Set a variable containing an equivalent number of X's for unknown compartments
				unknown_comp_no_replacer=$(echo ${string_of_xs:0:$max_digits_comp_field})
				#[NEW 20210708]: left-pad with the max number of digits encountered in the compartment numbers, or put the equivalent number of "X" when no compartment number is there.
				awk -v awk_mdcf=$max_digits_comp_field -v awk_ucnr=$unknown_comp_no_replacer '{if ($1 ~ /^[0-9]+$/) printf "%0"awk_mdcf"d\n",$1;else print awk_ucnr}' ${temp_f}/COMPARTMENT.txt > ${temp_f}/COMPARTMENT_left-padded.txt

		#1. Site code is passed as a string from ${site_code} parameter
		#2. Block number from BLOCK_left-padded.txt, already left-padded above, passed here as a string (%s)
		#3. Family/provenance number is from FAMPROV_left-padded.txt file, already left-padded above, passed here as a string (%s).
		#4. Tree number is from TREE_left-padded.txt file, already left-padded above, passed here as a string (%s).
		#5. Compartment number is from COMPARTMENT_left-padded.txt file, already left-padded above, passed here as a string (%s).
		
		paste ${temp_f}/BLOCK_left-padded.txt ${temp_f}/FAMPROV_left-padded.txt ${temp_f}/TREE_left-padded.txt ${temp_f}/COMPARTMENT_left-padded.txt|awk -F "\t" -v awk_SITE_CODE=$site_code '{printf awk_SITE_CODE"_%s_%s_%s_%s\n",$1,$2,$3,$4}'|cat <(printf "temp_germplasm_name\n") - > ${temp_f}/germplasm_names.txt
		
		
		
		;;
	*)
		
		;;
esac



#exit


printf "First 20 lines of calculated germplasm names:\n"
head -20 ${temp_f}/germplasm_names.txt|perl -pe 's/^/\t/g'


#Check if some names are repeated
tail -n+2 ${temp_f}/germplasm_names.txt |sort|uniq -d > ${temp_f}/repeated_germplasm_names.txt
if [ $(cat ${temp_f}/repeated_germplasm_names.txt|wc -l) -gt 0 ]
#If names repeated, warn and exit
then
	printf "WARNING - The following names are REPEATED:\n"
	cat ${temp_f}/repeated_germplasm_names.txt|perl -pe 's/^/\t/g'
	printf "Please, correct the names in ${infile} (${temp_f}/infile_utf.txt)\n"
	fnct_aborting
fi

#Add names to the cleaned file
paste ${temp_f}/cleaned_infile2.txt ${temp_f}/germplasm_names.txt  > ${temp_f}/temp.txt
mv ${temp_f}/temp.txt ${temp_f}/cleaned_infile2.txt
#check if column number is consistent
awk -F "\t" '{print NF}' ${temp_f}/cleaned_infile2.txt|sort|uniq -c > ${temp_f}/cleaned_file_col_count.txt
if [ $(cat ${temp_f}/cleaned_file_col_count.txt|wc -l) -gt 1 ]
then
	printf "WARNING - the number of columns in cleaned_file2.txt is inconsistent!\n"
	cat ${temp_f}/cleaned_file_col_count.txt|perl -pe 's/^/\t/g'
	fnct_aborting
else
	germplasm_name_col=$(awk -F "\t" '{print NF}' ${temp_f}/cleaned_infile2.txt|sort -u)
	echo "germplasm_name column: $germplasm_name_col"
fi

#------------------------------------------------------------------------------------------------------------------------------------------------------------
#23. TABULATED FILES CREATION
#------------------------------------------------------------------------------------------------------------------------------------------------------------
echo
echo "Generating tabulated files:"
echo "---------------------------"


#Note (15/1/2015): if existing measurements and/or coded observations were found in TreeSource, **empty** SQL INSERT scripts will be created for germplasms and germplasms_on_sites
#This is to turn off additional_data, for tests only
echo create_tabs_despite_additional_data: ${create_tabs_despite_additional_data}
if [ ${create_tabs_despite_additional_data} -eq 1 ];then additional_data=0;fi

#A. germplasms
#----------------------------------------------------------------
	printf "germplasms..."
	
	if [ $additional_data -eq 0 ] #No data already existing in TreeSource
	then
	
		#Get the number of records (headers removed) in cleaned_infile2.txt
		nb_of_records=$(tail -n+2 ${temp_f}/cleaned_infile2.txt|wc -l)
		
		#If species (SPECIES) column provided
			species_col_no=$(awk -F "\t" '{print $3"\t"$1}' ${temp_f}/all_headers.txt |grep '^SPECIES'|cut -f2)
			if [ ! -z $species_col_no ]
			then
				#Extract the species column and number each line
				cut -f${species_col_no} ${temp_f}/cleaned_infile2.txt|tail -n+2|cat -n|perl -pe 's/^ *(\d+)\t(\S+)/\2\t\1/g'|sort -t $'\t' -k1,1 > ${temp_f}/species_numbered.txt
				#Join the species abbreviated name with the real name using REQ_species_codes.txt. Any unrecognized abbreviation will get the '(Inconnu)' name, using a left join and a perl replacement
				join -t $'\t' ${temp_f}/species_numbered.txt <(cut -f2,3 ${req_files_folder}/REQ_species_codes.txt|sort -t $'\t' -k1,1) -a1 |awk -F "\t" '{print $3"\t"$2}'|perl -pe 's/^\t/(Inconnu)\t/g'|sort -t $'\t' -k1,1 > ${temp_f}/species_realname_numbered.txt
				#Get the species id from the real name and all_organisms.txt file
				join -t $'\t' ${temp_f}/species_realname_numbered.txt <(awk -F "\t" '{print $2"\t"$1}' ${all_species_filepath}|sort -t $'\t' -k1,1)|cut -f2,3|sort -t $'\t' -k1,1n > ${temp_f}/species_id_numbered.txt
				#Keep only the species id
				cut -f2 ${temp_f}/species_id_numbered.txt > ${temp_f}/extracted_col_species_id.txt
			else	
				#Create a extracted_col_species_id.txt containing the unique value specified in $organism_id
				for i in $(seq $nb_of_records);do echo ${organism_id};done > ${temp_f}/extracted_col_species_id.txt
			fi
		
		#If generation method (GEN_METH) provided
			gen_method_col_no=$(awk -F "\t" '{print $3"\t"$1}' ${temp_f}/all_headers.txt |grep '^GEN_METH'|cut -f2)
			if [ ! -z $gen_method_col_no ]
			then
				cut -f${gen_method_col_no} ${temp_f}/cleaned_infile2.txt|tail -n+2 > ${temp_f}/extracted_col_gen_method.txt
			else
				for i in $(seq $nb_of_records);do echo ${generation_method_code};done > ${temp_f}/extracted_col_gen_method.txt
			fi
		
			#Create the tabulated file by joining the different single-column files together and adding the invariable fields (e.g. $naming_organization_code, $data_source_id)
			#Note: single-column files have NO HEADER
			#Note: corrected for new structure (germplasm_name --> germplasm_id). 3/9/2013.
			paste <(cut -f${germplasm_name_col} ${temp_f}/cleaned_infile2.txt|tail -n+2) ${temp_f}/extracted_col_species_id.txt ${temp_f}/extracted_col_gen_method.txt|perl -pe "s/\n/\t${germplasm_type}\t${naming_organization_code}\t${owning_organization_code}\t${data_source_id}\t${date_psql_format}\n/g"|cat <(printf "temp_name\torganism_id\tgeneration_method_code\ttype\ttemp_naming_organization_code\towning_organization_code\tdata_source_id\ttemp_date_named\n") - > ${temp_f}/2import_germplasms.txt
			
			printf "done\n"

	else #There is already data in TreeSource for that site. Create empty tabulated file for germplasms.
		printf "${red_color}skipped: data already existing for ${site_code} in TreeSource!${normal_color}\n"
		#Create empty file that will tell the section 25 to ignore that tabulated file.
		printf "" > ${temp_f}/2import_germplasms.txt
	fi
	


#B. germplasms_on_sites
#----------------------------------------------------------------
	printf "germplasms_on_sites..."
	
	
	if [ $additional_data -eq 0 ] #No data already existing in TreeSource
	then
		#Look for column files produced at step 11 (e.g. BLOCK.txt, TREE.txt, ...) corresponding to fields listed in ${temp_f}/germplasms_on_sites_fields.txt, and put the columns together
		#germplasms_on_sites_fields.txt	
		#	BLOCK
		#	block_col_no
		#	block_row_no
		#	COMPARTMENT
		#	compartment_pos_x_within_site
		#	compartment_pos_y_within_site
		#	COMP_SEQ
		#	gos_comments
		#	reading_direction_two_way
		#	REPCOMPNO
		#	SEQUENCE
		#	site_part
		#	SUBBLOCK
		#	TREE
		#	tree_pos_x_within_compartment
		#	tree_pos_y_within_compartment
		#	X
		#	Y
		
		printf "" > ${temp_f}/temp_gos_opt.txt
		#Cycle through all column types possible in germplasms_on_sites
		for field in $(grep -v '^#' ${temp_f}/germplasms_on_sites_fields.txt)
		do
			if [ $test -eq 1 ];then printf "\n\tExtracting column: ${field}";fi
			
			#If there is a file for this column type that's been generated in step 11...
			if [ -s ${temp_f}/${field}.txt ]
			then 
				#...add it to a temp file
				paste ${temp_f}/temp_gos_opt.txt <(cat <(printf "${field}\n") ${temp_f}/${field}.txt) > ${temp_f}/temp_gos_opt2.txt
				mv ${temp_f}/temp_gos_opt2.txt ${temp_f}/temp_gos_opt.txt
			fi
		done
		#temp_gos_opt.txt
		#	BLOCK	COMPARTMENT	compartment_pos_x_within_site	compartment_pos_y_within_site	reading_direction_two_way	SEQUENCE	site_part	TREE	tree_pos_x_within_compartment	tree_pos_y_within_compartment
		#	11	10	1	1	A	0	A	1	1	1
		#	11	10	1	1	A	0	A	2	2	1
		#	11	10	1	1	A	0	A	3	3	1
		#	11	20	1	2	R	0	A	1	3	1
		#	11	20	1	2	R	0	A	2	2	1

		#germplasm_names.txt
		#	temp_germplasm_name
		#	SLC30498_11_20908_1_0010
		#	SLC30498_11_20908_2_0010
		#	SLC30498_11_20908_3_0010
		#	SLC30498_11_20916_1_0020
		#	SLC30498_11_20916_2_0020

		#remove first, empty column, then add germplasm names
		paste ${temp_f}/germplasm_names.txt <(cut -f1 --complement ${temp_f}/temp_gos_opt.txt) > ${temp_f}/temp_gos_main.txt

		#temp_gos_main.txt
		#	temp_germplasm_name	BLOCK	COMPARTMENT	compartment_pos_x_within_site	compartment_pos_y_within_site	reading_direction_two_way	SEQUENCE	site_part	TREE	tree_pos_x_within_compartment	tree_pos_y_within_compartment
		#	SLC30498_11_20908_1_0010	11	10	1	1	A	0	A	1	1	1
		#	SLC30498_11_20908_2_0010	11	10	1	1	A	0	A	2	2	1
		#	SLC30498_11_20908_3_0010	11	10	1	1	A	0	A	3	3	1
		#	SLC30498_11_20916_1_0020	11	20	1	2	R	0	A	1	3	1
		#	SLC30498_11_20916_2_0020	11	20	1	2	R	0	A	2	2	1


		#add other fields from variables
		#********************************************
		#***TEAMCODE_ok_20150108***
		tail -n+2 ${temp_f}/temp_gos_main.txt|perl -pe "s/(.+)/\1\t${site_code}\t${plantation_date}\t${plantation_date_is_approximate}\t${sub_location_name}\t${event}\t${group_type}\t${plantation_team_id}\t${data_source_id}/g" > ${temp_f}/temp_gos_main2.txt
		
		#temp_gos_main2.txt
		#	SLC30498_11_20908_1_0010	11	10	1	1	A	0	A	1	1	1	SLC30498	1995-05-15	TRUE	NULL	Plantation	Bloc et sous-bloc	\N	546
		#	SLC30498_11_20908_2_0010	11	10	1	1	A	0	A	2	2	1	SLC30498	1995-05-15	TRUE	NULL	Plantation	Bloc et sous-bloc	\N	546
		#	SLC30498_11_20908_3_0010	11	10	1	1	A	0	A	3	3	1	SLC30498	1995-05-15	TRUE	NULL	Plantation	Bloc et sous-bloc	\N	546
		#	SLC30498_11_20916_1_0020	11	20	1	2	R	0	A	1	3	1	SLC30498	1995-05-15	TRUE	NULL	Plantation	Bloc et sous-bloc	\N	546
		#	SLC30498_11_20916_2_0020	11	20	1	2	R	0	A	2	2	1	SLC30498	1995-05-15	TRUE	NULL	Plantation	Bloc et sous-bloc	\N	546


		#generate headers for required and optional fields and add to the "headerless" file (temp_gos_main2.txt)
		paste <(head -1 ${temp_f}/temp_gos_main.txt) <(printf "temp_site_code\tplantation_date\tdate_is_approximate\tsub_location_name\tevent\tgroup_type\tteam_id\tdata_source_id\n")|cat - ${temp_f}/temp_gos_main2.txt > ${temp_f}/temp_gos_main3.txt

		#temp_gos_main3.txt
		#	temp_germplasm_name	BLOCK	COMPARTMENT	compartment_pos_x_within_site	compartment_pos_y_within_site	reading_direction_two_way	SEQUENCE	site_part	TREE	tree_pos_x_within_compartment	tree_pos_y_within_compartment	temp_site_code	plantation_date	date_is_approximate	sub_location_name	event	group_type	team_id	data_source_id
		#	SLC30498_11_20908_1_0010	11	10	1	1	A	0	A	1	1	1	SLC30498	1995-05-15	TRUE	NULL	Plantation	Bloc et sous-bloc	\N	546
		#	SLC30498_11_20908_2_0010	11	10	1	1	A	0	A	2	2	1	SLC30498	1995-05-15	TRUE	NULL	Plantation	Bloc et sous-bloc	\N	546
		#	SLC30498_11_20908_3_0010	11	10	1	1	A	0	A	3	3	1	SLC30498	1995-05-15	TRUE	NULL	Plantation	Bloc et sous-bloc	\N	546
		#	SLC30498_11_20916_1_0020	11	20	1	2	R	0	A	1	3	1	SLC30498	1995-05-15	TRUE	NULL	Plantation	Bloc et sous-bloc	\N	546
		#	SLC30498_11_20916_2_0020	11	20	1	2	R	0	A	2	2	1	SLC30498	1995-05-15	TRUE	NULL	Plantation	Bloc et sous-bloc	\N	546



		#Replace headers (fnct_replace_headers function) with database headers, and generate final file (2import_germplasms_on_sites.txt)
			fnct_replace_headers ${temp_f}/temp_gos_main3.txt ${temp_f}/2import_germplasms_on_sites.txt germplasms_on_sites

		printf "done\n"
	
	else #There is already data in TreeSource for that site. Create empty tabulated file for germplasms_on_sites.
		printf "${red_color}skipped: data already existing for ${site_code} in TreeSource!${normal_color}\n"
		#Create empty file that will tell the section 25 to ignore that tabulated file.
		printf "" > ${temp_f}/2import_germplasms_on_sites.txt
	fi


#C. coded_observations_on_germplasms
#----------------------------------------------------------------
	printf "coded_observations_on_germplasms..."
	
	#From the all_obs_fields.txt file, list all tree status columns (e.g. 'E'), and put the column # associated with a date
	#Note: checking for replicated columns (i.e. type+date) was done in section 8
	printf "" > ${temp_f}/E_date_columns.txt
	for infile_E_column in $(grep "^${tree_status_obs_code}[[:space:]]" ${temp_f}/all_obs_fields.txt) #Line before 8/3/2012: for infile_E_column in $(grep '^E[[:space:]]' ${temp_f}/all_obs_fields.txt)
	do
		#Create a list containing the column number for each date (e.g. 1975-07-20	32)
		obs_date=$(echo $infile_E_column|cut -f3)
		col_no=$(echo $infile_E_column|cut -f2)
		printf "${obs_date}\t${col_no}\n" >> ${temp_f}/E_date_columns.txt
		
		#echo "obs_date=$obs_date, col_no=$col_no"
	done

	#Create a list of observation filenames based on observation types (e.g. E ---> E.txt, F ---> F.txt, D ---> D.txt, etc.)
	cut -f1 ${temp_f}/all_obs_fields.txt |sort -u|perl -pe 's/\n/.txt\n/g' > ${temp_f}/unique_obs_filenames.txt

	#Remove observation files generated from a previous run, if they exist.
	for obs_filename in $(cat ${temp_f}/unique_obs_filenames.txt)
	do
		if [ -e ${temp_f}/${obs_filename} ];then rm ${temp_f}/${obs_filename};fi
	done


	#${temp_f}/all_obs_fields.txt
		#E	5	1984-09-01	REQ_obs_codes_conv_table2_sel_codes.txt
		#E	6	1987-09-01	REQ_obs_codes_conv_table2_sel_codes.txt
		#F	10	1987-09-01	REQ_obs_codes_conv_table2_sel_codes.txt
		#F	9	1987-09-01	REQ_obs_codes_conv_table2_sel_codes.txt
		#D	8	1987-09-01	REQ_obs_codes_conv_table2_sel_codes.txt
		#E	11	1992-09-01	REQ_obs_codes_conv_table2_sel_codes.txt
		#F	14	1992-09-01	REQ_obs_codes_conv_table2_sel_codes.txt
		#F	15	1992-09-01	REQ_obs_codes_conv_table2_sel_codes.txt
		#D	13	1992-09-01	REQ_obs_codes_conv_table2_sel_codes.txt


	#Cycle through every line of all_obs_felds.txt
	#Treat all (including E and MAR) observation columns
	for infile_column in $(cat ${temp_f}/all_obs_fields.txt)
	do
		obs_type=$(echo $infile_column|cut -f1) 	#Read the type of observation (e.g. E, F, D, RT, QT, MAR)
		col_no=$(echo $infile_column|cut -f2)		#Read column number (applies to cleaned_infile2.txt)
		obs_date=$(echo $infile_column|cut -f3)		#Read the observation date
		obs_filename=${obs_type}.txt			#Define the observation filename

		#***TEAMCODE_OK_20150107***
		#Get date-based team id from the list '${temp_f}/date_employees_teamid.txt'
		team_id=$(grep "^${obs_date}[[:space:]]" ${temp_f}/date_employees_teamid.txt |cut -f3)
		if [ -z ${team_id} ];then team_id="NULL";fi
		
		#if there is NO corresponding E column for that date, warn and exit.
		#An "E" (health status) code is required
		#This is required for all but MAR observation types
		E_found_column=$(grep "^${obs_date}[[:space:]]" ${temp_f}/E_date_columns.txt |cut -f2)
		if [ -z ${E_found_column} ] && [ ${obs_type} != "MAR" ]
		then
			printf "${red_color}\tNo tree status (i.e. 'E') column was found for the date ${obs_date}!\n"
			printf "\tA tree status column (E) is required for all other observations (D,F, QT, RT, etc.) because:\n"
			printf "\t\t A. when a tree is present and alive (E01), a D=0, for example, will mean no damage.\n"
			printf "\t\t B. when a tree is absent (E03), a D=0 will mean NO DATA.\n${normal_color}"
			fnct_aborting
			exit
		fi
		
		#For test
		if [ $test -eq 1 ];then echo "obs_type=${obs_type}, col_no=${col_no}, obs_date=${obs_date}, Corresponding E col no=${E_found_column}";fi



		#If the observation file doesn't exist, create it. Otherwise, append to it
		#The file created below will contain:
			#1. germplasm_name
			#2. E column for the same date as the current observation field
			#3. current observation field
				#NOTE: columns 2 & 3 have leading/trailing spaces removed (perl -pe 's/^ +//g'|perl -pe 's/ +$//g')
			#4. date
			#5. date_is_approximate
			#6. team_id
			#7. data_source_id

		#NOTE1: could use the fnct_clean_values function instead of cleaning here directly
		#NOTE2: 	at this step, NO LINES ARE REMOVED (even those with empty values)
		#		ONLY leading/trailing spaces cleanup is done here.
		
		if [ ! -e ${temp_f}/${obs_filename} ]
		then
		#***TEAMCODE_ok_20150107***
			paste <(cut -f${germplasm_name_col} ${temp_f}/cleaned_infile2.txt|tail -n+2) <(cut -f${E_found_column} ${temp_f}/cleaned_infile2.txt|tail -n+2|perl -pe 's/^ +//g'|perl -pe 's/ +$//g') <(cut -f${col_no} ${temp_f}/cleaned_infile2.txt|tail -n+2|perl -pe 's/^ +//g'|perl -pe 's/ +$//g') |perl -pe "s/\n/\t${obs_date}\t${date_is_approximate}\t${team_id}\t${data_source_id}\n/g" > ${temp_f}/${obs_filename}
		else
			paste <(cut -f${germplasm_name_col} ${temp_f}/cleaned_infile2.txt|tail -n+2) <(cut -f${E_found_column} ${temp_f}/cleaned_infile2.txt|tail -n+2|perl -pe 's/^ +//g'|perl -pe 's/ +$//g') <(cut -f${col_no} ${temp_f}/cleaned_infile2.txt|tail -n+2|perl -pe 's/^ +//g'|perl -pe 's/ +$//g') |perl -pe "s/\n/\t${obs_date}\t${date_is_approximate}\t${team_id}\t${data_source_id}\n/g" >> ${temp_f}/${obs_filename}
		fi

	done
		#F.txt
		#E93E_01_01968_001_00001_00	1	0	1979-09-01	TRUE	NULL	88
		#E93E_01_01968_002_00002_00	3	0	1979-09-01	TRUE	NULL	88
		#E93E_01_01968_003_00003_00	1	0	1979-09-01	TRUE	NULL	88
		#E93E_01_01968_004_00004_00	3	0	1979-09-01	TRUE	NULL	88
		#E93E_01_01968_005_00005_00	1	0	1979-09-01	TRUE	NULL	88
		#(...)
		#E93E_01_01968_001_00001_00	1	0	1979-09-01	TRUE	NULL	88
		#E93E_01_01968_002_00002_00	3	0	1979-09-01	TRUE	NULL	88
		#E93E_01_01968_003_00003_00	1	0	1979-09-01	TRUE	NULL	88
		#E93E_01_01968_004_00004_00	3	0	1979-09-01	TRUE	NULL	88
		#E93E_01_01968_005_00005_00	1	0	1979-09-01	TRUE	NULL	88
		#(...)
		#E93E_01_01968_001_00001_00	1	0	1979-09-01	TRUE	NULL	88
		#E93E_01_01968_002_00002_00	3	0	1979-09-01	TRUE	NULL	88
		#E93E_01_01968_003_00003_00	1	0	1979-09-01	TRUE	NULL	88
		#E93E_01_01968_004_00004_00	3	0	1979-09-01	TRUE	NULL	88
		#E93E_01_01968_005_00005_00	1	0	1979-09-01	TRUE	NULL	88



	#List all unique observation files
	if [ $test -eq 1 ]
	then
		echo "unique observation filenames:"
		perl -pe 's/^/\t/g' ${temp_f}/unique_obs_filenames.txt
		echo
	fi
	
	#${temp_f}/unique_obs_filenames.txt
		#E.txt
		#F.txt
		#D.txt


	#Create a file that will contain all cleaned files names
	printf "" > ${temp_f}/cleaned_obs_files.txt

	#Cycle through each of the observation files (whatever the type, including E and MAR)
	for obs_file in $(cat ${temp_f}/unique_obs_filenames.txt)
	do
		obs_type=$(echo ${obs_file}|perl -pe 's/\.txt//g')		#type of observation (e.g. F) derived from filename (e.g. F.txt)
		obs_file_saved_errors=${obs_type}_saved_errors.txt		#Filename of the file to store lines where there are observations despite an abnormal tree status that shouldn't allow them (e.g. E<>1 and F/D/RT/QT <> 0). These lines should not exist
		obs_file_with_meas=${obs_type}_with_meas.txt		#Filename of the file to store germplasms+dates where >=1 observation was <> 0
		temp_fileA=${obs_type}_tempA_non0_marked.txt		#Filename of temp file A
		temp_fileB=${obs_type}_tempB_0s_removed.txt		#Filename of temp file B
		temp_fileC=${obs_type}_tempC_sorted_by_obs.txt		#Filename of temp file C
		obs_file_cleaned=${obs_type}_cleaned.txt			#Filename of final file
		ignore_zeros=$(grep "^${obs_type}[[:space:]]" ${req_files_folder}/REQ_data_types.txt|cut -f3)		#Get the value for the ignore_zeros column
		no_msmt_value=$(grep "^${obs_type}[[:space:]]" ${req_files_folder}/REQ_data_types.txt|cut -f11)	#No measurement value. Will be used to check if there are valide observations when tree status is abnormal (see fnct_clean_values), which SHOULD NOT BE. 
		
		
		#[ADDED: 12/7/2012]
		#Create an observation type-specific file for allowed observations depending on tree status
		#Based on the following file:
			#REQ_tree_status_and_allowed_observations.txt
			#List of allowed observations depending on the tree status
			#E	0	(NONE)
			#E	1	(ALL)
			#QcE	1	QcD
			#QcE	2	(ALL)
			#QcE	3	(ALL)
			#QcE	4	(ALL)
			#QcE	5	QcD
			#QcE	6	(NONE)
			#QcE	7	(NONE)
			#QcE	8	(ALL)
			#QcE	9	(ALL)
		
		#This allows only certain observations to be kept for specific tree statuses
		#For example, QcE_1 only allows the QcD observation to be kept, all others are discarded
		obs_file_allowed_tree_status=${obs_type}_allowed_tree_status.txt
		
		#Observation-specific file creation
		grep -vE "^$|^#" ${req_files_folder}/REQ_tree_status_and_allowed_observations.txt|cut -f1-3|grep -E "[[:space:]]${obs_type}$|(ALL)"|cut -f1,2|sort -u|grep "^${tree_status_obs_code}[[:space:]]"|cut -f2|sort > ${temp_f}/${obs_file_allowed_tree_status}
		#Example: QcD_allowed_tree_status.txt
			#1
			#2
			#3
			#4
			#5
			#8
			#9
			if [ $test -eq 1 ]
			then
				if [ ! -z $(grep "^${obs_type}$" ${req_files_folder}/REQ_obs_codes_other_than_tree_status.txt) ]
				then
					echo "$obs_type is allowed with the following tree statuses (${tree_status_obs_code}):"
					cat ${temp_f}/${obs_file_allowed_tree_status}|perl -pe 's/^/\t/g'
					read -p "Appuyez une touche"
				fi
			fi
		
		#This value is typically of 1 (yes, 0 values will be ignored) for QT and RT, and 2 (no, 0 values will be kept) for F, D

		if [ $test -eq 1 ]
		then
			printf "Now cleaning ${obs_file}\n"
			printf "\tobs_file_saved_errors $obs_file_saved_errors\n"
			printf "\tobs_file_with_meas $obs_file_with_meas\n"
			printf "\ttemp_fileA $temp_fileA\n"
			printf "\ttemp_fileB $temp_fileB\n"
			printf "\ttemp_fileC $temp_fileC\n"
			printf "\tobs_file_cleaned $obs_file_cleaned\n"
		fi
		
		#If observation is of any other type than E (e.g. F, D, QT or RT)
		if [ ! -z $(grep "^${obs_type}$" ${req_files_folder}/REQ_obs_codes_other_than_tree_status.txt) ] #Line before 8/3/2012: if [ ${obs_type} == "F" ] || [ ${obs_type} == "D" ] || [ ${obs_type} == "QT" ] || [ ${obs_type} == "RT" ]
		then
			fnct_clean_FMQTRT_files #Clean the file to remove lines that do not fit the criteria: observations <> 0 when tree status is abnormal (e.g. E > 1 for CFS)
			#F_tempB_0s_removed.txt
			#E93E_01_01968_001_00001_00		1	0	1979-09-01	TRUE	123	88	N
			#E93E_01_01968_003_00003_00		1	0	1979-09-01	TRUE	123	88	N
			#E93E_01_01968_005_00005_00		1	0	1979-09-01	TRUE	123	88	N
		
		#If observation is of any other type (e.g. E or MAR)
		else
			#The original file (e.g. E.txt) will be copied into a temp file (e.g. E_tempB_0s_removed.txt), to bring it at the same step as other obs types
			cp ${temp_f}/${obs_file} ${temp_f}/${temp_fileB}
			#E_tempB_0s_removed.txt
			#E292P1_01_05193_001_00417_00	1	1	1975-09-01	TRUE	123	99
			#E292P1_01_05193_002_00418_00	1	1	1975-09-01	TRUE	123	99
			#E292P1_01_05193_003_00419_00	1	1	1975-09-01	TRUE	123	99
			#E292P1_01_05193_004_00420_00	1	1	1975-09-01	TRUE	123	99
			#E292P1_01_05193_005_00421_00	1	1	1975-09-01	TRUE	123	99
		fi

		##########################################################
		#ADDED 19/1/2012:
		#At that point, if there are still observations = 0 (or the no-measurement value) and ignore_zeros=1 (yes), then remove those lines
		#This will be done for any type of observation (E included)
		##########################################################
		 
		if [ $ignore_zeros -eq 1 ]
		then
			awk -F "\t" -v awk_no_msmt_value=${no_msmt_value} '{if ($3 != awk_no_msmt_value) print }' ${temp_f}/${temp_fileB} > ${temp_f}/temp.txt
			mv ${temp_f}/temp.txt ${temp_f}/${temp_fileB}
			#E93E_01_01968_081_00081_00      1       5       1974-09-01      TRUE    NULL    88      Y
			#E93E_01_01968_101_00101_00      1       1       1974-09-01      TRUE    NULL    88      Y
			#E93E_01_01968_117_00117_00      1       2       1974-09-01      TRUE    NULL    88      Y
			#E93E_03_01968_027_08955_00      1       2       1974-09-01      TRUE    NULL    88      Y
		fi
		
				
		#Reformat to have the sorted observation in field #1
		awk -F "\t" '{print $3"\t"$1"\t"$4"\t"$5"\t"$6"\t"$7}' ${temp_f}/${temp_fileB}|sort -t $'\t' -k1,1 -k2,2 -k3,3 > ${temp_f}/${temp_fileC}
			#F_tempC_sorted_by_obs.txt
			#0       E292P1_01_05193_001_00417_00    1975-09-01    TRUE    123   99
			#0       E292P1_01_05193_001_00417_00    1981-09-01    TRUE    123   99
			#0       E292P1_01_05193_002_00418_00    1975-09-01    TRUE    123   99
			#0       E292P1_01_05193_002_00418_00    1981-09-01    TRUE    123   99
			#0       E292P1_01_05193_002_00418_00    1986-09-01    TRUE    123   99
			#0       E292P1_01_05193_003_00419_00    1986-09-01    TRUE    123   99
			#0       E292P1_01_05193_003_00419_00    1998-09-01    TRUE    123   99
			#0       E292P1_01_05193_004_00420_00    1975-09-01    TRUE    123   99
			#0       E292P1_01_05193_004_00420_00    1981-09-01    TRUE    123   99
			#0       E292P1_01_05193_004_00420_00    1986-09-01    TRUE    123   99
	
		#Replace the numeric observation with the real code, and format for the final file
		#Line before 30/8/2013: join -t $'\t' <(grep -v '^#' ${req_files_folder}/REQ_obs_codes_conv_table.txt |grep "^${obs_type}[[:space:]]" |cut -f2,3|sort -k1,1) ${temp_f}/${temp_fileC} |awk -F "\t" '{print $3"\t"$2"\t"$4"\t"$5"\t"$6"\t"$7}'|sort -k1,1 -k3,3 -k2,2 > ${temp_f}/${obs_file_cleaned}
		join -t $'\t' <(grep -v '^#' ${req_files_folder}/REQ_obs_codes_conv_table2_sel_codes.txt |grep "^${obs_type}[[:space:]]" |cut -f2,3|sort -t $'\t' -k1,1) ${temp_f}/${temp_fileC} |awk -F "\t" '{print $3"\t"$2"\t"$4"\t"$5"\t"$6"\t"$7}'|sort -t $'\t' -k1,1 -k3,3 -k2,2 > ${temp_f}/${obs_file_cleaned}
		
			#NOTE: any line with an EMPTY VALUE for the observation will be removed here because it has no equivalent in the  conversion table
			
			#F_cleaned.txt
			#---------------
			#E292P1_01_05193_001_00417_00    F_0     1975-09-01    TRUE    123   99
			#E292P1_01_05193_001_00417_00    F_0     1981-09-01    TRUE    123   99
			#E292P1_01_05193_001_00417_00    F_2     1986-09-01    TRUE    123   99
			#E292P1_01_05193_002_00418_00    F_0     1975-09-01    TRUE    123   99
			#E292P1_01_05193_002_00418_00    F_0     1981-09-01    TRUE    123   99
			#E292P1_01_05193_002_00418_00    F_0     1986-09-01    TRUE    123   99
			#E292P1_01_05193_003_00419_00    F_2     1975-09-01    TRUE    123   99
			#E292P1_01_05193_003_00419_00    F_2     1981-09-01    TRUE    123   99
			#E292P1_01_05193_003_00419_00    F_0     1986-09-01    TRUE    123   99
			#E292P1_01_05193_003_00419_00    F_0     1998-09-01    TRUE    123   99

		#Put the cleaned file name into a file:
		echo ${obs_file_cleaned} >> ${temp_f}/cleaned_obs_files.txt

		#if [ ${obs_type} == "F" ];then break;fi	

	done


	#Join all observation files into one
	printf "" > ${temp_f}/temp_all_cleaned_obs.txt

	for cleaned_file in $(cat ${temp_f}/cleaned_obs_files.txt)
	do
		cat ${temp_f}/${cleaned_file} >> ${temp_f}/temp_all_cleaned_obs.txt
	done

	#Add the header, remove the NULLs (eventually coming from REQ_obs_codes_conv_table2_sel_codes.txt) and create the final file
	#***TEAMCODE_ok_20150107***
	cat <(printf "temp_germplasm_name\tobs_code\tdate\tdate_is_approximate\tteam_id\tdata_source_id\n") <(sort -t $'\t' -k1,1 -k3,3 -k2,2 ${temp_f}/temp_all_cleaned_obs.txt|awk -F "\t" '{if ($2 != "NULL") print}') > ${temp_f}/2import_coded_observations_on_germplasms.txt
	
	printf "done\n"





	#If there are substitutions to be done after tabulated files have been created
	#Added: 20/3/2013
	if [ -s ${temp_f}/value_substitutions_for_current_site_3_on_final_tabulated_file.txt ]
	then
		#Add a line to the substitutions journal
		printf "\n3. Substitution on final tabulated file (${temp_f}/2import_coded_observations_on_germplasms.txt):\n-----------------------------------\nType\tYear\tCol#*\tSubst.\tRows\n----\t----\t----\t------\t----\n" >> ${temp_f}/subst_j.txt
		
		#Make a backup of the original tabulated file
		cp  ${temp_f}/2import_coded_observations_on_germplasms.txt ${temp_f}/2import_coded_observations_on_germplasms_BKP_before_modifications.txt
		
		for final_tab_file_line in $(cat ${temp_f}/value_substitutions_for_current_site_3_on_final_tabulated_file.txt|cut -f1,3-5|sort -u)
		#D	1984    D_9     D03
		#D	1984    D99     D_9e
		do
			#echo $final_tab_file_line
			year=$(echo $final_tab_file_line|cut -f2)
			value_in=$(echo $final_tab_file_line|cut -f3)
			value_out=$(echo $final_tab_file_line|cut -f4)

			awk -F "\t" -v search_str=${value_in} -v replace_str=${value_out} -v awk_year=${year} -v OFS="\t" '{if ($3 ~ awk_year) gsub("^"search_str"$",replace_str,$2);print}' ${temp_f}/2import_coded_observations_on_germplasms.txt > ${temp_f}/2import_coded_observations_on_germplasms_temp.txt
			
			#Counts the number of substitutions by comparing (sdiff) the files before and after substitutions
			substitutions_count=$(sdiff -s <(cut -f2 ${temp_f}/2import_coded_observations_on_germplasms.txt) <(cut -f2 ${temp_f}/2import_coded_observations_on_germplasms_temp.txt)|wc -l)
			#Ajoute au journal les paramÃ¨tres et le nombre de substitutions
			echo ${final_tab_file_line}|awk -F "\t" -v subst_count=${substitutions_count} '{print $1"\t"$2"\t\t"$3" ==> "$4":\t"subst_count}' >> ${temp_f}/subst_j.txt
			#Copy back temporary file on main file
			mv ${temp_f}/2import_coded_observations_on_germplasms_temp.txt ${temp_f}/2import_coded_observations_on_germplasms.txt

		done
		printf "\t*column position in original file. Always column #2 in final tabulated file.\n" >> ${temp_f}/subst_j.txt
	fi

#exit

#D. tree_mensurations
#----------------------------------------------------------------
	printf "tree_mensurations..."

	#Create a list of measurement filenames based on measurement types (e.g. dbh ---> dbh.txt, height ---> height.txt, etc.)
	cut -f1 ${temp_f}/all_meas_fields.txt |sort -u|perl -pe 's/\n/.txt\n/g' > ${temp_f}/unique_meas_filenames.txt

	#Remove measurement files generated from a previous run, if they exist.
	for meas_filename in $(cat ${temp_f}/unique_meas_filenames.txt)
	do
		if [ -e ${temp_f}/${meas_filename} ];then rm ${temp_f}/${meas_filename};fi
	done


	#Cycle through every line of all_meas_felds.txt
	#Treat all measurement columns
	for infile_column in $(cat ${temp_f}/all_meas_fields.txt)
	do

		meas_type=$(echo $infile_column|cut -f1) 	#Read the type of measurement (e.g. dbh, height)
		col_no=$(echo $infile_column|cut -f2)		#Read column number (applies to cleaned_infile2.txt)
		meas_date=$(echo $infile_column|cut -f3)	#Read the measurement date
		units_from=$(echo $infile_column|cut -f4)	#Read the units in the original file
			#if [ ${meas_type} == "reading_sequence_number" ]; then units_from="-";fi
			#if [ ${meas_type} == "reading_direction" ]; then units_from="-";fi
		units_to=$(echo $infile_column|cut -f5)		#Read the reference units to which they should be converted
		meas_filename=${meas_type}.txt			#Define the measurement filename
		meas_year=$(expr substr ${meas_date} 1 4)	#Gets the measurement year with a substring (using the expr command)
		
		#Get date-based team id from the list '${temp_f}/date_employees_teamid.txt'
		#***TEAMCODE_OK_20150107***
		team_id=$(grep "^${meas_date}[[:space:]]" ${temp_f}/date_employees_teamid.txt |cut -f3)
		if [ -z ${team_id} ];then team_id="NULL";fi
		
		#For test
		if [ $test -eq 1 ]; then echo "meas_type=${meas_type}, col_no=${col_no}, meas_date=${meas_date}, units_from=${units_from}, units_to=${units_to}";fi


		#When needed (when units_to <> "-"), get the conversion factor from units_from, units_to and REQ_units_conversion_table.txt
		if [ $units_to != "-" ]
		then
			conv_factor=$(grep "^${units_from}[[:space:]]${units_to}[[:space:]]" ${req_files_folder}/REQ_units_conversion_table.txt |cut -f3)
		fi
		
		#Clean the column:
		#If the measurement file doesn't exist, create it. Otherwise, append to it
		#The file created below will contain:
			#1. germplasm_name
			#2. current measurement
				#NOTES: 
					#1. remove leading/trailing spaces (perl -pe 's/^ +//g'|perl -pe 's/ +$//g')
					#2. replace commas with periods
					#3. convert from original to reference units, removing along unnecessary trailing decimal digits (e.g. 0 in 1.30) (awk)
					#4. replace 0s/empties with NULLs. NULLs MUST be kept for the join between different measurement types for a given date
			#3. date
			#4. date_is_approximate
			#5. team_id
			#6. data_source_id
			#7. year
		#NOTE: could use the fnct_clean_values function instead of cleaning here directly
		if [ ! -e ${temp_f}/${meas_filename} ]
		then
			#If no units conversion is needed (units_to="-"), don't include it
			if [ $units_to == "-" ]
			then
				#***TEAMCODE_ok_20150107***
				paste <(cut -f${germplasm_name_col} ${temp_f}/cleaned_infile2.txt|tail -n+2) <(cut -f${col_no} ${temp_f}/cleaned_infile2.txt|tail -n+2|perl -pe 's/^ +//g'|perl -pe 's/ +$//g'|perl -pe 's/,/./g'|perl -pe 's/(^0$|^$)/NULL/g') |perl -pe "s/\n/\t${meas_date}\t${date_is_approximate}\t${team_id}\t${data_source_id}\t${meas_year}\n/g" > ${temp_f}/${meas_filename}
			else
				paste <(cut -f${germplasm_name_col} ${temp_f}/cleaned_infile2.txt|tail -n+2) <(cut -f${col_no} ${temp_f}/cleaned_infile2.txt|tail -n+2|perl -pe 's/^ +//g'|perl -pe 's/ +$//g'|perl -pe 's/,/./g'|awk -v awk_cf=${conv_factor} '{print $1*awk_cf}'|perl -pe 's/(^0$|^$)/NULL/g') |perl -pe "s/\n/\t${meas_date}\t${date_is_approximate}\t${team_id}\t${data_source_id}\t${meas_year}\n/g" > ${temp_f}/${meas_filename}
			fi
		else
			#If no units conversion is needed (units_to="-"), don't include it
			if [ $units_to == "-" ]
			then
				#***TEAMCODE_ok_20150107***
				paste <(cut -f${germplasm_name_col} ${temp_f}/cleaned_infile2.txt|tail -n+2) <(cut -f${col_no} ${temp_f}/cleaned_infile2.txt|tail -n+2|perl -pe 's/^ +//g'|perl -pe 's/ +$//g'|perl -pe 's/,/./g'|perl -pe 's/(^0$|^$)/NULL/g') |perl -pe "s/\n/\t${meas_date}\t${date_is_approximate}\t${team_id}\t${data_source_id}\t${meas_year}\n/g" >> ${temp_f}/${meas_filename}
			else
				paste <(cut -f${germplasm_name_col} ${temp_f}/cleaned_infile2.txt|tail -n+2) <(cut -f${col_no} ${temp_f}/cleaned_infile2.txt|tail -n+2|perl -pe 's/^ +//g'|perl -pe 's/ +$//g'|perl -pe 's/,/./g'|awk -v awk_cf=${conv_factor} '{print $1*awk_cf}'|perl -pe 's/(^0$|^$)/NULL/g') |perl -pe "s/\n/\t${meas_date}\t${date_is_approximate}\t${team_id}\t${data_source_id}\t${meas_year}\n/g" >> ${temp_f}/${meas_filename}
			fi
		fi

	done


	#Check for measurements inconsistency with time (e.g. decreasing height)
	for meas_filename in $(cat ${temp_f}/unique_meas_filenames.txt)
	do
		meas_type=${meas_filename%.txt} #Remove .txt from filename
		#If it is a real measurement (e.g. dbh, height), the calcs_on_numeric variable in REQ_data_types.txt will be = 1
		calcs_on_numeric=$(grep "^${meas_type}[[:space:]]" ${req_files_folder}/REQ_data_types.txt |cut -f5)
		units=$(grep "^${meas_type}[[:space:]]" ${req_files_folder}/REQ_reference_measurement_units.txt|cut -f2)
		
		if [ $test -eq 1 ]; then echo meas_type: $meas_type, calcs_on_numeric: $calcs_on_numeric;fi
		
		#If this is a real measurement field
		if [ $calcs_on_numeric -eq 1 ]
		then
			printf "Checking for inconsistent ${meas_type} measurements..."
			
			#Prepare the measurement file: sort by germplasm name and date, remove the NULL measurements
			cut -f1-3 ${temp_f}/${meas_filename}|sort -t $'\t' -k1,1 -k3,3|grep -v '[[:space:]]NULL[[:space:]]' > ${temp_f}/temp_file2check.txt
				#${temp_f}/temp_file2check.txt
				#E410D4_01_00001_001_00597_00    120     1987-09-01
				#E410D4_01_00001_001_00597_00    370     1992-09-01
				#E410D4_01_00001_001_00601_00    126     1987-09-01
				#E410D4_01_00001_001_00601_00    460     1992-09-01
				#E410D4_01_00001_002_00598_00    133     1987-09-01
				#E410D4_01_00001_002_00598_00    395     1992-09-01
				#E410D4_01_00001_002_00602_00    161     1987-09-01
				#E410D4_01_00001_002_00602_00    450     1992-09-01
				#E410D4_01_00001_003_00599_00    77      1987-09-01
				#E410D4_01_00001_003_00599_00    280     1992-09-01

			#Create error the files
			echo "start" > ${temp_f}/${meas_type}_sort_error.txt
			printf "" > ${temp_f}/${meas_type}_problematic_names.txt
			printf "" > ${temp_f}/${meas_type}_problematic_lines.txt

			#Do as long as there are sort errors
			while [ -s ${temp_f}/${meas_type}_sort_error.txt ]
			do
				#Put the sort error message into a file
				sort -t $'\t' -k1,1 -k2,2n -c ${temp_f}/temp_file2check.txt 2> ${temp_f}/${meas_type}_sort_error.txt
				if [ $test -eq 1 ]; then cat <(printf "sort error:\n") <(sort -t $'\t' -k1,1 -k2,2n -c ${temp_f}/temp_file2check.txt);fi
				
				#If the error message is there
				if [ -s ${temp_f}/${meas_type}_sort_error.txt ]
				then
					printf "."
					#Grab from the error message: 1) line number, 2) individual name
					#error_info=$(perl -pe 's/^sort:.+:(\d+): disorder: (\S+)\t(\S+)\t(\S+)/\1\t\2\t\3\t\4/g' ${temp_f}/${meas_type}_sort_error.txt)
					line_to_remove=$(perl -pe 's/^sort.+:(\d+):.+: *(\S+)\s.+/\1\t\2/g' ${temp_f}/${meas_type}_sort_error.txt|cut -f1)
					#Line before 28/3/2013 (did not work when error message was in French: 'dÃ©sordre'): line_to_remove=$(perl -pe 's/^sort:.+:(\d+): disorder: (\S+).+/\1\t\2/g' ${temp_f}/${meas_type}_sort_error.txt|cut -f1)
					#
					#line_to_remove=$(echo $error_info|cut -f1 -d " ") #get line number
					indiv_name=$(perl -pe 's/^sort.+:(\d+):.+: *(\S+)\s.+/\1\t\2/g' ${temp_f}/${meas_type}_sort_error.txt|cut -f2)
					#Line before 28/3/2013 (did not work when error message was in French: 'dÃ©sordre'):  indiv_name=$(perl -pe 's/^sort:.+:(\d+): disorder: (\S+).+/\1\t\2/g' ${temp_f}/${meas_type}_sort_error.txt|cut -f2)
					#indiv_name=$(echo $error_info|cut -f2 -d " ") #get individual name
					
					if [ $test -eq 1 ]; then printf "\n\tLine to remove: ${line_to_remove}\n\tIndividual name: ${indiv_name}\n";fi
					
					#Append the problematic line to a separate file
					sed -n "${line_to_remove},${line_to_remove}p" ${temp_f}/temp_file2check.txt >> ${temp_f}/${meas_type}_problematic_lines.txt
					
					#Append the individual name to a separate file
					echo ${indiv_name} >> ${temp_f}/${meas_type}_problematic_names.txt
					
					#Remove the problematic line from the analyzed file
					sed "${line_to_remove}d" ${temp_f}/temp_file2check.txt > ${temp_f}/temp_file2check_2.txt
					mv ${temp_f}/temp_file2check_2.txt ${temp_f}/temp_file2check.txt
					
					#if [ $(cat problematic_lines.txt|wc -l) -eq ${threshold2stop} ];then printf "\nThe maximum set number of errors (${threshold2stop}) was met!\nSTOPPING\n\n"; break ;fi
				fi
				
			done
			#Number of errors
			errors_count=$(cat ${temp_f}/${meas_type}_problematic_lines.txt|wc -l)


			if [ $errors_count -gt 0 ]
			then
				#Create a tentative SQL UPDATE script to correct the erraneous lines
				#Line before 30/8/2013: awk -v awk_meas_type=${meas_type} -v awk_units=${units} -F "\t" '{print "UPDATE tree_mensurations SET possible_data_error=TRUE, comments=(CASE WHEN comments IS NULL THEN '\''Valeur de "awk_meas_type" ("$2" "awk_units") infÃ©rieure Ã  la mesure faite antÃ©rieurement!'\'' ELSE comments||'\''. '\''||'\''Valeur de "awk_meas_type" ("$2" "awk_units") infÃ©rieure Ã  la mesure faite antÃ©rieurement!'\'' END) WHERE germplasm_name='\''"$1"'\'' AND date='\''"$3"'\'';"}' ${temp_f}/${meas_type}_problematic_lines.txt|perl -pe 's/^/\t/g'|cat <(echo "--Correction of erraneous ${meas_type} values.") - <(echo)> ${temp_f}/SQL_update_meas_${meas_type}.txt
				awk -v awk_meas_type=${meas_type} -v awk_units=${units} -F "\t" '{print "UPDATE tree_mensurations SET possible_data_error=TRUE, comments=(CASE WHEN comments IS NULL THEN '\''Valeur de "awk_meas_type" ("$2" "awk_units") infÃ©rieure Ã  la mesure faite antÃ©rieurement!'\'' ELSE comments||'\''. '\''||'\''Valeur de "awk_meas_type" ("$2" "awk_units") infÃ©rieure Ã  la mesure faite antÃ©rieurement!'\'' END) WHERE germplasm_id=fnct_get_germplasm_id_from_refname('\''"$1"'\'') AND date='\''"$3"'\'';"}' ${temp_f}/${meas_type}_problematic_lines.txt|perl -pe 's/^/\t/g'|cat <(echo "--Correction of erraneous ${meas_type} values.") - <(echo)> ${temp_f}/SQL_update_meas_${meas_type}.txt
						
				#Put together the correct, remaining lines, and the problematic lines, keep only lines for problematic individuals
				cat <(perl -pe 's/\n/\t<===\n/g' ${temp_f}/${meas_type}_problematic_lines.txt) <(perl -pe 's/\n/\t\n/g' ${temp_f}/temp_file2check.txt)|sort -t $'\t' -k1,1|join -t $'\t' - <(sort -u ${temp_f}/${meas_type}_problematic_names.txt)|sort -t $'\t' -k1,1 -k3,3 > ${temp_f}/${meas_type}_final_sort_errors.txt
				final_sort_errors_lines_count=$(cat ${temp_f}/${meas_type}_final_sort_errors.txt|wc -l)
				printf "\n${red_color}WARNING: ${errors_count} suspicious measurements were found. Here are the first 20 lines, with marked possible errors:${normal_color}\n\n"
				
				cat <(printf "${temp_f}/${meas_type}_final_sort_errors.txt\n--------------------------------------------\n") <(head -20 ${temp_f}/${meas_type}_final_sort_errors.txt) <(echo $(if [ $final_sort_errors_lines_count -gt 20 ];then printf "(...${final_sort_errors_lines_count})\n";else printf "";fi))

				printf "\n${red_color}PLEASE VIEW the ${temp_f}/${meas_type}_final_sort_errors.txt file.${normal_color}\nIf theses values are normal, or if they are not normal but you choose to correct them later:\n\tPress Y to accept them and continue\n\tPress N to stop\n\n"  
				choice="-";while [  ! $(echo $choice|grep '[yn]') ];do printf "\r\t"; read -p "Continue [y/n]?" -n1 -s choice;done;printf "\n"
				if [ $choice == "n" ]; then printf "${red_color}***ABORTING***${normal_color}\n";exit;fi
			else
				printf "none found.\n"
			fi

		fi

	done
	

	#If there were measurements abnormalities, create a SQL script to insert a comment in observations_on_sites
	#Added: 26/2/2013
	if [ $(cat ${temp_f}/*_problematic_lines.txt|wc -l) -gt 0 ]
	then 
		echo
		if [ ${verification_of_aberrant_meas} -eq 1 ]
		then
			#***TEAMCODE_ok_20150107***
			grep '^#MV1' ${req_files_folder}/REQ_aberrant_meas_messages_utf.txt|cut -f2|perl -pe 's/'\''/'\'''\''/g' |perl -pe 's/^/\tINSERT INTO observations_on_sites(observation,temp_site_code,date, date_is_approximate, team_id,data_source_id,action) VALUES ('\''/g'|perl -pe "s/\n/"\'","\'"${site_code}"\'",now(),FALSE,fnct_get_team_from_employees("\'"SÃ©bastien ClÃ©ment"\'"),${data_source_id},"\'"Validation"\'");\n/g" > ${temp_f}/SQL_insert_observations_on_sites.txt
			printf "\t${blue_color}NOTE: SQL commands to annotate those errors both at the measurements (tree_mensurations) and site (observations_on_sites) levels, into the database will appear in ${ready2import_tabs_folder}/SQL_insert_and_update2.txt${normal_color}\n"  

		else
			grep '^#MV' ${req_files_folder}/REQ_aberrant_meas_messages_utf.txt|grep -v '#MV1'|cut -f2|perl -pe 's/'\''/'\'''\''/g' |perl -pe 's/^/\tINSERT INTO observations_on_sites(observation,temp_site_code,date, date_is_approximate, team_id,data_source_id,action) VALUES ('\''/g'|perl -pe "s/\n/"\'","\'"${site_code}"\'",now(),FALSE,fnct_get_team_from_employees("\'"SÃ©bastien ClÃ©ment"\'"),${data_source_id},"\'"Validation"\'");\n/g" > ${temp_f}/SQL_insert_observations_on_sites.txt
			printf "\t${blue_color}Please edit the part related to observations_on_sites in the script ${ready2import_tabs_folder}/SQL_insert_and_update2.txt to add a comment on the site telling how potentially aberrant measurements were verified.${normal_color}\n"  

		fi
	fi
	
	#[20210708]: this parameter was removed in favor of tree_name_format.
	#If sub-block is used instead of block for the tree name, prepare a SQL command to put a note in observations_on_sites
	#if [ ${use_subblock_in_tree_name} -eq 2 ]
	#then
	#	#***TEAMCODE_ok_20150107***
	#	printf "INSERT INTO observations_on_sites(observation,temp_site_code,date,date_is_approximate,team_id,data_source_id,action) VALUES ('Le numÃ©ro de sous-bloc a Ã©tÃ© utilisÃ© au lieu du numÃ©ro de bloc dans le nom des arbres de ce site.','${site_code}',now(),FALSE,fnct_get_team_from_employees('SÃ©bastien ClÃ©ment'),${data_source_id},'Tree identification');\n" > ${temp_f}/SQL_insert_observations_on_sites2.txt
	#fi
	
	

	#Join on the same line different measurement types (e.g. height, dbh) for a same date
	#
	#Create a base file made of all unique germplasm_name+date concatenations from all measurement files found in unique_meas_filenames.txt, along with other repeated fields: date, date_is_approximate, team_id, data_source_id, year
	printf "" > ${temp_f}/all_measurements.txt
	for meas_file in $(cat ${temp_f}/unique_meas_filenames.txt);do cat ${temp_f}/$meas_file >> ${temp_f}/all_measurements.txt;done

	#Remove measurement column and replicated lines
	awk -F "\t" '{print $1"_"$3"\t"$1"\t"$3"\t"$4"\t"$5"\t"$6"\t"$7}' ${temp_f}/all_measurements.txt|sort -u|sort -t $'\t' -k1,1 > ${temp_f}/meas_base.txt
		#meas_base.txt
		#----------------
		#SLC30498_11_20901_1_0160_2002-09-15	SLC30498_11_20901_1_0160	2002-09-15	TRUE	NULL	546	2002
		#SLC30498_11_20901_1_0160_2007-09-15	SLC30498_11_20901_1_0160	2007-09-15	TRUE	NULL	546	2007
		#SLC30498_11_20901_1_0160_2017-09-15	SLC30498_11_20901_1_0160	2017-09-15		TRUE	NULL	546	2017
		#SLC30498_11_20901_2_0160_2002-09-15	SLC30498_11_20901_2_0160	2002-09-15	TRUE	NULL	546	2002
		#SLC30498_11_20901_2_0160_2007-09-15	SLC30498_11_20901_2_0160	2007-09-15	TRUE	NULL	546	2007
		#SLC30498_11_20901_2_0160_2017-09-15	SLC30498_11_20901_2_0160	2017-09-15		TRUE	NULL	546	2017
		#SLC30498_11_20901_3_0160_2002-09-15	SLC30498_11_20901_3_0160	2002-09-15	TRUE	NULL	546	2002
		#SLC30498_11_20901_3_0160_2007-09-15	SLC30498_11_20901_3_0160	2007-09-15	TRUE	NULL	546	2007

	if [ $test -eq 1 ]
	then 
		echo "Joining together all measurements of the same date:"
	fi

	#Cycle through all fines in unique_meas_filenames.txt, and extract only the germplasm_name+date and measurement
	#then join it with the base file
	#e.g. unique_meas_filenames.txt
	#	dbh.txt
	#	height.txt
	#	leader_shoot_length.txt
	
	for meas_file in $(cat ${temp_f}/unique_meas_filenames.txt )
	#e.g. meas_file=dbh.txt
	#	SLC30498_11_20908_1_0010	61	2007-09-15	TRUE	NULL	546	2007
	#	SLC30498_11_20908_2_0010	49	2007-09-15	TRUE	NULL	546	2007
	#	SLC30498_11_20908_3_0010	48	2007-09-15	TRUE	NULL	546	2007
	#	SLC30498_11_20916_1_0020	21	2007-09-15	TRUE	NULL	546	2007
	#	SLC30498_11_20916_2_0020	NULL	2007-09-15	TRUE	NULL	546	2007
	
	do
		if [ $test -eq 1 ]; then printf "\t${meas_file}\n";fi
		
		#Put tree name+date and measurement in a temporary file
		awk -F "\t" '{print $1"_"$3"\t"$2}' ${temp_f}/${meas_file}|sort -t $'\t' -k1,1 > ${temp_f}/temp_current_meas_col.txt
		#e.g.
		#	SLC30498_11_20901_1_0160_2002-09-15	53
		#	SLC30498_11_20901_2_0160_2002-09-15	13
		#	SLC30498_11_20901_3_0160_2002-09-15	44
		#	SLC30498_11_20902_1_0070_2002-09-15	23
		#	SLC30498_11_20902_2_0070_2002-09-15	57

		#If this is the first measurement file, the filename to paste column into will be meas_base.txt (see example above), else meas_base+meas.txt
		if [ $(head -1 ${temp_f}/unique_meas_filenames.txt) == $meas_file ]
		then
			file2pasteinto="meas_base.txt"
		else
			file2pasteinto="meas_base+meas.txt"
		fi

		#Join the base and measurement files. Note that the base file contains all dates, while measurement file may miss some dates for some measurement types, hence the -a 1 option (LEFT join)
		join -t $'\t' ${temp_f}/${file2pasteinto} ${temp_f}/temp_current_meas_col.txt -a 1 > ${temp_f}/temp_meas_base+meas.txt
		
		#SLC30498_11_20901_1_0160_2002-09-15	SLC30498_11_20901_1_0160	2002-09-15	TRUE	NULL	546	2002
		#SLC30498_11_20901_1_0160_2007-09-15	SLC30498_11_20901_1_0160	2007-09-15	TRUE	NULL	546	2007	66	<===msmt added
		#SLC30498_11_20901_1_0160_2017-09-15	SLC30498_11_20901_1_0160	2017-09-15		TRUE	NULL	546	2017	127	<===msmt added
		#SLC30498_11_20901_2_0160_2002-09-15	SLC30498_11_20901_2_0160	2002-09-15	TRUE	NULL	546	2002
		#SLC30498_11_20901_2_0160_2007-09-15	SLC30498_11_20901_2_0160	2007-09-15	TRUE	NULL	546	2007	60	<===msmt added
		#SLC30498_11_20901_2_0160_2017-09-15	SLC30498_11_20901_2_0160	2017-09-15		TRUE	NULL	546	2017	135	<===msmt added
		#SLC30498_11_20901_3_0160_2002-09-15	SLC30498_11_20901_3_0160	2002-09-15	TRUE	NULL	546	2002
		#SLC30498_11_20901_3_0160_2007-09-15	SLC30498_11_20901_3_0160	2007-09-15	TRUE	NULL	546	2007	52	<===msmt added
		
		
		
		
		#Uniformize column numbers, putting a NULL value where no measurements could be linked in the LEFT join
		bash ${req_files_folder}/uniformize_col_no.txt ${temp_f}/temp_meas_base+meas.txt NULL FALSE ${temp_f}/meas_base+meas.txt
		#exit 	#**********************************	
	done




	#Count the number of measurement types (e.g. dbh + height + 
	unique_meas_filenames_count=$(cat ${temp_f}/unique_meas_filenames.txt|wc -l)
	#Generate a string (e.g. "[[:space:]]NULL[[:space:]]NULL") to cull the lines where 1 single measurement is NULL, or where ALL measurements are null
	nulls_string=$(for i in $(seq 1 ${unique_meas_filenames_count});do printf "[[:space:]]NULL";done)


	#Remove lines where ALL measurements are NULL, and add headers
	#***TEAMCODE_ok_20150108***
	#25/5/2015: date ---> msmt_date, year ---> msmt_year
	cat <(printf "temp_germplasm_name\tmsmt_date\tdate_is_approximate\tteam_id\tdata_source_id\tmsmt_year\t$(perl -pe 's/\.txt\n/\t/g' ${temp_f}/unique_meas_filenames.txt|perl -pe 's/\t$//g')\n") <(grep -v "${nulls_string}$" ${temp_f}/meas_base+meas.txt|cut -f1 --complement) > ${temp_f}/2import_tree_mensurations.txt

	printf "done\n"


#E. alternative_germplasm_names
#TO BE REMADE COMPLETELY (8/1/2015)
#1. TO ACCOMODATE NEW STRUCTURE (germplasm_name ---> germplasm_id)
#2. ALSO:  the following fields ARE NO LONGER IN THE PARAMETERS:
#	naming_team_code ---> Constant (does not change within a single column). Different than team responsible for measurements (changes with years), this one should be stable
#	date_named 		---> Constant. Date should be the same as that of the earliest measurement columns. Only one date should apply to all alternative names, even if there are >1 alternive name columns.
#	description		---> Constant. Can be "Nom d'arbre sélectionné", or others.
#	refers_to		---> Constant. Usually NULL, could be a "Clone" name in some cases? TO BE VERIFIED..
#	site_specificity	---> Constant. Usually NULL. Mostly for seedlots, not for trees.
#	test_specificity	---> Constant. Usually NULL. Mostly for seedlots, not for trees.
#	
#Only this field remains in the parameters, as it is also used to give the reference names (e.g. E560A3_01_01234_03_05678_00) earlier in the script
#	naming_organization_code
#----------------------------------------------------------------
	printf "alternative_germplasm_names..."

	alt_name_filename="tree_alt_name.txt"
	printf "" > ${temp_f}/${alt_name_filename}

	for infile_column in $(awk -F "\t" '{print $3"\t"$1}' ${temp_f}/all_headers.txt|grep '^TREE_ALT_NAME[[:space:]]')
	do
		col_no=$(echo $infile_column|cut -f2)	#Read column number (applies to cleaned_infile2.txt)
		

	#Clean the column:
		#If the measurement file doesn't exist, create it. Otherwise, append to it
		#The file created below will contain:
			#1. germplasm_name
			#2. alternative_name
				#NOTES: 
					#1. remove leading/trailing spaces (perl -pe 's/^ +//g'|perl -pe 's/ +$//g')
					#2. replace 0s/empties with NULLs.
			#(...)
		#NOTE: could use the fnct_clean_values function instead of cleaning here directly
		if [ ! -e ${temp_f}/${alt_name_filename} ]
		then
			#***TEAMCODE_TO_BE_FIXED***
			paste <(cut -f${germplasm_name_col} ${temp_f}/cleaned_infile2.txt|tail -n+2) <(cut -f${col_no} ${temp_f}/cleaned_infile2.txt|tail -n+2|perl -pe 's/^ +//g'|perl -pe 's/ +$//g'|perl -pe 's/(^0$|^$)/NULL/g') |perl -pe "s/\n/\t${description}\t${refers_to}\t${date_named}\t${naming_team_code}\t${naming_organization_code}\t${site_specificity}\t${test_specificity}\t${data_source_id}\n/g" > ${temp_f}/${alt_name_filename}
		else
			paste <(cut -f${germplasm_name_col} ${temp_f}/cleaned_infile2.txt|tail -n+2) <(cut -f${col_no} ${temp_f}/cleaned_infile2.txt|tail -n+2|perl -pe 's/^ +//g'|perl -pe 's/ +$//g'|perl -pe 's/(^0$|^$)/NULL/g') |perl -pe "s/\n/\t${description}\t${refers_to}\t${date_named}\t${naming_team_code}\t${naming_organization_code}\t${site_specificity}\t${test_specificity}\t${data_source_id}\n/g" >> ${temp_f}/${alt_name_filename}
		fi
		
	done

	#clean the file (remove alternative names of NULL value) and add headers
	#***TEAMCODE_TO_BE_FIXED***
	cat <(printf "germplasm_name\talternative_name\tdescription\trefers_to\tdate_named\tnaming_team_code\tnaming_organization_code\tsite_specificity\ttest_specificity\tdata_source_id\n") <(awk -F "\t" '{if ($2 != "NULL") print}' ${temp_f}/${alt_name_filename}) > ${temp_f}/2import_alternative_germplasm_names.txt

	printf "done\n"
	
	
#F. comments_on_germplasms
#----------------------------------------------------------------
	printf "comments_on_germplasms..."

	comments_filename="comments.txt"
	printf "" > ${temp_f}/${comments_filename}

	for infile_column in $(awk -F "\t" '{print $3"\t"$1"\t"$4}' ${temp_f}/all_headers.txt|grep '^COMMENT[[:space:]]')
	do
		col_no=$(echo $infile_column|cut -f2)	#Read column number (applies to cleaned_infile2.txt)
		comment_date=$(echo $infile_column|cut -f3)

		#Get date-based team id from the list '${temp_f}/date_employees_teamid.txt'
		#***TEAMCODE_ok_20150107***
		team_id=$(grep "^${comment_date}[[:space:]]" ${temp_f}/date_employees_teamid.txt |cut -f3)
		if [ -z ${team_id} ];then team_id="NULL";fi

	#Clean the column:
		#If the measurement file doesn't exist, create it. Otherwise, append to it
		#The file created below will contain:
			#1. germplasm_name
			#2. comment
				#NOTES: 
					#1. remove leading/trailing spaces (perl -pe 's/^ +//g'|perl -pe 's/ +$//g')
					#3. replace empties with NULLs.
			#(...)
		#NOTE: could use the fnct_clean_values function instead of cleaning here directly
		if [ ! -e ${temp_f}/${comments_filename} ]
		then
			#***TEAMCODE_ok_20150108***
			paste <(cut -f${germplasm_name_col} ${temp_f}/cleaned_infile2.txt|tail -n+2) <(cut -f${col_no} ${temp_f}/cleaned_infile2.txt|tail -n+2|perl -pe 's/^ +//g'|perl -pe 's/ +$//g'|perl -pe 's/^$/NULL/g') |perl -pe "s/\n/\t${comment_date}\t${date_is_approximate}\t${context}\t${team_id}\t${data_source_id}\n/g" > ${temp_f}/${comments_filename}
		else
			paste <(cut -f${germplasm_name_col} ${temp_f}/cleaned_infile2.txt|tail -n+2) <(cut -f${col_no} ${temp_f}/cleaned_infile2.txt|tail -n+2|perl -pe 's/^ +//g'|perl -pe 's/ +$//g'|perl -pe 's/^$/NULL/g') |perl -pe "s/\n/\t${comment_date}\t${date_is_approximate}\t${context}\t${team_id}\t${data_source_id}\n/g" >> ${temp_f}/${comments_filename}
		fi
		
	done

	#clean the file (remove alternative names of NULL value) and add headers
	#***TEAMCODE_ok_20150108***
	cat <(printf "temp_germplasm_name\tcomment\tdate\tdate_is_approximate\tcontext\tteam_id\tdata_source_id\n") <(awk -F "\t" '{if ($2 != "NULL") print}' ${temp_f}/${comments_filename}) > ${temp_f}/2import_comments_on_germplasms.txt

	printf "done\n"

#------------------------------------------------------------------------------------------------------------------------------------------------------------
#24. SHOWING SUBSTITUTIONS JOURNAL
#------------------------------------------------------------------------------------------------------------------------------------------------------------
echo
echo "Substitutions made (see ${temp_f}/subst_j.txt):"
echo "------------------------------------------"
perl -pe 's/^/\t/g' ${temp_f}/subst_j.txt
echo

#------------------------------------------------------------------------------------------------------------------------------------------------------------
#25. FINAL TABULATED FILES CREATION
#------------------------------------------------------------------------------------------------------------------------------------------------------------
echo
echo "Formatting the tabulated files for import:"
echo "------------------------------------------"


	printf "" > ${ready2import_tabs_folder}/${psql_commands_file}
	printf "" > ${temp_f}/psql_commands_2insert_DEV_AUTO.txt
		
	#Cycle through each of the tabulated files listed in REQ_final_tabs_creation_parameters.txt
	for tab_file in $(cat ${req_files_folder}/REQ_final_tabs_creation_parameters.txt|grep -vE '^#|^$')
	do
		tab_infile=$(echo ${tab_file}|cut -f1)
		destination_table=$(echo ${tab_file}|cut -f2)
		pos=$(echo ${tab_file}|cut -f3)
		outfile="2insert_${pos}_${destination_table}_utf.txt"
		

		printf "\tDestination table: ${destination_table}\n"
		if [ $test -eq 1 ];then printf "\tInfile: $tab_infile, Destination table: $destination_table, Outfile: $outfile\n";fi

		#If there are records in the file, proceed, otherwise skip (e.g. if 2import_germplasms.txt or 2import_germplasms_on_sites.txt are empty)
		if [ $(cat ${temp_f}/${tab_infile}|wc -l) -gt 1 ]
		then
		
							
			#Generate the field list
			head -1 ${temp_f}/${tab_infile} |perl -pe 's/\t/\n/g'|perl -pe 's/ //g'|cat -n > field_list.txt
			
			#Get first column's name
			first_field_name=$(cut -f2 field_list.txt |head -1)
			
			#Remove header row
			#Replace commas with periods in real numbers
			#Replace NULL values with \N
			grep -v "^${first_field_name}" ${temp_f}/${tab_infile}|perl -pe 's/(\d),(\d)/\1.\2/g'|perl -pe 's/\tNULL\t/\t\\N\t/g'|perl -pe 's/\tNULL\t/\t\\N\t/g'|perl -pe 's/^NULL\t/\\N\t/g'|perl -pe 's/\tNULL\n/\t\\N\n/g'> ${outfile}

			#Convert to utf-8
				#NOT NECESSARY ANYMORE, because the original file (e.g. E57B.txt), from which the current files are derived, has already been converted to utf-8. 
				#fnct_convert_to_utf8 tempA.txt ${outfile}
			
			#Prepare import command to be run by cleseb user:
			grep "^${first_field_name}" ${temp_f}/${tab_infile}|perl -pe 's/ //g'|perl -pe 's/\t/,/g'|awk -v awk_outfilename=${outfile} -v awk_destination_table=${destination_table} '{print "psql -U cleseb -h \"132.156.208.30\" -d ts_d -c \"\\copy "awk_destination_table" ("$1") FROM '\''"awk_outfilename"'\''\""}' >> ${ready2import_tabs_folder}/${psql_commands_file}
			#Prepare the same command to be run by xtraksites user:
			#Note: from 10/2/2015 on, also include the table name and tabulated filename in front of the psql command, separated from the latter by a tab. This will serve when executing the command later to grab table name more easily
				#Line before 10/2/2015: grep "^${first_field_name}" ${temp_f}/${tab_infile}|perl -pe 's/ //g'|perl -pe 's/\t/,/g'|awk -v awk_outfilename=${outfile} -v awk_destination_table=${destination_table} '{print "psql -d ___DATABASE___ -c \"\\copy "awk_destination_table" ("$1") FROM '\''___TABS_FOLDER___"awk_outfilename"'\''\""}' >> ${temp_f}/psql_commands_2insert_DEV_AUTO.txt
			grep "^${first_field_name}" ${temp_f}/${tab_infile}|perl -pe 's/ //g'|perl -pe 's/\t/,/g'|awk -v awk_outfilename=${outfile} -v awk_destination_table=${destination_table} '{print "psql -d ___DATABASE___ -c \"\\copy "awk_destination_table" ("$1") FROM '\''___TABS_FOLDER___"awk_outfilename"'\''\""}'|perl -pe "s/^/${tab_infile}\t${destination_table}\t/g" >> ${temp_f}/psql_commands_2insert_DEV_AUTO.txt

			#Get some stats
			tab_infile_rows_count=$(grep -v "^${first_field_name}" ${temp_f}/${tab_infile}|wc -l)
			tab_infile_col_count=$(awk -F "\t" '{print NF}' ${temp_f}/${tab_infile}|sort -nu|tail -1)
			outfile_rows_count=$(cat ${outfile}|wc -l)
			outfile_col_count=$(awk -F "\t" '{print NF}' ${outfile}|sort -nu|tail -1)

			if [ $test -eq 1 ]
			then
				printf "\tStats\n"; printf "\t------\n"
				printf "\tInfile: ${tab_infile}, ${tab_infile_rows_count} rows, ${tab_infile_col_count} columns.\n"
				printf "\tOutfile: ${outfile}, ${outfile_rows_count} rows, ${outfile_col_count} columns.\n"
				
			fi
			mv ${outfile} ${ready2import_tabs_folder}
			
		fi
		
		


	done
	
	#Create the commands for the production version
	perl -pe 's/ts_d/ts_pi/g' ${ready2import_tabs_folder}/${psql_commands_file} > ${ready2import_tabs_folder}/$(echo $psql_commands_file|perl -pe 's/_DEV.txt/_PROD.txt/g')
	
	rm field_list.txt
	

#exit

####################################################################
#############################SKIP_TAG#################################
####################################################################
fi
#THIS IS THE END OF THE CODE TO BE SKIPPED, IF skip=="yes"
####################################################################
####################################################################
####################################################################

#------------------------------------------------------------------------------------------------------------------------------------------------------------
#26. PREPARING SQL SCRIPTS
#------------------------------------------------------------------------------------------------------------------------------------------------------------
echo
echo "Preparing SQL UPDATE/INSERT scripts:"
echo "------------------------------------------"

#Prepare final SQL INSERT/UPDATE files 1 & 2:

#Script 1: to be executed before insertion of all other data
	echo "--SCRIPT 1 -- EXECUTE BEFORE INSERTION OF ALL OTHER DATA (germplasms, germplasms_on_sites, coded_observations_on_germplasms, etc.)" > ${ready2import_tabs_folder}/SQL_insert_and_update1.txt
	echo "--This script was automatically generated on ${timestamp} by the script ${0}." >> ${ready2import_tabs_folder}/SQL_insert_and_update1.txt
	echo "-----------------------------------------------------------------------------" >> ${ready2import_tabs_folder}/SQL_insert_and_update1.txt
	printf "START TRANSACTION;\n" >> ${ready2import_tabs_folder}/SQL_insert_and_update1.txt

	if [ -s ${temp_f}/SQL_insert_new_codes.txt ]
	then
		printf "\n%s\n" "--Insert new codes:" >> ${ready2import_tabs_folder}/SQL_insert_and_update1.txt
		cat ${temp_f}/SQL_insert_new_codes.txt >> ${ready2import_tabs_folder}/SQL_insert_and_update1.txt
	fi
	printf "COMMIT;\n" >> ${ready2import_tabs_folder}/SQL_insert_and_update1.txt

#Script 2: to be executed after insertion of all other data
	echo "--SCRIPT 2 -- EXECUTE AFTER INSERTION OF ALL OTHER DATA (germplasms, germplasms_on_sites, coded_observations_on_germplasms, etc.)" > ${ready2import_tabs_folder}/SQL_insert_and_update2.txt
	echo "--This script was automatically generated on ${timestamp} by the script ${0}." >> ${ready2import_tabs_folder}/SQL_insert_and_update2.txt
	echo "-----------------------------------------------------------------------------" >> ${ready2import_tabs_folder}/SQL_insert_and_update2.txt
	printf "START TRANSACTION;\n" >> ${ready2import_tabs_folder}/SQL_insert_and_update2.txt
	
	if [ -s ${temp_f}/SQL_insert_observations_on_sites.txt ]
	then
		printf "\n%s\n" "--Add observations on sites:" >> ${ready2import_tabs_folder}/SQL_insert_and_update2.txt
		cat ${temp_f}/SQL_insert_observations_on_sites.txt >> ${ready2import_tabs_folder}/SQL_insert_and_update2.txt
	fi
	
	if [ -s ${temp_f}/SQL_insert_observations_on_sites2.txt ]
	then
		printf "\n%s\n" "--Add observations on sites - sub-block number used instead of block number:" >> ${ready2import_tabs_folder}/SQL_insert_and_update2.txt
		cat ${temp_f}/SQL_insert_observations_on_sites2.txt >> ${ready2import_tabs_folder}/SQL_insert_and_update2.txt
	fi
	
	
	if [ $(ls -1 ${temp_f}|grep 'SQL_update_meas'|wc -l) -gt 0 ]
	then
		printf "\n%s\n" "--Notifications on potential measurement errors (e.g. decreasing height or DBH):" >> ${ready2import_tabs_folder}/SQL_insert_and_update2.txt
		cat ${temp_f}/SQL_update_meas* >> ${ready2import_tabs_folder}/SQL_insert_and_update2.txt
	fi

	if [ $(ls -1 ${temp_f}|grep 'SQL_insert_saved_errors'|wc -l) -gt 0 ]
	then
		printf "\n%s\n" "--Notifications on observation code errors (e.g. when a dead or missing tree has received a code indicating a bent shape.):" >> ${ready2import_tabs_folder}/SQL_insert_and_update2.txt
		cat ${temp_f}/SQL_insert_saved_errors* >> ${ready2import_tabs_folder}/SQL_insert_and_update2.txt
	fi
	printf "COMMIT;\n" >> ${ready2import_tabs_folder}/SQL_insert_and_update2.txt

#Script 3: To be edited by user and executed after insertion of all other data
	echo "--SCRIPT 3 -- EXECUTE AFTER INSERTION OF ALL OTHER DATA (germplasms, germplasms_on_sites, coded_observations_on_germplasms, etc.)" > ${ready2import_tabs_folder}/SQL_insert_and_update3.txt
	echo "--This script was automatically generated on ${timestamp} by the script ${0}." >> ${ready2import_tabs_folder}/SQL_insert_and_update3.txt
	echo "-----------------------------------------------------------------------------" >> ${ready2import_tabs_folder}/SQL_insert_and_update3.txt
	printf "START TRANSACTION;\n" >> ${ready2import_tabs_folder}/SQL_insert_and_update3.txt
	cat ${req_files_folder}/SQL_OTHER_COMMANDS_EXAMPLE.txt|perl -pe "s/___site_code___/${site_code}/g"|perl -pe "s/___data_source_id___/${data_source_id}/g" >> ${ready2import_tabs_folder}/SQL_insert_and_update3.txt
	printf "COMMIT;\n" >> ${ready2import_tabs_folder}/SQL_insert_and_update3.txt
	printf "done\n"

#Script for stats
	perl -pe "s/___DSI___/${data_source_id}/g" ${req_files_folder}/SQL_count_records.txt > ${temp_f}/SQL_count_records.txt

#Script for validation. Now (23/1/2015) based on site code rather than on data source id, because searching with the latter in germplasm_names did not work when no records were added to germplasms, but only to tree_mensurations and/or coded_observations_on_germplasms
	perl -pe "s/___site_code___/${site_code}/g" ${req_files_folder}/SQL_extract_data_for_validation.txt > ${temp_f}/SQL_extract_data_for_validation.txt


#------------------------------------------------------------------------------------------------------------------------------------------------------------
#27. AUTOMATIC INSERTION INTO TreeSource & PRODUCTION OF A FILE TO VALIDATE DATA
#------------------------------------------------------------------------------------------------------------------------------------------------------------
echo
echo "Inserting data automatically in TreeSource:"
echo "------------------------------------------"

	printf "${red_color}Insert data in ts_d right now?\n\t('n' will continue script without inserting data)\n${normal_color}"
	auto_insert_dev_choice="-";while [ ! $(echo $auto_insert_dev_choice|grep '[Yn]') ];do printf "\r\t"; read -p "[Y/n]?" -n1 -s auto_insert_dev_choice;done;printf "\n"
	echo
	#If insertion into ts_d was chosen
	#If not, will simply skip this section
	if [ ${auto_insert_dev_choice} == "Y" ]
	then
		#disable_trigger ts_d germplasms_on_sites trg_germplasms_on_sites_03
			#13/1/2015: No disabling necessary now, because that trigger lauches proc_set_updated_site_or_container_to_1(), which doesn't take time to execute.
			#Trigger disabling was necessary before, because it recalculated mv_current_germplasms_site_and_container for every row.
			#Since it could occur in the middle of an insertion in the same table by another instance of the same script, it could become very long.
			#Reenabling the trigger was done with: enable_trigger ts_d germplasms_on_sites trg_germplasms_on_sites_03

		fnct_auto_insert_all_in_db 1
			fi

	printf "${red_color}Insert data in ts_pi right now?\n\t('n' will continue script without inserting data)\n${normal_color}"
	auto_insert_prod_choice="-";while [ ! $(echo $auto_insert_prod_choice|grep '[Yn]') ];do printf "\r\t"; read -p "[Y/n]?" -n1 -s auto_insert_prod_choice;done;printf "\n"
	echo
	#If insertion into ts_d was chosen
	#If not, will simply skip this section
	if [ ${auto_insert_prod_choice} == "Y" ]
	then
		#disable_trigger ts_pi germplasms_on_sites trg_germplasms_on_sites_03
			#13/1/2015: No disabling necessary now, because that trigger lauches proc_set_updated_site_or_container_to_1(), which doesn't take time to execute.
			#Trigger disabling was necessary before, because it recalculated mv_current_germplasms_site_and_container for every row.
			#Since it could occur in the middle of an insertion in the same table by another instance of the same script, it could become very long.
			#Reenabling the trigger was done with: enable_trigger ts_d germplasms_on_sites trg_germplasms_on_sites_03
		fnct_auto_insert_all_in_db 2
	fi

#------------------------------------------------------------------------------------------------------------------------------------------------------------
#28. MOVING ALL FILES TO A COMMON FOLDER 
#------------------------------------------------------------------------------------------------------------------------------------------------------------

echo
echo "Moving the files to ${final_folder} folder:"
echo "------------------------------------------"


if [ ! -e ${final_folder} ]; then mkdir ${final_folder};fi


#Move files
mv ${temp_f}/team_employees_per_date.txt ${final_folder}				#Move file with team employees for each date
mv ${temp_f} ${final_folder} 					#Move temp folder.
mv ${ready2import_tabs_folder} ${final_folder} 			#Move final files folder
#mv uniformize_temp ${final_folder}					#Move column number uniformization temp folder
mv ${validation_temp_f} ${final_folder} 				#Move column validation temp folder
cp ${infile} ${final_folder} 					#Move infile.
cp -r ${req_files_folder} ${final_folder}				#Copy required files
cp $0 ${final_folder}						#Copy current script
echo "done"

echo
printf "${green_color}SCRIPT COMPLETED!${normal_color}\n"
echo

if [ ${auto_insert_prod_choice} == "n" ]
then
	printf "${red_color}If not already done:\n--------------------------------\n"
	printf "1. REVIEW AND EXECUTE THE SCRIPT ${ready2import_tabs_folder}/SQL_insert_and_update1.txt before inserting other data.\n"
	printf "2. THEN, INSERT MAIN DATA (e.g. ${ready2import_tabs_folder}/2insert_01_germplasms_utf.txt).\n"
	printf "3. FINALLY, REVIEW AND EXECUTE THE SCRIPTS ${ready2import_tabs_folder}/SQL_insert_and_update2.txt and ${ready2import_tabs_folder}/SQL_insert_and_update3.txt.${normal_color}\n"
fi


#echo "journal_f: ${journal_f}" 
#echo "final_folder: ${final_folder}"

} 2>&1|tee  ${ready2import_tabs_folder}/extract_sites_log.txt
#} 2>&1|tee ${journal_f}
#End of the capture in the log
#echo "journal_f: ${journal_f}" 
#echo "final_folder: ${final_folder}"

#mv ${journal_f} ${final_folder}

exit 0
